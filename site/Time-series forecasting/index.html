<!DOCTYPE html>

<html lang="en">


<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../TImmunology/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.12">
    
    
<title>Literature Survey (VPE)</title>

    
      <link rel="stylesheet" href="../assets/stylesheets/main.7e359304.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
  <!-- Add scripts that need to run before here -->
  <!-- Add jquery script -->
  <script src="https://code.jquery.com/jquery-3.7.1.js"></script>
  <!-- Add data table libraries -->
  <script src="https://cdn.datatables.net/2.0.1/js/dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.1/css/dataTables.dataTables.css">
  <!-- Load plotly.js into the DOM -->
	<script src='https://cdn.plot.ly/plotly-2.29.1.min.js'></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/buttons/3.0.1/css/buttons.dataTables.css">
  <!-- fixedColumns -->
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/dataTables.fixedColumns.js"></script>
  <script src="https://cdn.datatables.net/fixedcolumns/5.0.0/js/fixedColumns.dataTables.js"></script>
  <link rel="stylesheet" href="https://cdn.datatables.net/fixedcolumns/5.0.0/css/fixedColumns.dataTables.css">
  <!-- Already specified in mkdocs.yml -->
  <!-- <link rel="stylesheet" href="../docs/custom.css"> -->
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/dataTables.buttons.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.dataTables.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/pdfmake.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.2.7/vfs_fonts.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.html5.min.js"></script>
  <script src="https://cdn.datatables.net/buttons/3.0.1/js/buttons.print.min.js"></script>
  <!-- Google fonts -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <!-- Intro.js -->
  <script src="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/intro.min.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/intro.js@7.2.0/minified/introjs.min.css">


  <!-- 
      
     -->
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../custom.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Literature Survey for EDC" class="md-header__button md-logo" aria-label="Literature Survey for EDC" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Literature Survey for EDC
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Time-series forecasting
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="white" data-md-color-accent="black"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Time-series forecasting

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../TImmunology/" class="md-tabs__link">
        
  
    
  
  TImmunology

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Literature Survey for EDC" class="md-nav__button md-logo" aria-label="Literature Survey for EDC" data-md-component="logo">
      
  <img src="../assets/VPE.png" alt="logo">

    </a>
    Literature Survey for EDC
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/VirtualPatientEngine/literatureSurvey" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    VPE/LiteratureSurvey
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Time-series forecasting
  </span>
  

      </a>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../TImmunology/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TImmunology
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Time-series forecasting</h1>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
</head>

<body>
  <p>
  <i class="footer">This page was last updated on 2024-08-08 07:42:29 UTC</i>
  </p>

  <div class="note info" onclick="startIntro()">
    <p>
      <button type="button" class="buttons">
        <div style="display: flex; align-items: center;">
        Click here for a quick intro of the page! <i class="material-icons">help</i>
        </div>
      </button>
    </p>
  </div>

  <!--
  <div data-intro='Table of contents'>
    <p>
    <h3>Table of Contents</h3>
      <a href="#plot1">1. Citations over time on Time-series forecasting</a><br>
      <a href="#manually_curated_articles">2. Manually curated articles on Time-series forecasting</a><br>
      <a href="#recommended_articles">3. Recommended articles on Time-series forecasting</a><br>
    <p>
  </div>

  <div data-intro='Plot displaying number of citations over time 
                  on the given topic based on recommended articles'>
    <p>
    <h3 id="plot1">1. Citations over time on Time-series forecasting</h3>
      <div id='myDiv1'>
      </div>
    </p>
  </div>
  -->

  <div data-intro='Manually curated articles on the given topic'>
    <p>
    <h3 id="manually_curated_articles">Manually curated articles on <i>Time-series forecasting</i></h3>
    <table id="table1" class="display" style="width:100%">
    <thead>
      <tr>
          <th data-intro='Click to view the abstract (if available)'>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th data-intro='Highest h-index among the authors'>Highest h-index</th>
          <th data-intro='Recommended articles extracted by considering
                          only the given article'>
              View recommendations
              </th>
      </tr>
    </thead>
    <tbody>

        <tr id="Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. These approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative research works and introduce mainstream applications of GNN4TS. A comprehensive discussion of potential future research directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series research, highlighting foundations, practical applications, and opportunities of graph neural networks for time series analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9" target='_blank'>
                A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection
                </a>
              </td>
          <td>
            Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, C. Alippi, G. I. Webb, Irwin King, Shirui Pan
          </td>
          <td>2023-07-07</td>
          <td>ArXiv, arXiv.org</td>
          <td>61</td>
          <td>49</td>

            <td><a href='../recommendations/d3dbbd0f0de51b421a6220bd6480b8d2e99a88e9' target='_blank'>
              <i class="material-icons">open_in_new</i></a>
            </td>

        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/ Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
          <th>View recommendations</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

  <div data-intro='Recommended articles extracted by contrasting
                  articles that are relevant against not relevant for Time-series forecasting'>
    <p>
    <h3 id="recommended_articles">Recommended articles on <i>Time-series forecasting</i></h3>
    <table id="table2" class="display" style="width:100%">
    <thead>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </thead>
    <tbody>

        <tr id="Time series, characterized by a sequence of data points arranged in a discrete-time order, are ubiquitous in real-world applications. Different from other modalities, time series present unique challenges due to their complex and dynamic nature, including the entanglement of nonlinear patterns and time-variant trends. Analyzing time series data is of great significance in real-world scenarios and has been widely studied over centuries. Recent years have witnessed remarkable breakthroughs in the time series community, with techniques shifting from traditional statistical methods to advanced deep learning models. In this paper, we delve into the design of deep time series models across various analysis tasks and review the existing literature from two perspectives: basic modules and model architectures. Further, we develop and release Time Series Library (TSLib) as a fair benchmark of deep time series models for diverse analysis tasks, which implements 24 mainstream models, covers 30 datasets from different domains, and supports five prevalent analysis tasks. Based on TSLib, we thoroughly evaluate 12 advanced deep time series models on different tasks. Empirical results indicate that models with specific structures are well-suited for distinct analytical tasks, which offers insights for research and adoption of deep time series models. Code is available at https://github.com/thuml/Time-Series-Library.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/425182f5c96c1d239da5cbe3a24371b2bab6319b" target='_blank'>
              Deep Time Series Models: A Comprehensive Survey and Benchmark
              </a>
            </td>
          <td>
            Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, Jianmin Wang
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>65</td>
        </tr>

        <tr id="To achieve more accurate prediction results in Time Series Forecasting (TSF), it is essential to distinguish between the valuable patterns (invariant patterns) of the spatial-temporal relationship and the patterns that are prone to generate distribution shift (variant patterns), then combine them for forecasting.The existing works, such as transformer-based models and GNN-based models, focus on capturing main forecasting dependencies whether it is stable or not, and they tend to overlook patterns that carry both useful information and distribution shift. In this paper, we propose a model for better forecasting time series: Decoupled Invariant Attention Network (DIAN), which contains two modules to learn spatial and temporal relationships respectively: 1) Spatial Decoupled Invariant-Variant Learning (SDIVL) to decouple the spatial invariant and variant attention scores, and then leverage convolutional networks to effectively integrate them for subsequent layers; 2) Temporal Augmented Invariant-Variant Learning (TAIVL) to decouple temporal invariant and variant patterns and combine them for further forecasting.In this module, we also design Temporal Intervention Mechanism to create multiple intervened samples by reassembling variant patterns across time stamps to eliminate the spurious impacts of variant patterns.In addition, we propose Joint Optimization to minimize the loss function considering all invariant patterns, variant patterns and intervened patterns so that our model can gain a more stable predictive ability.Extensive experiments on five datasets have demonstrated our superior performance with higher efficiency compared with state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/08c5216266696b3eea52e8c688f6e7f32711e5a6" target='_blank'>
              Decoupled Invariant Attention Network for Multivariate Time-series Forecasting
              </a>
            </td>
          <td>
            Haihua Xu, Wei Fan, Kun Yi, Pengyang Wang
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Time Series Anomaly Detection (TSAD) finds widespread applications across various domains such as financial markets, industrial production, and healthcare. Its primary objective is to learn the normal patterns of time series data, thereby identifying deviations in test samples. Most existing TSAD methods focus on modeling data from the temporal dimension, while ignoring the semantic information in the spatial dimension. To address this issue, we introduce a novel approach, called Spatial-Temporal Normality learning (STEN). STEN is composed of a sequence Order prediction-based Temporal Normality learning (OTN) module that captures the temporal correlations within sequences, and a Distance prediction-based Spatial Normality learning (DSN) module that learns the relative spatial relations between sequences in a feature space. By synthesizing these two modules, STEN learns expressive spatial-temporal representations for the normal patterns hidden in the time series data. Extensive experiments on five popular TSAD benchmarks show that STEN substantially outperforms state-of-the-art competing methods. Our code is available at https://github.com/mala-lab/STEN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6136ae3b84b2b1b9e6215a5c67a3153abd0eedc2" target='_blank'>
              Self-Supervised Spatial-Temporal Normality Learning for Time Series Anomaly Detection
              </a>
            </td>
          <td>
            Yutong Chen, Hongzuo Xu, Guansong Pang, Hezhe Qiao, Yuan Zhou, Mingsheng Shang
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Multivariate time series anomaly detection is a crucial data mining technique with a wide range of applications in areas such as IT applications. Currently, the majority of anomaly detection methods for time series data rely on unsupervised approaches due to the rarity of anomaly labels. However, in real-world scenarios, obtaining a limited number of anomaly labels is feasible and affordable. Effective usage of these labels can offer valuable insights into the temporal characteristics of anomalies and play a pivotal role in guiding anomaly detection efforts. To improve the performance of multivariate time series anomaly detection, we proposed a novel deep learning model named EDD (Encoder-Decoder-Discriminator) that leverages limited anomaly samples. The EDD model innovatively integrates a graph attention network with long short term memory (LSTM) to extract spatial and temporal features from multivariate time series data. This integrated approach enables the model to capture complex patterns and dependencies within the data. Additionally, the model skillfully maps series data into a latent space, utilizing a carefully crafted loss function to cluster normal data tightly in the latent space while dispersing abnormal data randomly. This innovative design results in distinct probability distributions for normal and abnormal data in the latent space, enabling precise identification of anomalous data. To evaluate the performance of our EDD model, we conducted extensive experimental validation across three diverse datasets. The results demonstrate the significant superiority of our model in multivariate time series anomaly detection. Specifically, the average F1-Score of our model outperformed the second-best method by 2.7% and 73.4% in both evaluation approaches, respectively, highlighting its superior detection capabilities. These findings validate the effectiveness of our proposed EDD model in leveraging limited anomaly samples for accurate and robust anomaly detection in multivariate time series data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a0b2af4fae72e31c61e9b23f7aa121732e8dfa5" target='_blank'>
              An anomaly detection model for multivariate time series with anomaly perception
              </a>
            </td>
          <td>
            Dong Wei, Wu Sun, Xiaofeng Zou, Dan Ma, Huarong Xu, Panfeng Chen, Chaoshu Yang, Mei Chen, Hui Li
          </td>
          <td>2024-07-31</td>
          <td>PeerJ Computer Science</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Traffic prediction is a challenging spatio-temporal forecasting problem that involves highly complex spatio-temporal correlations. This paper proposes a Multi-level Multi-view Augmented Spatio-temporal Transformer (LVSTformer) for traffic prediction. The model aims to capture spatial dependencies from three different levels: local geographic, global semantic, and pivotal nodes, along with long- and short-term temporal dependencies. Specifically, we design three spatial augmented views to delve into the spatial information from the perspectives of local, global, and pivotal nodes. By combining three spatial augmented views with three parallel spatial self-attention mechanisms, the model can comprehensively captures spatial dependencies at different levels. We design a gated temporal self-attention mechanism to effectively capture long- and short-term temporal dependencies. Furthermore, a spatio-temporal context broadcasting module is introduced between two spatio-temporal layers to ensure a well-distributed allocation of attention scores, alleviating overfitting and information loss, and enhancing the generalization ability and robustness of the model. A comprehensive set of experiments is conducted on six well-known traffic benchmarks, the experimental results demonstrate that LVSTformer achieves state-of-the-art performance compared to competing baselines, with the maximum improvement reaching up to 4.32%.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/22af70b0b268da0f71549bad07d5b3f24528fb16" target='_blank'>
              Rethinking Spatio-Temporal Transformer for Traffic Prediction:Multi-level Multi-view Augmented Learning Framework
              </a>
            </td>
          <td>
            Jiaqi Lin, Qianqian Ren
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Multi-variate time series forecasting is an important problem with a wide range of applications. Recent works model the relations between time-series as graphs and have shown that propagating information over the relation graph can improve time series forecasting. However, in many cases, relational information is not available or is noisy and reliable. Moreover, most works ignore the underlying uncertainty of time-series both for structure learning and deriving the forecasts resulting in the structure not capturing the uncertainty resulting in forecast distributions with poor uncertainty estimates. We tackle this challenge and introduce STOIC, that leverages stochastic correlations between time-series to learn underlying structure between time-series and to provide well-calibrated and accurate forecasts. Over a wide-range of benchmark datasets STOIC provides around 16% more accurate and 14% better-calibrated forecasts. STOIC also shows better adaptation to noise in data during inference and captures important and useful relational information in various benchmarks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7d4b412399f89c9cd66cffc4ab811b74172dcc63" target='_blank'>
              Learning Graph Structures and Uncertainty for Accurate and Calibrated Time-series Forecasting
              </a>
            </td>
          <td>
            Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodríguez, Chao Zhang, B. A. Prakash
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Spatio-temporal graph (STG) forecasting is a critical task with extensive applications in the real world, including traffic and weather forecasting. Although several recent methods have been proposed to model complex dynamics in STGs, addressing long-range spatio-temporal dependencies remains a significant challenge, leading to limited performance gains. Inspired by a recently proposed state space model named Mamba, which has shown remarkable capability of capturing long-range dependency, we propose a new STG forecasting framework named SpoT-Mamba. SpoT-Mamba generates node embeddings by scanning various node-specific walk sequences. Based on the node embeddings, it conducts temporal scans to capture long-range spatio-temporal dependencies. Experimental results on the real-world traffic forecasting dataset demonstrate the effectiveness of SpoT-Mamba.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/249b1ae3616ef24f7cea210b7e049f39f2586206" target='_blank'>
              SpoT-Mamba: Learning Long-Range Dependency on Spatio-Temporal Graphs with Selective State Spaces
              </a>
            </td>
          <td>
            Jinhyeok Choi, Heehyeon Kim, Minhyeong An, Joyce Jiyoung Whang
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Traffic flow forecasting is considered a critical task in the field of intelligent transportation systems. In this paper, to address the issue of low accuracy in long-term forecasting of spatial-temporal big data on traffic flow, we propose an innovative model called Spatial-Temporal Retentive Network (ST-RetNet). We extend the Retentive Network to address the task of traffic flow forecasting. At the spatial scale, we integrate a topological graph structure into Spatial Retentive Network(S-RetNet), utilizing an adaptive adjacency matrix to extract dynamic spatial features of the road network. We also employ Graph Convolutional Networks to extract static spatial features of the road network. These two components are then fused to capture dynamic and static spatial correlations. At the temporal scale, we propose the Temporal Retentive Network(T-RetNet), which has been demonstrated to excel in capturing long-term dependencies in traffic flow patterns compared to other time series models, including Recurrent Neural Networks based and transformer models. We achieve the spatial-temporal traffic flow forecasting task by integrating S-RetNet and T-RetNet to form ST-RetNet. Through experimental comparisons conducted on four real-world datasets, we demonstrate that ST-RetNet outperforms the state-of-the-art approaches in traffic flow forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2ff0fb7e804a47b3a7fc6957f58fb65aa01ba12a" target='_blank'>
              ST-RetNet: A Long-term Spatial-Temporal Traffic Flow Prediction Method
              </a>
            </td>
          <td>
            Baichao Long, Wang Zhu, Jianli Xiao
          </td>
          <td>2024-07-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Continuous efforts are being made to advance anomaly detection in various manufacturing processes to increase the productivity and safety of industrial sites. Deep learning replaced rule-based methods and recently emerged as a promising method for anomaly detection in diverse industries. However, in the real world, the scarcity of abnormal data and difficulties in obtaining labeled data create limitations in the training of detection models. In this study, we addressed these shortcomings by proposing a learnable data augmentation-based time-series anomaly detection (LATAD) technique that is trained in a self-supervised manner. LATAD extracts discriminative features from time-series data through contrastive learning. At the same time, learnable data augmentation produces challenging negative samples to enhance learning efficiency. We measured anomaly scores of the proposed technique based on latent feature similarities. As per the results, LATAD exhibited comparable or improved performance to the state-of-the-art anomaly detection assessments on several benchmark datasets and provided a gradient-based diagnosis technique to help identify root causes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b19b76743a2f29bfb6a2705fd1f2877a8558f467" target='_blank'>
              Self-Supervised Time-Series Anomaly Detection Using Learnable Data Augmentation
              </a>
            </td>
          <td>
            K. Choi, Jihun Yi, J. Mok, Sungroh Yoon
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Time series forecasting (TSF) is crucial in fields like economic forecasting, weather prediction, traffic flow analysis, and public health surveillance. Real-world time series data often include noise, outliers, and missing values, making accurate forecasting challenging. Traditional methods model point-to-point relationships, which limits their ability to capture complex temporal patterns and increases their susceptibility to noise.To address these issues, we introduce the WindowMixer model, built on an all-MLP framework. WindowMixer leverages the continuous nature of time series by examining temporal variations from a window-based perspective. It decomposes time series into trend and seasonal components, handling them individually. For trends, a fully connected (FC) layer makes predictions. For seasonal components, time windows are projected to produce window tokens, processed by Intra-Window-Mixer and Inter-Window-Mixer modules. The Intra-Window-Mixer models relationships within each window, while the Inter-Window-Mixer models relationships between windows. This approach captures intricate patterns and long-range dependencies in the data.Experiments show WindowMixer consistently outperforms existing methods in both long-term and short-term forecasting tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b5e7cfba3feae0a37aa3ce200bec06808907851" target='_blank'>
              WindowMixer: Intra-Window and Inter-Window Modeling for Time Series Forecasting
              </a>
            </td>
          <td>
            Quangao Liu, Ruiqi Li, Maowei Jiang, Wei Yang, Chen Liang, Longlong Pang, Zhuozhang Zou
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The COVID-19 pandemic and influenza outbreaks have underscored the critical need for predictive models that can effectively integrate spatial and temporal dynamics to enable accurate epidemic forecasting. Traditional time-series analysis approaches have fallen short in capturing the intricate interplay between these factors. Recent advancements have witnessed the incorporation of graph neural networks and machine learning techniques to bridge this gap, enhancing predictive accuracy and providing novel insights into disease spread mechanisms. Notable endeavors include leveraging human mobility data, employing transfer learning, and integrating advanced models such as Transformers and Graph Convolutional Networks (GCNs) to improve forecasting performance across diverse geographies for both influenza and COVID-19. However, these models often face challenges related to data quality, model transferability, and potential overfitting, highlighting the necessity for more adaptable and robust approaches. This paper introduces the Graph Attention-based Spatial Temporal (GAST) model, which employs graph attention networks (GATs) to overcome these limitations by providing a nuanced understanding of epidemic dynamics through a sophisticated spatio-temporal analysis framework. Our contributions include the development and validation of the GAST model, demonstrating its superior forecasting capabilities for influenza and COVID-19 spread, with a particular focus on short-term, daily predictions. The model’s application to both influenza and COVID-19 datasets showcases its versatility and potential to inform public health interventions across a range of infectious diseases.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/628bd28abb35127fc3d4e794dfc8e0860b738427" target='_blank'>
              Modeling epidemic dynamics using Graph Attention based Spatial Temporal networks
              </a>
            </td>
          <td>
            Xiaofeng Zhu, Yi Zhang, Haoru Ying, Huanning Chi, Guanqun Sun, Lingxia Zeng
          </td>
          <td>2024-07-15</td>
          <td>PLOS ONE</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Modeling complex networks allows us to analyze the characteristics and discover the basic mechanisms governing phenomena such as disease outbreaks, information diffusion, transportation efficiency, social influence, and even human brain function. Consequently, various network generative models (called temporal network models) have been presented to model how the network topologies evolve dynamically over time. Temporal network models face the challenge of results evaluation because common evaluation methods are appropriate only for static networks. This paper proposes an automatic approach based on deep learning to handle this issue. In addition to an evaluation method, the proposed method can also be used for anomaly detection in evolving networks. The proposed method has been evaluated on five different datasets, and the evaluations show that it outperforms the alternative methods based on the error rate measure in different datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/55513696e921cce0caed3c011cca450bc7bc3e2f" target='_blank'>
              Model Evaluation and Anomaly Detection in Temporal Complex Networks using Deep Learning Methods
              </a>
            </td>
          <td>
            Alireza Rashnu, Sadegh Aliakbary
          </td>
          <td>2024-06-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="The emergence of 5G technology marks a significant milestone in developing telecommunication networks, enabling exciting new applications such as augmented reality and self-driving vehicles. However, these improvements bring an increased management complexity and a special concern in dealing with failures, as the applications 5G intends to support heavily rely on high network performance and low latency. Thus, automatic self-healing solutions have become effective in dealing with this requirement, allowing a learning-based system to automatically detect anomalies and perform Root Cause Analysis (RCA). However, there are inherent challenges to the implementation of such intelligent systems. First, there is a lack of suitable data for anomaly detection and RCA, as labelled data for failure scenarios is uncommon. Secondly, current intelligent solutions are tailored to LTE networks and do not fully capture the spatio-temporal characteristics present in the data. Considering this, we utilize a calibrated simulator, Simu5G, and generate open-source data for normal and failure scenarios. Using this data, we propose Simba, a state-of-the-art approach for anomaly detection and root cause analysis in 5G Radio Access Networks (RANs). We leverage Graph Neural Networks to capture spatial relationships while a Transformer model is used to learn the temporal dependencies of the data. We implement a prototype of Simba and evaluate it over multiple failures. The outcomes are compared against existing solutions to confirm the superiority of Simba.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/65281bb8f04f4b18640fa6daeb5e16da79148d24" target='_blank'>
              Root Cause Analysis of Anomalies in 5G RAN Using Graph Neural Network and Transformer
              </a>
            </td>
          <td>
            Antor Hasan, Conrado Boeira, K. Papry, Yue Ju, Zhongwen Zhu, I. Haque
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Urban traffic speed prediction aims to estimate the future traffic speed for improving urban transportation services. Enormous efforts have been made to exploit Graph Neural Networks (GNNs) for modeling spatial correlations and temporal dependencies of traffic speed evolving patterns, regularized by graph topology. While achieving promising results, current traffic speed prediction methods still suffer from ignoring topology-free patterns, which cannot be captured by GNNs. To tackle this challenge, we propose a generic model for enabling the current GNN-based methods to preserve topology-free patterns. Specifically, we first develop a Dual Cross-Scale Transformer (DCST) architecture, including a Spatial Transformer and a Temporal Transformer, to preserve the cross-scale topology-free patterns and associated dynamics, respectively. Then, to further integrate both topology-regularized/-free patterns, we propose a distillation-style learning framework, in which the existing GNN-based methods are considered as the teacher model, and the proposed DCST architecture is considered as the student model. The teacher model would inject the learned topology-regularized patterns into the student model for integrating topology-free patterns. The extensive experimental results demonstrated the effectiveness of our methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3911024df853ccf11138d35835572ce863df51bf" target='_blank'>
              Make Graph Neural Networks Great Again: A Generic Integration Paradigm of Topology-Free Patterns for Traffic Speed Prediction
              </a>
            </td>
          <td>
            Yicheng Zhou, P. Wang, Hao Dong, Denghui Zhang, Dingqi Yang, Yanjie Fu, Pengyang Wang
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Representing temporal-structured samples is essential for effective time series analysis tasks. So far, recurrent networks, convolution networks and transformer-style models have been successively applied in temporal data representation, yielding notable results. However, most existing methods primarily focus on modeling and representing the variation patterns within time series in the time domain. As a highly abstracted information entity, 1D time series couples various patterns such as trends, seasonality, and dramatic changes (instantaneous high dynamic), it is difficult to exploit these highly coupled properties merely by analysis tools on purely time domain. To this end, we present Spectrogram Analysis and Representation Network (SpecAR-Net). SpecAR-Net aims at learning more comprehensive representations by modeling raw time series in both time and frequency domain, where an efficient joint extraction of time-frequency features is achieved through a group of learnable 2D multi-scale parallel complex convolution blocks. Experimental results show that the SpecAR-Net achieves excellent performance on 5 major downstream tasks i.e., classification, anomaly detection, imputation, long- and short-term forecasting. Code and appendix are available at https://github.com/Dongyi2go/SpecAR_Net.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1dfaff572dd20e45c52824bd90981982d242c6dc" target='_blank'>
              SpecAR-Net: Spectrogram Analysis and Representation Network for Time Series
              </a>
            </td>
          <td>
            Yi Dong, Liwen Zhang, Youcheng Zhang, Shi Peng, Wen Chen, Zhe Ma
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Long time series forecasting aims to utilize historical information to forecast future states over extended horizons. Traditional RNN-based series forecasting methods struggle to effectively address long-term dependencies and gradient issues in long time series problems. Recently, SegRNN has emerged as a leading RNN-based model tailored for long-term series forecasting, demonstrating state-of-the-art performance while maintaining a streamlined architecture through innovative segmentation and parallel decoding techniques. Nevertheless, SegRNN has several limitations: its fixed segmentation disrupts data continuity and fails to effectively leverage information across different segments, the segmentation strategy employed by SegRNN does not fundamentally address the issue of information loss within the recurrent structure. To address these issues, we propose the ISMRNN method with three key enhancements: we introduce an implicit segmentation structure to decompose the time series and map it to segmented hidden states, resulting in denser information exchange during the segmentation phase. Additionally, we incorporate residual structures in the encoding layer to mitigate information loss within the recurrent structure. To extract information more effectively, we further integrate the Mamba architecture to enhance time series information extraction. Experiments on several real-world long time series forecasting datasets demonstrate that our model surpasses the performance of current state-of-the-art models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9f37604f928c48d5acef8b85dbddb518050f7da9" target='_blank'>
              ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time Series Forecasting
              </a>
            </td>
          <td>
            Gaoxiang Zhao, Li Zhou, Xiaoqiang Wang
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time series anomaly detection is a critical task with applications in various domains. Due to annotation challenges, self-supervised methods have become the mainstream approach

for time series anomaly detection in recent years. However,

current contrastive methods categorize data perturbations into

binary classes, normal or anomaly, which lack clarity on the specific impact of different perturbation methods. Inspired by the hypothesis that "the higher the probability of misclassifying perturbation types, the higher the probability of anomalies", we propose PCRTA, our

approach firstly devises a perturbation classifier to learn the

pseudo-labels of data perturbations. Furthermore, for addressing "class collapse issue" in contrastive learning, we propose a perturbation guiding positive and negative samples

selection strategy by introducing learnable perturbation classification networks. Extensive experiments on six realworld datasets demonstrate the significant superiority of our

model over thirteen state-of-the-art competitors, and obtains average

5.14%, 8.24% improvement in F1 score and AUC-PR, respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b59b7d561eb3d48ba61a0c0a1a65c9d5b4916eae" target='_blank'>
              Perturbation Guiding Contrastive Representation Learning for Time Series Anomaly Detection
              </a>
            </td>
          <td>
            Liaoyuan Tang, Zheng Wang, Guanxiong He, Rong Wang, Feiping Nie
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>28</td>
        </tr>

        <tr id="Discrete-Time Dynamic Graphs (DTDGs), which are prevalent in real-world implementations and notable for their ease of data acquisition, have garnered considerable attention from both academic researchers and industry practitioners. The representation learning of DTDGs has been extensively applied to model the dynamics of temporally changing entities and their evolving connections. Currently, DTDG representation learning predominantly relies on GNN+RNN architectures, which manifest the inherent limitations of both Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs). GNNs suffer from the over-smoothing issue as the models architecture goes deeper, while RNNs struggle to capture long-term dependencies effectively. GNN+RNN architectures also grapple with scaling to large graph sizes and long sequences. Additionally, these methods often compute node representations separately and focus solely on individual node characteristics, thereby overlooking the behavior intersections between the two nodes whose link is being predicted, such as instances where the two nodes appear together in the same context or share common neighbors. This paper introduces a novel representation learning method DTFormer for DTDGs, pivoting from the traditional GNN+RNN framework to a Transformer-based architecture. Our approach exploits the attention mechanism to concurrently process topological information within the graph at each timestamp and temporal dynamics of graphs along the timestamps, circumventing the aforementioned fundamental weakness of both GNNs and RNNs. Moreover, we enhance the model's expressive capability by incorporating the intersection relationships among nodes and integrating a multi-patching module. Extensive experiments conducted on six public dynamic graph benchmark datasets confirm our model's efficacy, achieving the SOTA performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d0d562378884600e23eb9392361a2aa22e512b2e" target='_blank'>
              DTFormer: A Transformer-Based Method for Discrete-Time Dynamic Graph Representation Learning
              </a>
            </td>
          <td>
            Xi Chen, Yun Xiong, Siwei Zhang, Jiawei Zhang, Yao Zhang, Shiyang Zhou, Xixi Wu, Mingyang Zhang, Tengfei Liu, Weiqiang Wang
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Real-time lightweight time series anomaly detection has become increasingly crucial in cybersecurity and many other domains. Its ability to adapt to unforeseen pattern changes and swiftly identify anomalies enables prompt responses and critical decision-making. While several such anomaly detection approaches have been introduced in recent years, they primarily utilize a single type of recurrent neural networks (RNNs) and have been implemented in only one deep learning framework. It is unclear how the use of different types of RNNs available in various deep learning frameworks affects the performance of these anomaly detection approaches due to the absence of comprehensive evaluations. Arbitrarily choosing a RNN variant and a deep learning framework to implement an anomaly detection approach may not reflect its true performance and could potentially mislead users into favoring one approach over another. In this paper, we aim to study the influence of various types of RNNs available in popular deep learning frameworks on real-time lightweight time series anomaly detection. We reviewed several state-of-the-art approaches and implemented a representative anomaly detection approach using well-known RNN variants supported by three widely recognized deep learning frameworks. A comprehensive evaluation is then conducted to analyze the performance of each implementation across real-world, open-source time series datasets. The evaluation results provide valuable guidance for selecting the appropriate RNN variant and deep learning framework for real-time, lightweight time series anomaly detection.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4e9a7be2c4b74758dc6223627d57183497a8fda5" target='_blank'>
              Impact of Recurrent Neural Networks and Deep Learning Frameworks on Real-time Lightweight Time Series Anomaly Detection
              </a>
            </td>
          <td>
            Ming-Chang Lee, Jia-Chun Lin, Sokratis Katsikas
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Graph Neural Networks (GNNs) have achieved notable success in the analysis of non-Euclidean data across a wide range of domains. However, their applicability is constrained by the dependence on the observed graph structure. To solve this problem, Latent Graph Inference (LGI) is proposed to infer a task-specific latent structure by computing similarity or edge probability of node features and then apply a GNN to produce predictions. Even so, existing approaches neglect the noise from node features, which affects generated graph structure and performance. In this work, we introduce a novel method called Probability Passing to refine the generated graph structure by aggregating edge probabilities of neighboring nodes based on observed graph. Furthermore, we continue to utilize the LGI framework, inputting the refined graph structure and node features into GNNs to obtain predictions. We name the proposed scheme as Probability Passing-based Graph Neural Network (PPGNN). Moreover, the anchor-based technique is employed to reduce complexity and improve efficiency. Experimental results demonstrate the effectiveness of the proposed method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/68942a69e4eac01516b5b152a6cbf1a3dfd57c2c" target='_blank'>
              Probability Passing for Graph Neural Networks: Graph Structure and Representations Joint Learning
              </a>
            </td>
          <td>
            Ziteng Wang, YaXuan He, Bin Liu
          </td>
          <td>2024-07-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Early and accurate detection of anomalous events on the freeway, such as accidents, can improve emergency response and clearance. However, existing delays and errors in event identification and reporting make it a difficult problem to solve. Current large-scale freeway traffic datasets are not designed for anomaly detection and ignore these challenges. In this paper, we introduce the first large-scale lane-level freeway traffic dataset for anomaly detection. Our dataset consists of a month of weekday radar detection sensor data collected in 4 lanes along an 18-mile stretch of Interstate 24 heading toward Nashville, TN, comprising over 3.7 million sensor measurements. We also collect official crash reports from the Nashville Traffic Management Center and manually label all other potential anomalies in the dataset. To show the potential for our dataset to be used in future machine learning and traffic research, we benchmark numerous deep learning anomaly detection models on our dataset. We find that unsupervised graph neural network autoencoders are a promising solution for this problem and that ignoring spatial relationships leads to decreased performance. We demonstrate that our methods can reduce reporting delays by over 10 minutes on average while detecting 75% of crashes. Our dataset and all preprocessing code needed to get started are publicly released at https://vu.edu/ft-aed/ to facilitate future research.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b5d203913c33fc7edadcb9b4684e53d137811d2b" target='_blank'>
              FT-AED: Benchmark Dataset for Early Freeway Traffic Anomalous Event Detection
              </a>
            </td>
          <td>
            Austin Coursey, Junyi Ji, Marcos Quiñones-Grueiro, William Barbour, Yuhang Zhang, Tyler Derr, Gautam Biswas, Dan Work
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>8</td>
        </tr>

        <tr id="Dynamic link prediction aims to predict future connections among unconnected nodes in a network. It can be applied for friend recommendations, link completion and other tasks. Network representation learning algorithms have demonstrated considerable effectiveness in various prediction tasks. However, most network representation learning algorithms are based on homogeneous networks and static networks for link prediction that do not consider rich semantic and dynamic information. Additionally, existing dynamic network representation learning methods neglect the neighborhood interaction structure of the node. In this work, we design a neighbor-enhanced dynamic heterogeneous attributed network embedding method (NeiDyHNE) for link prediction. In light of the impressive achievements of the heuristic methods, we learn the information of common neighbors and neighbors’ interaction in heterogeneous networks to preserve the neighbors proximity and common neighbors proximity. NeiDyHNE encodes the attributes and neighborhood structure of nodes as well as the evolutionary features of the dynamic network. More specifically, NeiDyHNE consists of the hierarchical structure attention module and the convolutional temporal attention module. The hierarchical structure attention module captures the rich features and semantic structure of nodes. The convolutional temporal attention module captures the evolutionary features of the network over time in dynamic heterogeneous networks. We evaluate our method and various baseline methods on the dynamic link prediction task. Experimental results demonstrate that our method is superior to baseline methods in terms of accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/db30350ef98aa15da12c11ae524452d4c7980c14" target='_blank'>
              Neighbor-enhanced Representation Learning for Link Prediction in Dynamic Heterogeneous Attributed Networks
              </a>
            </td>
          <td>
            Xiangyu Wei, Wei Wang, Chongsheng Zhang, Weiping Ding, Bin Wang, Yaguan Qian, Zhen Han, Chunhua Su
          </td>
          <td>2024-07-04</td>
          <td>ACM Transactions on Knowledge Discovery from Data</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="How to capture dynamic spatial-temporal dependencies remains an open question in multivariate time series (MTS) forecasting. Although recent advanced spatial-temporal graph neural networks (STGNNs) achieve superior forecasting performance, they either consider pre-defined spatial correlations or simply learn static graphs. Some research has tried to learn many adjacent matrices to reveal time-varying spatial correlations, but they generate discrete graphs which cannot encode evolutionary information and also face computational complexity problem. In this paper, we propose two significant plugins to help automatically learn enhanced dynamic spatial-temporal embedding of MTS data: (1) a novel neural conditional random field (CRF) layer. We find that the implicit time-varying spatial dependencies are reflected by the explicit changeable links between edges, and we propose the neural CRF to encode such pairwise changeable evolutionary inter-dependencies; (2) a structure adaptive graph convolution (SAGC) that does not require pre-defined graphs to capture semantically richer spatial correlations. Then, we integrate the neural CRF, SAGC with recurrent neural network to develop a new STGNN paradigm termed Adaptive Spatial-Temporal graph neural network with Conditional Random Field (ASTCRF), which can be trained in an end-to-end fashion. We validate the effectiveness, efficiency and scalability of ASTCRF on five public benchmark MTS datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/667940911f90cfd6980e5e1731731f7c2af49b08" target='_blank'>
              Dynamic Spatial-Temporal Embedding via Neural Conditional Random Field for Multivariate Time Series Forecasting
              </a>
            </td>
          <td>
            Peiyu Yi, Feihu Huang, Jian Peng, Zhifeng Bao
          </td>
          <td>2024-06-27</td>
          <td>ACM Transactions on Spatial Algorithms and Systems</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="High quality spatiotemporal signal is vitally important for real application scenarios like energy management, traffic planning and cyber security. Due to the uncontrollable factors like abrupt sensors breakdown or communication fault, the spatiotemporal signal collected by sensors is always incomplete. A dynamic graph convolutional network (DGCN) is effective for processing spatiotemporal signal recovery. However, it adopts a static GCN and a sequence neural network to explore the spatial and temporal patterns, separately. Such a separated two-step processing is loose spatiotemporal, thereby failing to capture the complex inner spatiotemporal correlation. To address this issue, this paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for spatiotemporal signal recovery with the following two-fold ideas: a) leveraging the tensor M-product to build a unified tensor graph convolution framework, which considers both spatial and temporal patterns simultaneously; and b) constructing a differential smoothness-based objective function to reduce the noise interference in spatiotemporal signal, thereby further improve the recovery accuracy. Experiments on real-world spatiotemporal datasets demonstrate that the proposed CDGCN significantly outperforms the state-of-the-art models in terms of recovery accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/301dc8dd3ee66671e8ffbb458a2544c71e3ee426" target='_blank'>
              A Differential Smoothness-based Compact-Dynamic Graph Convolutional Network for Spatiotemporal Signal Recovery
              </a>
            </td>
          <td>
            Pengcheng Gao, Zicheng Gao, Ye Yuan
          </td>
          <td>2024-08-06</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Long-term time series forecasting (LTSF) has been widely applied in finance, traffic prediction, and other domains. Recently, patch-based transformers have emerged as a promising approach, segmenting data into sub-level patches that serve as input tokens. However, existing methods mostly rely on predetermined patch lengths, necessitating expert knowledge and posing challenges in capturing diverse characteristics across various scales. Moreover, time series data exhibit diverse variations and fluctuations across different temporal scales, which traditional approaches struggle to model effectively. In this paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm to capture diverse receptive fields and sparse patterns of time series data. In order to build hierarchical receptive fields, we develop a multi-scale Transformer model, coupled with multi-scale sequence extraction, capable of capturing multi-resolution features. Additionally, we introduce a group-aware rotary position encoding technique to enhance intra- and inter-group position awareness among representations across different temporal scales. Our proposed model, named DRFormer, is evaluated on various real-world datasets, and experimental results demonstrate its superiority compared to existing methods. Our code is available at: https://github.com/ruixindingECNU/DRFormer.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/93c28e3a8cfde1fbb13658d2f625053edc878a02" target='_blank'>
              DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting
              </a>
            </td>
          <td>
            Ruixin Ding, Yuqi Chen, Yu-Ting Lan, Wei Zhang
          </td>
          <td>2024-08-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Graph neural networks (GNNs) are widely applied in graph data modeling. However, existing GNNs are often trained in a task-driven manner that fails to fully capture the intrinsic nature of the graph structure, resulting in sub-optimal node and graph representations. To address this limitation, we propose a novel Graph structure Prompt Learning method (GPL) to enhance the training of GNNs, which is inspired by prompt mechanisms in natural language processing. GPL employs task-independent graph structure losses to encourage GNNs to learn intrinsic graph characteristics while simultaneously solving downstream tasks, producing higher-quality node and graph representations. In extensive experiments on eleven real-world datasets, after being trained by GPL, GNNs significantly outperform their original performance on node classification, graph classification, and edge prediction tasks (up to 10.28%, 16.5%, and 24.15%, respectively). By allowing GNNs to capture the inherent structural prompts of graphs in GPL, they can alleviate the issue of over-smooth and achieve new state-of-the-art performances, which introduces a novel and effective direction for GNN research with potential applications in various domains.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7879caf6538fc087678244d45d2ba15cd1a2482" target='_blank'>
              Graph Structure Prompt Learning: A Novel Methodology to Improve Performance of Graph Neural Networks
              </a>
            </td>
          <td>
            Zhenhua Huang, Kunhao Li, Shaojie Wang, Zhaohong Jia, Wentao Zhu, Sharad Mehrotra
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Time series forecasting is one of the crucial tasks in many industrial fields and bears financial, resource, and control benefits. The leading solutions in this domain are often associated with machine learning approaches. The drawback in many approaches is the missing interpretability of the lookback and the predicted time series. Therefore, we propose a novel method named Bag-of-Future, to introduce interpretability in a backcast as well as forecast of the given series. Our approach aims to select and predict multiple patterns for the input series, based on continuous functions in a preselected bag of functions. Furthermore, stacking multiple encodings results in a continuous parameterization of the given dataset. The predicted bags are used to give precise forecasts and can unveil black box systems. Among synthetic as well as multiple industrial energy datasets we present competitive results in terms of performance, interpretability and pattern detection of the forecast as well as the given time series. Lastly, we present ablations on the main parameters of our approach, evaluated on multiple criterions to provide parameter recommendations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/354ecda4d141b4bec61e3c359ef1cf109f3f29c3" target='_blank'>
              Bag-of-Future: Time Series Forecasting by Continuous Function Candidates
              </a>
            </td>
          <td>
            Hendrik Klopries, Vignesh Chandramouli, Andreas Schwung
          </td>
          <td>2024-06-18</td>
          <td>2024 IEEE 33rd International Symposium on Industrial Electronics (ISIE)</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="The non-stationary nature of real-world Multivariate Time Series (MTS) data presents forecasting models with a formidable challenge of the time-variant distribution of time series, referred to as distribution shift. Existing studies on the distribution shift mostly adhere to adaptive normalization techniques for alleviating temporal mean and covariance shifts or time-variant modeling for capturing temporal shifts. Despite improving model generalization, these normalization-based methods often assume a time-invariant transition between outputs and inputs but disregard specific intra-/inter-series correlations, while time-variant models overlook the intrinsic causes of the distribution shift. This limits model expressiveness and interpretability of tackling the distribution shift for MTS forecasting. To mitigate such a dilemma, we present a unified Probabilistic Graphical Model to Jointly capturing intra-/inter-series correlations and modeling the time-variant transitional distribution, and instantiate a neural framework called JointPGM for non-stationary MTS forecasting. Specifically, JointPGM first employs multiple Fourier basis functions to learn dynamic time factors and designs two distinct learners: intra-series and inter-series learners. The intra-series learner effectively captures temporal dynamics by utilizing temporal gates, while the inter-series learner explicitly models spatial dynamics through multi-hop propagation, incorporating Gumbel-softmax sampling. These two types of series dynamics are subsequently fused into a latent variable, which is inversely employed to infer time factors, generate final prediction, and perform reconstruction. We validate the effectiveness and efficiency of JointPGM through extensive experiments on six highly non-stationary MTS datasets, achieving state-of-the-art forecasting performance of MTS forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c259b05da12062cdb1e96cd3994b71a1b4841e89" target='_blank'>
              Robust Multivariate Time Series Forecasting against Intra- and Inter-Series Transitional Shift
              </a>
            </td>
          <td>
            Hui He, Qi Zhang, Kun Yi, Xiaojun Xue, Shoujin Wang, Liang Hu, Longbin Cao
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Alongside the continuous process of improving AI performance through the development of more sophisticated models, researchers have also focused their attention to the emerging concept of data-centric AI, which emphasizes the important role of data in a systematic machine learning training process. Nonetheless, the development of models has also continued apace. One result of this progress is the development of the Transformer Architecture, which possesses a high level of capability in multiple domains such as Natural Language Processing (NLP), Computer Vision (CV) and Time Series Forecasting (TSF). Its performance is, however, heavily dependent on input data preprocessing and output data evaluation, justifying a data-centric approach to future research. We argue that data-centric AI is essential for training AI models, particularly for transformer-based TSF models efficiently. However, there is a gap regarding the integration of transformer-based TSF and data-centric AI. This survey aims to pin down this gap via the extensive literature review based on the proposed taxonomy. We review the previous research works from a data-centric AI perspective and we intend to lay the foundation work for the future development of transformer-based architecture and data-centric AI.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4df609838f79c84e7e68d38c32072bc0d41efb09" target='_blank'>
              Survey and Taxonomy: The Role of Data-Centric AI in Transformer-Based Time Series Forecasting
              </a>
            </td>
          <td>
            Jingjing Xu, Caesar Wu, Yuan-Fang Li, Grégoire Danoy, Pascal Bouvry
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="Anomaly subgraph detection is a crucial task in various real-world applications, including identifying high-risk areas, detecting river pollution, and monitoring disease outbreaks. Early traditional graph-based methods can obtain high-precision detection results in scenes with small-scale graphs and obvious anomaly features. Most existing anomaly detection methods based on deep learning primarily concentrate on identifying anomalies at the node level, while neglecting to detect anomaly groups in the internal structure. In this paper, we propose a novel end-to-end Graph Neural Network (GNN) based anomaly subgraph detection approach(ASD-HC) in graph-structured data. 1)We propose a high-order neighborhood sampling strategy to construct our node and k-order neighbor-subgraph instance pairs. 2)Anomaly features of nodes are captured through a self-supervised contrastive learning model. 3) Detecting the maximum connected anomaly subgraph is performed by integrating the Non-parameter Graph Scan statistics and a Random Walk module. We evaluate ASD-HC against five state-of-the-art baselines using five benchmark datasets. ASD-HC outperforms the baselines by over 13.01% in AUC score. Various experiments demonstrate that our approach effectively detects anomaly subgraphs within large-scale graphs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a0cba6d607f91786953354ed1a699981247bc70" target='_blank'>
              Anomaly Subgraph Detection through High-Order Sampling Contrastive Learning
              </a>
            </td>
          <td>
            Ying Sun, Wenjun Wang, Nannan Wu, Chunlong Bao
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper introduces BiLSTM-MLAM, a novel multi-scale time series prediction model. Initially, the approach utilizes bidirectional long short-term memory to capture information from both forward and backward directions in time series data. Subsequently, a multi-scale patch segmentation module generates various long sequences composed of equal-length segments, enabling the model to capture data patterns across multiple time scales by adjusting segment lengths. Finally, the local attention mechanism enhances feature extraction by accurately identifying and weighting important time segments, thereby strengthening the model’s understanding of the local features of the time series, followed by feature fusion. The model demonstrates outstanding performance in time series prediction tasks by effectively capturing sequence information across various time scales. Experimental validation illustrates the superior performance of BiLSTM-MLAM compared to six baseline methods across multiple datasets. When predicting the remaining life of aircraft engines, BiLSTM-MLAM outperforms the best baseline model by 6.66% in RMSE and 11.50% in MAE. In the LTE dataset, it achieves RMSE improvements of 12.77% and MAE enhancements of 3.06%, while in the load dataset, it demonstrates RMSE enhancements of 17.96% and MAE improvements of 30.39%. Additionally, ablation experiments confirm the positive impact of each module on prediction accuracy. Through segment length parameter tuning experiments, combining different segment lengths has resulted in lower prediction errors, affirming the effectiveness of the multi-scale fusion strategy in enhancing prediction accuracy by integrating information from multiple time scales.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ea6da7e837a4e1fc91974188bf6ea513a85abf72" target='_blank'>
              BiLSTM-MLAM: A Multi-Scale Time Series Prediction Model for Sensor Data Based on Bi-LSTM and Local Attention Mechanisms
              </a>
            </td>
          <td>
            Yongxin Fan, Qian Tang, Yangming Guo, Yifei Wei
          </td>
          <td>2024-06-01</td>
          <td>Sensors (Basel, Switzerland)</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Time series forecasting (TSF) plays a pivotal role in many real-world applications. Recently, the utilization of Large Language Models (LLM) in TSF has demonstrated exceptional predictive performance, surpassing most task-specific forecasting models. The success of LLM-based forecasting methods underscores the importance of causal dependence modeling and pre-trained knowledge transfer. However, challenges persist in directly applying LLM to TSF, i.e., the unacceptable parameter scales for resource-intensive model optimization, and the significant gap of feature space between structural numerical time series and natural language. To this end, we propose LeRet, a Language-empowered Retentive network for TSF. Technically, inspired by the causal extraction in LLM, we propose a causal dependence learner, enhanced by a patch-level pre-training task, to capture sequential causal evolution. To minimize the gap between numeric and language, we initialize a language description protocol for time series and design a TS-related language knowledge extractor to learn from language description, avoiding training with large-scale parameters. Finally, we dedicatedly achieve a Language-TS Modality Integrator for the fusion of two types data, and enable language-empowered sequence forecasting. Extensive evaluations demonstrate the effectiveness of our LeRet, especially reveal superiority on few-shot, and zero-shot forecasting tasks. Code is available at https://github.com/hqh0728/LeRet.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a08b1fc4c1621dc54a57b39a4eb7f9f19abbac63" target='_blank'>
              LeRet: Language-Empowered Retentive Network for Time Series Forecasting
              </a>
            </td>
          <td>
            Qihe Huang, Zhen-Qiang Zhou, Kuo Yang, Gengyu Lin, Zhongchao Yi, Yang Wang
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="In the field of machine learning and artificial intelligence, time series forecasting plays a pivotal role across various domains such as finance, healthcare, and weather. However, the task of selecting the most suitable forecasting method for a given dataset is a complex task due to the diversity of data patterns and characteristics. This research aims to address this challenge by proposing a comprehensive benchmark for evaluating and ranking time series forecasting methods across a wide range of datasets. This study investigates the comparative performance of many methods from two prominent time series forecasting frameworks, AutoGluon-Timeseries, and sktime to shed light on their applicability in different real-world scenarios. This research contributes to the field of time series forecasting by providing a robust benchmarking methodology and facilitating informed decision-making when choosing forecasting methods for achieving optimal prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/52869870f9e404f4e69a10b76ca410b00d19088d" target='_blank'>
              Can time series forecasting be automated? A benchmark and analysis
              </a>
            </td>
          <td>
            Anvitha Thirthapura Sreedhara, Joaquin Vanschoren
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Short fixed-length inputs are the main bottleneck of deep learning methods in long time-series forecasting tasks. Prolonging input length causes overfitting, rapidly deteriorating accuracy. Our research indicates that the overfitting is a combination reaction of the multi-scale pattern coupling in time series and the fixed focusing scale of current models. First, we find that the patterns exhibited by a time series across various scales are reflective of its multi-periodic nature, where each scale corresponds to specific period length. Second, We find that the token size predominantly dictates model behavior, as it determines the scale at which the model focuses and the context size it can accommodate. Our idea is to decouple the multi-scale temporal patterns of time series and to model each pattern with its corresponding period length as token size. We introduced a novel series-decomposition module(MPSD), and a Multi-Token Pattern Recognition neural network(MTPR), enabling the model to handle \textit{inputs up to $10\times$ longer}. Sufficient context enhances performance(\textit{38% maximum precision improvement}), and the decoupling approach offers \textit{Low complexity($0.22\times$ cost)} and \textit{high interpretability}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/380b86c3909fe3e098a4ec9618f09d59bb9ef626" target='_blank'>
              Long Input Sequence Network for Long Time Series Forecasting
              </a>
            </td>
          <td>
            Chao Ma, Yikai Hou, Xiang Li, Yinggang Sun, Haining Yu
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In response to the insufficient consideration of spatio-temporal dependencies and traffic pattern similarity in traffic flow prediction methods based on federated learning, as well as the neglect of model heterogeneity and objective heterogeneity, a traffic flow prediction model based on federated learning and spatio-temporal graph neural networks is proposed. The model is divided into two stages. In the road network division stage, the traffic road network is divided into subnetworks by the dynamic time warping algorithm and the K-means algorithm, to ensure the same subnetwork has the similar traffic flow pattern. The federated learning stage is divided into two sub-stages. In the local training phase, the spatio-temporal graph neural network with an attention mechanism is utilized to create personalized models and meme models to capture the spatio-temporal dependencies of each subnetwork. At the same time, deep mutual learning is utilized to address model heterogeneity and objective heterogeneity through knowledge distillation. In the global aggregation phase, a multi-factor weighted aggregation strategy is designed to measure the contribution of each local model to the global model, to enhance the fairness of aggregation. Three sets of experiments were conducted on two real datasets, and the experimental results demonstrate that the proposed model outperforms the baseline models in three common evaluation metrics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2e28c42dd920b2051aed0b2541a2f1897a0594b3" target='_blank'>
              Traffic Flow Prediction Based on Federated Learning and Spatio-Temporal Graph Neural Networks
              </a>
            </td>
          <td>
            Jian Feng, Cailing Du, Qi Mu
          </td>
          <td>2024-06-18</td>
          <td>ISPRS Int. J. Geo Inf.</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Analyzing sequential data is crucial in many domains, particularly due to the abundance of data collected from the Internet of Things paradigm. Time series classification, the task of categorizing sequential data, has gained prominence, with machine learning approaches demonstrating remarkable performance on public benchmark datasets. However, progress has primarily been in designing architectures for learning representations from raw data at fixed (or ideal) time scales, which can fail to generalize to longer sequences. This work introduces a \textit{compositional representation learning} approach trained on statistically coherent components extracted from sequential data. Based on a multi-scale change space, an unsupervised approach is proposed to segment the sequential data into chunks with similar statistical properties. A sequence-based encoder model is trained in a multi-task setting to learn compositional representations from these temporal components for time series classification. We demonstrate its effectiveness through extensive experiments on publicly available time series classification benchmarks. Evaluating the coherence of segmented components shows its competitive performance on the unsupervised segmentation task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0f550adb5e176a948a8df78a459210a01fa137a7" target='_blank'>
              Capturing Temporal Components for Time Series Classification
              </a>
            </td>
          <td>
            Venkata Ragavendra Vavilthota, Ranjith Ramanathan, Sathyanarayanan N. Aakur
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Stock prices, as an economic indicator, reflect changes in economic development and market conditions. Traditional stock price prediction models often only consider time-series data and are limited by the mechanisms of the models themselves. Some deep learning models have high computational costs, depend on a large amount of high-quality data, and have poor interpretations, making it difficult to intuitively understand the driving factors behind the predictions. Some studies have used deep learning models to extract text features and combine them with price data to make joint predictions, but there are issues with dealing with information noise, accurate extraction of text sentiment, and how to efficiently fuse text and numerical data. To address these issues in this paper, we propose a background-aware multi-source fusion financial trend forecasting mechanism. The system leverages a large language model to extract key information from policy and stock review texts, utilizing the MacBERT model to generate feature vectors. These vectors are then integrated with stock price data to form comprehensive feature representations. These integrated features are input into a neural network comprising various deep learning architectures. By integrating multiple data sources, the system offers a holistic view of market dynamics. It harnesses the comprehensive analytical and interpretative capabilities of large language models, retaining deep semantic and sentiment information from policy texts to provide richer input features for stock trend prediction. Additionally, we compare the accuracy of six models (LSTM, BiLSTM, MogrifierLSTM, GRU, ST-LSTM, SwinLSTM). The results demonstrate that our system achieves generally better accuracy in predicting stock movements, attributed to the incorporation of large language model processing, policy information, and other influential features.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/71bb0a6dfcebd80314575c81e752deb73c312df2" target='_blank'>
              Background-aware Multi-source Fusion Financial Trend Forecasting Mechanism
              </a>
            </td>
          <td>
            Fengting Mo, Shanshan Yan, Yinhao Xiao
          </td>
          <td>2024-07-01</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Time-series forecasting (TSF) finds broad applications in real-world scenarios. Due to the dynamic nature of time-series data, it is crucial to equip TSF models with out-of-distribution (OOD) generalization abilities, as historical training data and future test data can have different distributions. In this paper, we aim to alleviate the inherent OOD problem in TSF via invariant learning. We identify fundamental challenges of invariant learning for TSF. First, the target variables in TSF may not be sufficiently determined by the input due to unobserved core variables in TSF, breaking the conventional assumption of invariant learning. Second, time-series datasets lack adequate environment labels, while existing environmental inference methods are not suitable for TSF. To address these challenges, we propose FOIL, a model-agnostic framework that enables timeseries Forecasting for Out-of-distribution generalization via Invariant Learning. FOIL employs a novel surrogate loss to mitigate the impact of unobserved variables. Further, FOIL implements a joint optimization by alternately inferring environments effectively with a multi-head network while preserving the temporal adjacency structure, and learning invariant representations across inferred environments for OOD generalized TSF. We demonstrate that the proposed FOIL significantly improves the performance of various TSF models, achieving gains of up to 85%.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4abd3e56a26bd8301e7ac2f4cd4eccb543744471" target='_blank'>
              Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning
              </a>
            </td>
          <td>
            Haoxin Liu, Harshavardhan Kamarthi, Lingkai Kong, Zhiyuan Zhao, Chao Zhang, B. A. Prakash
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="With the remarkable success of generative models like ChatGPT, Artificial Intelligence Generated Content (AIGC) is undergoing explosive development. Not limited to text and images, generative models can generate industrial time series data, addressing challenges such as the difficulty of data collection and data annotation. Due to their outstanding generation ability, they have been widely used in Internet of Things, metaverse, and cyber-physical-social systems to enhance the efficiency of industrial production. In this paper, we present a comprehensive overview of generative models for industrial time series from deep generative models (DGMs) to large generative models (LGMs). First, a DGM-based AIGC framework is proposed for industrial time series generation. Within this framework, we survey advanced industrial DGMs and present a multi-perspective categorization. Furthermore, we systematically analyze the critical technologies required to construct industrial LGMs from four aspects: large-scale industrial dataset, LGMs architecture for complex industrial characteristics, self-supervised training for industrial time series, and fine-tuning of industrial downstream tasks. Finally, we conclude the challenges and future directions to enable the development of generative models in industry.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dc1ce4a6d24e82a1c9c05d41f69b1e606efa4115" target='_blank'>
              AIGC for Industrial Time Series: From Deep Generative Models to Large Generative Models
              </a>
            </td>
          <td>
            Lei Ren, Haiteng Wang, Yang Tang, Chunhua Yang
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="We introduce an anomaly detection method for multivariate time series data with the aim of identifying critical periods and features influencing extreme climate events like snowmelt in the Arctic. This method leverages the Variational Autoencoder (VAE) integrated with dynamic thresholding and correlation-based feature clustering. This framework enhances the VAE's ability to identify localized dependencies and learn the temporal relationships in climate data, thereby improving the detection of anomalies as demonstrated by its higher F1-score on benchmark datasets. The study's main contributions include the development of a robust anomaly detection method, improving feature representation within VAEs through clustering, and creating a dynamic threshold algorithm for localized anomaly detection. This method offers explainability of climate anomalies across different regions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ad66d86d0226f8a3056c3754e70d93854d591edb" target='_blank'>
              Harnessing Feature Clustering For Enhanced Anomaly Detection With Variational Autoencoder And Dynamic Threshold
              </a>
            </td>
          <td>
            Tolulope Ale, Nicole-Jeanne Schlegel, V. P. J. U. O. M. B. C. Usa, National Oceanic, Atmospheric Administration Geophysical Fluid Dynamics Laborato Usa
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Computer network anomaly detection and log analysis, as an important topic in the field of network security, has been a key task to ensure network security and system reliability. First, existing network anomaly detection and log analysis methods are often challenged by high-dimensional data and complex network topologies, resulting in unstable performance and high false-positive rates. In addition, traditional methods are usually difficult to handle time-series data, which is crucial for anomaly detection and log analysis. Therefore, we need a more efficient and accurate method to cope with these problems. To compensate for the shortcomings of current methods, we propose an innovative fusion model that integrates Isolation Forest, GAN (Generative Adversarial Network), and Transformer with each other, and each of them plays a unique role. Isolation Forest is used to quickly identify anomalous data points, and GAN is used to generate synthetic data with the real data distribution characteristics to augment the training dataset, while the Transformer is used for modeling and context extraction on time series data. The synergy of these three components makes our model more accurate and robust in anomaly detection and log analysis tasks. We validate the effectiveness of this fusion model in an extensive experimental evaluation. Experimental results show that our model significantly improves the accuracy of anomaly detection while reducing the false alarm rate, which helps to detect potential network problems in advance. The model also performs well in the log analysis task and is able to quickly identify anomalous behaviors, which helps to improve the stability of the system. The significance of this study is that it introduces advanced deep learning techniques, which work anomaly detection and log analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9d240b5aa22e310b31d52afd729a1195390da871" target='_blank'>
              Deep Learning-based Anomaly Detection and Log Analysis for Computer Networks
              </a>
            </td>
          <td>
            Shuzhan Wang, Ruxue Jiang, Zhaoqi Wang, Yan Zhou
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="Time series is a typical data type in numerous domains; however, labeling large amounts of time series data can be costly and time-consuming. Learning effective representation from unlabeled time series data is a challenging task. Contrastive learning stands out as a promising method to acquire representations of unlabeled time series data. Therefore, we propose a self-supervised time-series representation learning framework via Time-Frequency Fusion Contrasting (TF-FC) to learn time-series representation from unlabeled data. Specifically, TF-FC combines time-domain augmentation with frequency-domain augmentation to generate the diverse samples. For time-domain augmentation, the raw time series data pass through the time-domain augmentation bank (such as jitter, scaling, permutation, and masking) and get time-domain augmentation data. For frequency-domain augmentation, first, the raw time series undergoes conversion into frequency domain data following Fast Fourier Transform (FFT) analysis. Then, the frequency data passes through the frequency-domain augmentation bank (such as low pass filter, remove frequency, add frequency, and phase shift) and gets frequency-domain augmentation data. The fusion method of time-domain augmentation data and frequency-domain augmentation data is kernel PCA, which is useful for extracting nonlinear features in high-dimensional spaces. By capturing both the time and frequency domains of the time series, the proposed approach is able to extract more informative features from the data, enhancing the model's capacity to distinguish between different time series. To verify the effectiveness of the TF-FC method, we conducted experiments on four time series domain datasets (i.e., SleepEEG, HAR, Gesture, and Epilepsy). Experimental results show that TF-FC significantly improves in recognition accuracy compared with other SOTA methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/51190de29ec8ff369f3082560caa7c46fc2885b5" target='_blank'>
              Time-series representation learning via Time-Frequency Fusion Contrasting
              </a>
            </td>
          <td>
            Wenbo Zhao, Ling Fan
          </td>
          <td>2024-06-12</td>
          <td>Frontiers in Artificial Intelligence</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2cc142b8d9cab1725118000f6656fe235e85029e" target='_blank'>
              On data efficiency of univariate time series anomaly detection models
              </a>
            </td>
          <td>
            Wu Sun, Hui Li, Qingqing Liang, Xiaofeng Zou, Mei Chen, Yanhao Wang
          </td>
          <td>2024-06-11</td>
          <td>Journal of Big Data</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Evaluating the contribution of individual data points to a model's prediction is critical for interpreting model predictions and improving model performance. Existing data contribution methods have been applied to various data types, including tabular data, images, and texts; however, their primary focus has been on i.i.d. settings. Despite the pressing need for principled approaches tailored to time series datasets, the problem of estimating data contribution in such settings remains unexplored, possibly due to challenges associated with handling inherent temporal dependencies. This paper introduces TimeInf, a data contribution estimation method for time-series datasets. TimeInf uses influence functions to attribute model predictions to individual time points while preserving temporal structures. Our extensive empirical results demonstrate that TimeInf outperforms state-of-the-art methods in identifying harmful anomalies and helpful time points for forecasting. Additionally, TimeInf offers intuitive and interpretable attributions of data values, allowing us to easily distinguish diverse anomaly patterns through visualizations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3cf948c645144f6d8565608c19f9779114df6df2" target='_blank'>
              TimeInf: Time Series Data Contribution via Influence Functions
              </a>
            </td>
          <td>
            Yizi Zhang, Jingyan Shen, Xiaoxue Xiong, Yongchan Kwon
          </td>
          <td>2024-07-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Continuous-time dynamic graphs (CTDGs) are essential for modeling interconnected, evolving systems. Traditional methods for extracting knowledge from these graphs often depend on feature engineering or deep learning. Feature engineering is limited by the manual and time-intensive nature of crafting features, while deep learning approaches suffer from high inference latency, making them impractical for real-time applications. This paper introduces Deep-Graph-Sprints (DGS), a novel deep learning architecture designed for efficient representation learning on CTDGs with low-latency inference requirements. We benchmark DGS against state-of-the-art feature engineering and graph neural network methods using five diverse datasets. The results indicate that DGS achieves competitive performance while improving inference speed up to 12x compared to other deep learning approaches on our tested benchmarks. Our method effectively bridges the gap between deep representation learning and low-latency application requirements for CTDGs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2527f621a171b190f5a8d3aeb83a60fde586df1" target='_blank'>
              Deep-Graph-Sprints: Accelerated Representation Learning in Continuous-Time Dynamic Graphs
              </a>
            </td>
          <td>
            Ahmad Naser Eddin, Jacopo Bono, David Apar'icio, Hugo Ferreira, Pedro Ribeiro, P. Bizarro
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Purpose: This paper examines the most recent techniques for identifying irregularities in network data, with an emphasis upon machine learning (ML) and artificial intelligence (AI). Understanding how these technologies improve anomaly detection and overall network security is the goal of the study.Design/Methodology/Approach: A thorough examination of scholarly works, business analyses, and conference proceedings from the previous ten years was carried out. The study looks into supervised learning, unsupervised learning, and deep learning, among other AI and ML approaches. In order to evaluate these techniques' efficacy, advantages, and disadvantages in network anomaly detection, a comparative analysis was carried out.Findings/Results: The analysis shows that the identification of anomalies in network traffic is greatly enhanced by the use of AI and ML approaches. Methods such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have shown to be very successful in recognizing intricate patterns. Nonetheless, issues with data quality, computational complexity, and interpretability of models continue to exist.Originality/Value: This paper offers a current assessment of machine learning and artificial intelligence applications in network anomaly detection, emphasizing emerging trends and areas for further study. For academics and practitioners looking to improve network security using sophisticated detection methods, it provides insightful information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/00bf6357648a0821a69e6fbcbc768cc98aedd41c" target='_blank'>
              Advancements in Anomaly Detection Techniques in Network Traffic: The Role of Artificial Intelligence and Machine Learning
              </a>
            </td>
          <td>
            Vishnu Priya P M, Soumya S
          </td>
          <td>2024-06-25</td>
          <td>Journal of Scientific Research and Technology</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Non-Euclidean data, such as social networks and citation relationships between documents, have node and structural information. The Graph Convolutional Network (GCN) can automatically learn node features and association information between nodes. The core ideology of the Graph Convolutional Network is to aggregate node information by using edge information, thereby generating a new node feature. In updating node features, there are two core influencing factors. One is the number of neighboring nodes of the central node; the other is the contribution of the neighboring nodes to the central node. Due to the previous GCN methods not simultaneously considering the numbers and different contributions of neighboring nodes to the central node, we design the adaptive attention mechanism (AAM). To further enhance the representational capability of the model, we utilize Multi-Head Graph Convolution (MHGC). Finally, we adopt the cross-entropy (CE) loss function to describe the difference between the predicted results of node categories and the ground truth (GT). Combined with backpropagation, this ultimately achieves accurate node classification. Based on the AAM, MHGC, and CE, we contrive the novel Graph Adaptive Attention Network (GAAN). The experiments show that classification accuracy achieves outstanding performances on Cora, Citeseer, and Pubmed datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f922de3d996a26e2499025257b94a9e1b21c80c2" target='_blank'>
              Graph Adaptive Attention Network with Cross-Entropy
              </a>
            </td>
          <td>
            Zhao Chen
          </td>
          <td>2024-07-01</td>
          <td>Entropy</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Demand prediction is a crucial task for e-commerce and physical retail businesses, especially during high-stake sales events. However, the limited availability of historical data from these peak periods poses a significant challenge for traditional forecasting methods. In this paper, we propose a novel approach that leverages strategically chosen proxy data reflective of potential sales patterns from similar entities during non-peak periods, enriched by features learned from a graph neural networks (GNNs)-based forecasting model, to predict demand during peak events. We formulate the demand prediction as a meta-learning problem and develop the Feature-based First-Order Model-Agnostic Meta-Learning (F-FOMAML) algorithm that leverages proxy data from non-peak periods and GNN-generated relational metadata to learn feature-specific layer parameters, thereby adapting to demand forecasts for peak events. Theoretically, we show that by considering domain similarities through task-specific metadata, our model achieves improved generalization, where the excess risk decreases as the number of training tasks increases. Empirical evaluations on large-scale industrial datasets demonstrate the superiority of our approach. Compared to existing state-of-the-art models, our method demonstrates a notable improvement in demand prediction accuracy, reducing the Mean Absolute Error by 26.24% on an internal vending machine dataset and by 1.04% on the publicly accessible JD.com dataset.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fb0749b9bc04914e294f57c89199572e3cb5183c" target='_blank'>
              F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data
              </a>
            </td>
          <td>
            Zexing Xu, Linjun Zhang, Sitan Yang, Rasoul Etesami, Hanghang Tong, Huan Zhang, Jiawei Han
          </td>
          <td>2024-06-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This technical report describes the Time Series Optimized Transformer for Observability (Toto), a new state of the art foundation model for time series forecasting developed by Datadog. In addition to advancing the state of the art on generalized time series benchmarks in domains such as electricity and weather, this model is the first general-purpose time series forecasting foundation model to be specifically tuned for observability metrics. Toto was trained on a dataset of one trillion time series data points, the largest among all currently published time series foundation models. Alongside publicly available time series datasets, 75% of the data used to train Toto consists of fully anonymous numerical metric data points from the Datadog platform. In our experiments, Toto outperforms existing time series foundation models on observability data. It does this while also excelling at general-purpose forecasting tasks, achieving state-of-the-art zero-shot performance on multiple open benchmark datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/325e7ce8ff734f133cfa379bd8ac1021604c68aa" target='_blank'>
              Toto: Time Series Optimized Transformer for Observability
              </a>
            </td>
          <td>
            Ben Cohen, E. Khwaja, Kan Wang, Charles Masson, Elise Ram'e, Youssef Doubli, Othmane Abou-Amal
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="To overcome the challenges of feature selection in traditional machine learning and enhance the accuracy of deep learning methods for anomaly traffic detection, we propose a novel method called DCGCANet. This model integrates dilated convolution, a GRU, and a Channel Attention Network, effectively combining dilated convolutional structures with GRUs to extract both temporal and spatial features for identifying anomalous patterns in network traffic. The one-dimensional dilated convolution (DC-1D) structure is designed to expand the receptive field, allowing for comprehensive traffic feature extraction while minimizing information loss typically caused by pooling operations. The DC structure captures spatial dependencies in the data, while the GRU processes time series data to capture dynamic traffic changes. Furthermore, the channel attention (CA) module assigns importance-based weights to features in different channels, enhancing the model’s representational capacity and improving its ability to detect abnormal traffic. DCGCANet achieved an accuracy rate of 99.6% on the CIC-IDS-2017 dataset, outperforming other algorithms. Additionally, the model attained precision, recall, and F1 score rates of 99%. The generalization capability of DCGCANet was validated on a subset of CIC-IDS-2017, demonstrating superior detection performance and robust generalization potential.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/dfdc641a83f4cff536e56c91b2eff77336e1dfdd" target='_blank'>
              Network Traffic Anomaly Detection Based on Spatiotemporal Feature Extraction and Channel Attention
              </a>
            </td>
          <td>
            Changpeng Ji, Haofeng Yu, Wei Dai
          </td>
          <td>2024-07-07</td>
          <td>Processes</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Current Transformer methods for Multivariate Time-Series Forecasting (MTSF) are all based on the conventional attention mechanism. They involve sequence embedding and performing a linear projection of Q, K, and V, and then computing attention within this latent space. We have never delved into the attention mechanism to explore whether such a mapping space is optimal for MTSF. To investigate this issue, this study first proposes Frequency Spectrum attention (FSatten), a novel attention mechanism based on the frequency domain space. It employs the Fourier transform for embedding and introduces Multi-head Spectrum Scaling (MSS) to replace the conventional linear mapping of Q and K. FSatten can accurately capture the periodic dependencies between sequences and outperform the conventional attention without changing mainstream architectures. We further design a more general method dubbed Scaled Orthogonal attention (SOatten). We propose an orthogonal embedding and a Head-Coupling Convolution (HCC) based on the neighboring similarity bias to guide the model in learning comprehensive dependency patterns. Experiments show that FSatten and SOatten surpass the SOTA which uses conventional attention, making it a good alternative as a basic attention mechanism for MTSF. The codes and log files will be released at: https://github.com/Joeland4/FSatten-SOatten.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0b4ec3cddfc06c1f060d66a3123f8a9a0b157d23" target='_blank'>
              Revisiting Attention for Multivariate Time Series Forecasting
              </a>
            </td>
          <td>
            Haixiang Wu
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Financial assets exhibit complex dependency structures, which are crucial for investors to create diversified portfolios to mitigate risk in volatile financial markets. To explore the financial asset dependencies dynamics, we propose a novel approach that models the dependencies of assets as an Asset Dependency Matrix (ADM) and treats the ADM sequences as image sequences. This allows us to leverage deep learning-based video prediction methods to capture the spatiotemporal dependencies among assets. However, unlike images where neighboring pixels exhibit explicit spatiotemporal dependencies due to the natural continuity of object movements, assets in ADM do not have a natural order. This poses challenges to organizing the relational assets to reveal better the spatiotemporal dependencies among neighboring assets for ADM forecasting. To tackle the challenges, we propose the Asset Dependency Neural Network (ADNN), which employs the Convolutional Long Short-Term Memory (ConvLSTM) network, a highly successful method for video prediction. ADNN can employ static and dynamic transformation functions to optimize the representations of the ADM. Through extensive experiments, we demonstrate that our proposed framework consistently outperforms the baselines in the ADM prediction and downstream application tasks. This research contributes to understanding and predicting asset dependencies, offering valuable insights for financial market participants.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c51f53bdd4ab87d20b8e0a174f968f4dcf42a6a3" target='_blank'>
              Financial Assets Dependency Prediction Utilizing Spatiotemporal Patterns
              </a>
            </td>
          <td>
            Haoren Zhu, Pengfei Zhao, NG WilfredSiuHung, Dik Lun Lee
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Stock price forecasting is a challenging area of research, particularly due to the complexity and unpredictability of financial markets. The accuracy of prediction models is influenced by various factors, including nonlinearity, seasonality, and economic shocks. Deep learning has demonstrated better forecasts of stock prices than traditional approaches. This study, therefore, proposed a new approach to improve forecasting system based on an end-to-end convolutional recurrent neural network (CRNN) with attention mechanism. Our approach first investigates local stock price features using 1D convolutional neural network, and then employs a bidirectional long short-term memory (Bi-LSTM) network for forecasting. This model stands out by effectively utilizing contextual data and representing the temporal character of data. The Bi-LSTM is helpful for understanding the history and future contextual information since it uncovers both past and future contexts of stock data. Furthermore, integrating attention mechanism within the CRNN represents a significant improvement. This allows our model to handle long input sequences more effectively and capture the inherent stochasticity in stock prices, which is often missed by traditional models. The effectiveness of our approach is investigated using data on 10 stock indexes from Yahoo Finance. The results show that our method outperforms ARIMA, LSTM, and conventional methods. ">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d308e58a2b17825f4c41dbf390403ee62eb1a34c" target='_blank'>
              An improved convolutional recurrent neural network for stock price forecasting
              </a>
            </td>
          <td>
            Hoang Vuong Pham, Hung Phu Lam, Le Nhat Duy, T. Pham, T. Trinh
          </td>
          <td>2024-09-01</td>
          <td>IAES International Journal of Artificial Intelligence (IJ-AI)</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Accurate traffic forecasting is more necessary than ever for transportation departments, especially given its significant role in traffic planning, management, and control. However, most existing methods struggle to address complex spatial correlations on road networks, nonlinear temporal dynamics, and difficult long‐term prediction. This article proposes a novel spatial temporal graph gated transformer (STGGT) to overcome these challenges. The suggested model differs from Google's transformer because it uses a hybrid architecture that integrates graph convolutional networks (GCNs), attention, and gated recurrent units (GRUs) instead of solely relying on attention. Specifically, STGGT uses GCNs to extract spatial dependencies, utilizes attention and GRUs to extract temporal dependencies, and handle long‐term prediction. Experiments indicate that STGGT outperforms the state‐of‐the‐art baseline models on two real‐world traffic datasets of 9%–40%. The proposed model offers a promising solution for accurate traffic forecasting, simultaneously addressing the challenges of complex spatial correlations, nonlinear temporal dynamics, and long‐term prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/64e802ce9b30e768f218018adb39b56cd14ee339" target='_blank'>
              A spatial-temporal graph gated transformer for traffic forecasting
              </a>
            </td>
          <td>
            Haroun Bouchemoukha, M. Zennir, Ahmed Alioua
          </td>
          <td>2024-06-26</td>
          <td>Trans. Emerg. Telecommun. Technol.</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Real-time anomaly detection in smart grid networks is critical for ensuring the reliability and security of energy distribution systems. Traditional methods often struggle with the complexity and volume of data generated by these networks. This paper presents a novel deep learning-based approach that integrates Cross-Domain Generalization (CDG) and Multi-Task Learning (MTL) to enhance the detection of anomalies in smart grid data. By leveraging diverse datasets and iterative learning techniques, our method improves model robustness and generalization. Experimental results demonstrate significant improvements over baseline methods, showcasing the effectiveness of our approach. We provide comprehensive evaluations and discuss the broader implications for anomaly detection in industrial applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9b9d3142248c696841dcb9cfa2294843b178171b" target='_blank'>
              Real-Time Anomaly Detection in Smart Grid Networks Using Deep Learning with Cross-Domain Generalization and Multi-Task Learning
              </a>
            </td>
          <td>
            Michael Felser, Christian Moon, Thorne Stephenson
          </td>
          <td>2024-07-31</td>
          <td>International Journal of Mechanical and Electrical Engineering</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0084b07c9c1f87776c162af58347aa0ba6767f3d" target='_blank'>
              DyHANE: dynamic heterogeneous attributed network embedding through experience node replay
              </a>
            </td>
          <td>
            Liliana Martirano, D. Ienco, R. Interdonato, Andrea Tagarelli
          </td>
          <td>2024-07-05</td>
          <td>Appl. Netw. Sci.</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="The field of temporal graph learning aims to learn from evolving network data to forecast future interactions. Given a collection of observed temporal graphs, is it possible to predict the evolution of an unseen network from the same domain? To answer this question, we first present the Temporal Graph Scaling (TGS) dataset, a large collection of temporal graphs consisting of eighty-four ERC20 token transaction networks collected from 2017 to 2023. Next, we evaluate the transferability of Temporal Graph Neural Networks (TGNNs) for the temporal graph property prediction task by pre-training on a collection of up to sixty-four token transaction networks and then evaluating the downstream performance on twenty unseen token networks. We find that the neural scaling law observed in NLP and Computer Vision also applies in temporal graph learning, where pre-training on greater number of networks leads to improved downstream performance. To the best of our knowledge, this is the first empirical demonstration of the transferability of temporal graphs learning. On downstream token networks, the largest pre-trained model outperforms single model TGNNs on thirteen unseen test networks. Therefore, we believe that this is a promising first step towards building foundation models for temporal graphs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7296550cef5154a13eafc3eb00a54296113d8a8a" target='_blank'>
              Towards Neural Scaling Laws for Foundation Models on Temporal Graphs
              </a>
            </td>
          <td>
            Razieh Shirzadkhani, Tran Gia Bao Ngo, Kiarash Shamsi, Shenyang Huang, Farimah Poursafaei, Poupak Azad, Reihaneh Rabbany, Baris Coskunuzer, Guillaume Rabusseau, C. Akcora
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>19</td>
        </tr>

        <tr id="In the context of escalating safety concerns across various domains, the tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have emerged as critically important for applications in intelligent surveillance, evidence investigation, violence alerting, etc. These tasks, aimed at identifying and classifying deviations from normal behavior in video data, face significant challenges due to the rarity of anomalies which leads to extremely imbalanced data and the impracticality of extensive frame-level data annotation for supervised learning. This paper introduces a novel hierarchical graph neural network (GNN) based model MissionGNN that addresses these challenges by leveraging a state-of-the-art large language model and a comprehensive knowledge graph for efficient weakly supervised learning in VAR. Our approach circumvents the limitations of previous methods by avoiding heavy gradient computations on large multimodal models and enabling fully frame-level training without fixed video segmentation. Utilizing automated, mission-specific knowledge graph generation, our model provides a practical and efficient solution for real-time video analysis without the constraints of previous segmentation-based or multimodal approaches. Experimental validation on benchmark datasets demonstrates our model's performance in VAD and VAR, highlighting its potential to redefine the landscape of anomaly detection and recognition in video surveillance systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d953a0bd70b0f8735e2cbbd55e6af4ad3f759281" target='_blank'>
              MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation
              </a>
            </td>
          <td>
            Sanggeon Yun, Ryozo Masukawa, Minhyoung Na, Mohsen Imani
          </td>
          <td>2024-06-27</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="This study introduces a novel approach that integrates dynamic Bayesian network with attention based spatio-temporal graph convolutional network to forecast railway train delays, capturing the intricate operation interactions between train events and the dynamic evolution of train delays. Initially, train delay patterns are identified using the K-means clustering algorithm and incorporated as additional variables into the prediction model. To capture dynamic causality in delay propagation, we utilize a dynamic Bayesian network-based dynamic causality graph, incorporating train delay data and domain knowledge to effectively model the train delay propagation. Leveraging these insights on delay dynamic propagation, we propose an attention based spatio-temporal graph convolutional network that effectively models the dynamic spatio-temporal dependency among train events and enhances the accuracy of delay predictions. The proposed method is assessed using operational data from the Wuhan-Guangzhou high-speed railway. Results show that the proposed model outperforms the baseline models, particularly with the expansion of the prediction horizon. The learned dynamic causality of the train delay propagation enhances interpretability and results in a 6.97% reduction in mean absolute error. Furthermore, train delay patterns and weather variables contribute to a respective 12.85% and 4.37% reduction in mean absolute error. The statistical tests further validate the efficacy of the proposed model.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/31fbcd9b82bde6579a24f9baa5171614e4509e9c" target='_blank'>
              Bayesian Spatio-Temporal Graph Convolutional Network for Railway Train Delay Prediction
              </a>
            </td>
          <td>
            Jianmin Li, Xinyue Xu, Xin Ding, Jun Liu, Bin Ran
          </td>
          <td>2024-07-01</td>
          <td>IEEE Transactions on Intelligent Transportation Systems</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="In multivariate time series (MTS) forecasting, existing state-of-the-art deep learning approaches tend to focus on autoregressive formulations and overlook the information within exogenous indicators. To address this limitation, we present DeformTime, a neural network architecture that attempts to capture correlated temporal patterns from the input space, and hence, improve forecasting accuracy. It deploys two core operations performed by deformable attention blocks (DABs): learning dependencies across variables from different time steps (variable DAB), and preserving temporal dependencies in data from previous time steps (temporal DAB). Input data transformation is explicitly designed to enhance learning from the deformed series of information while passing through a DAB. We conduct extensive experiments on 6 MTS data sets, using previously established benchmarks as well as challenging infectious disease modelling tasks with more exogenous variables. The results demonstrate that DeformTime improves accuracy against previous competitive methods across the vast majority of MTS forecasting tasks, reducing the mean absolute error by 10% on average. Notably, performance gains remain consistent across longer forecasting horizons.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6c5ad359f1ca77ebaca62cae8263c48864b786d4" target='_blank'>
              DeformTime: Capturing Variable Dependencies with Deformable Attention for Time Series Forecasting
              </a>
            </td>
          <td>
            Yuxuan Shu, Vasileios Lampos
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="Time series analysis is pivotal for business and financial decision making, especially with the increasing integration of the Internet of Things (IoT). However, leveraging time series data for forecasting requires extensive preprocessing to address challenges such as missing values, heteroscedasticity, seasonality, outliers, and noise. Different approaches are necessary for univariate and multivariate time series, Gaussian and non-Gaussian time series, and stationary versus non-stationary time series. Handling missing data alone is complex, demanding unique solutions for each type. Extracting statistical features, identifying data quality issues, and selecting appropriate cleaning and forecasting techniques require significant effort, time, and expertise. To streamline this process, we propose an automated strategy called Preptimize, which integrates statistical and machine learning techniques and recommends prediction model blueprints, suggesting the most suitable approaches for a given dataset as an initial step towards further analysis. Preptimize reads a sample from a large dataset and recommends the blueprint model based on optimization, making it easy to use even for non-experts. The results of various experiments indicated that Preptimize either outperformed or had comparable performance to benchmark models across multiple sectors, including stock prices, cryptocurrency, and power consumption prediction. This demonstrates the framework’s effectiveness in recommending suitable prediction models for various time series datasets, highlighting its broad applicability across different domains in time series forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/228a2e2a0f2984f0be41b8a4718860d9d8b020d5" target='_blank'>
              Preptimize: Automation of Time Series Data Preprocessing and Forecasting
              </a>
            </td>
          <td>
            Mehak Usmani, Z. Memon, Adil Zulfiqar, Rizwan Qureshi
          </td>
          <td>2024-08-01</td>
          <td>Algorithms</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Time series data complexity presents new challenges in clustering analysis across fields such as electricity, energy, industry, and finance. Despite advances in representation learning and clustering with Variational Autoencoders (VAE) based deep learning techniques, issues like the absence of discriminative power in feature representation, the disconnect between instance reconstruction and clustering objectives, and scalability challenges with large datasets persist. This paper introduces a novel deep time series clustering approach integrating VAE with metric learning. It leverages a VAE based on Gated Recurrent Units for temporal feature extraction, incorporates metric learning for joint optimization of latent space representation, and employs the sum of log likelihoods as the clustering merging criterion, markedly improving clustering accuracy and interpretability. Experimental findings demonstrate a 27.16% improvement in average clustering accuracy and a 47.15% increase in speed on industrial load data. This study offers novel insights and tools for the thorough analysis and application of time series data, with further exploration of VAE’s potential in time series clustering anticipated in future research.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5c8bdd5e4eb0940f762713b94bd43e70c528c42c" target='_blank'>
              Research on load clustering algorithm based on variational autoencoder and hierarchical clustering
              </a>
            </td>
          <td>
            Miaozhuang Cai, Yin Zheng, Zhengyang Peng, Chunyan Huang, Haoxia Jiang
          </td>
          <td>2024-06-13</td>
          <td>PLOS ONE</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Graph neural networks (GNNs) excel in learning from network-like data but often lack interpretability, making their application challenging in domains requiring transparent decision-making. We propose the Graph Kolmogorov-Arnold Network (GKAN), a novel GNN model leveraging spline-based activation functions on edges to enhance both accuracy and interpretability. Our experiments on five benchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN models in node classification, link prediction, and graph classification tasks. In addition to the improved accuracy, GKAN's design inherently provides clear insights into the model's decision-making process, eliminating the need for post-hoc explainability techniques. This paper discusses the methodology, performance, and interpretability of GKAN, highlighting its potential for applications in domains where interpretability is crucial.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ce82880f5527b26fdb12cd1fa13d4cbc45c9e012" target='_blank'>
              Kolmogorov-Arnold Graph Neural Networks
              </a>
            </td>
          <td>
            Gianluca De Carlo, A. Mastropietro, Aris Anagnostopoulos
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>4</td>
          <td>4</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6fbaa841f1f012a9be06d56da6a1921f5d50f305" target='_blank'>
              MGLEP: Multimodal Graph Learning for Modeling Emerging Pandemics with Big Data
              </a>
            </td>
          <td>
            Khanh-Tung Tran, T. Hy, Lili Jiang, Xuan-Son Vu
          </td>
          <td>2023-10-23</td>
          <td>Scientific Reports</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Traffic forecasting is crucial for smart cities and intelligent transportation initiatives, where deep learning has made significant progress in modeling complex spatio-temporal patterns in recent years. However, current public datasets have limitations in reflecting the ultra-dynamic nature of real-world scenarios, characterized by continuously evolving infrastructures, varying temporal distributions, and temporal gaps due to sensor downtimes or changes in traffic patterns. These limitations inevitably restrict the practical applicability of existing traffic forecasting datasets. To bridge this gap, we present XXLTraffic, the largest available public traffic dataset with the longest timespan and increasing number of sensor nodes over the multiple years observed in the data, curated to support research in ultra-dynamic forecasting. Our benchmark includes both typical time-series forecasting settings with hourly and daily aggregated data and novel configurations that introduce gaps and down-sample the training size to better simulate practical constraints. We anticipate the new XXLTraffic will provide a fresh perspective for the time-series and traffic forecasting communities. It would also offer a robust platform for developing and evaluating models designed to tackle ultra-dynamic and extremely long forecasting problems. Our dataset supplements existing spatio-temporal data resources and leads to new research directions in this domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/11aaf60abe4ce0825c53f16f0d34e6f3d8d20624" target='_blank'>
              XXLTraffic: Expanding and Extremely Long Traffic Dataset for Ultra-Dynamic Forecasting Challenges
              </a>
            </td>
          <td>
            Du Yin, Hao Xue, Arian Prabowo, Shuang Ao, Flora D. Salim
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="Time series models, typically trained on numerical data, are designed to forecast future values. These models often rely on weighted averaging techniques over time intervals. However, real-world time series data is seldom isolated and is frequently influenced by non-numeric factors. For instance, stock price fluctuations are impacted by daily random events in the broader world, with each event exerting a unique influence on price signals. Previously, forecasts in financial markets have been approached in two main ways: either as time-series problems over price sequence or sentiment analysis tasks. The sentiment analysis tasks aim to determine whether news events will have a positive or negative impact on stock prices, often categorizing them into discrete labels. Recognizing the need for a more comprehensive approach to accurately model time series prediction, we propose a collaborative modeling framework that incorporates textual information about relevant events for predictions. Specifically, we leverage the intuition of large language models about future changes to update real number time series predictions. We evaluated the effectiveness of our approach on financial market data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7c2fd193f7d8f4fb40c5f865884bd2b528a475a4" target='_blank'>
              Text2TimeSeries: Enhancing Financial Forecasting through Time Series Prediction Updates with Event-Driven Insights from Large Language Models
              </a>
            </td>
          <td>
            Litton J. Kurisinkel, Pruthwik Mishra, Yue Zhang
          </td>
          <td>2024-07-04</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Accurate traffic flow forecasting is vital for intelligent transportation systems, especially with urbanization worsening traffic congestion, which affects daily life, economic growth, and the environment. Precise forecasts aid in managing and optimizing transportation systems, reducing congestion, and improving air quality by cutting emissions. However, predicting outcomes is difficult due to intricate spatial relationships, nonlinear temporal patterns, and the challenges associated with long-term forecasting. Current research often uses static graph structures, overlooking dynamic and long-range dependencies. To tackle these issues, we introduce the spatiotemporal dynamic multi-hop network (ST-DMN), a Seq2Seq framework. This model incorporates spatiotemporal convolutional blocks (ST-Blocks) with residual connections in the encoder to condense historical traffic data into a fixed-dimensional vector. A dynamic graph represents time-varying inter-segment relationships, and multi-hop operation in the encoder’s spatial convolutional layer and the decoder’s diffusion multi-hop graph convolutional gated recurrent units (DMGCGRUs) capture long-range dependencies. Experiments on two real-world datasets METR-LA and PEMS-BAY show that ST-DMN surpasses existing models in three metrics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a95e1e501ec2f4452c161c53c744ae576c08979c" target='_blank'>
              Spatiotemporal Dynamic Multi-Hop Network for Traffic Flow Forecasting
              </a>
            </td>
          <td>
            Wenguang Chai, Qingfeng Luo, Zhizhe Lin, Jingwen Yan, Jinglin Zhou, Teng Zhou
          </td>
          <td>2024-07-09</td>
          <td>Sustainability</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="The Variable Subset Forecasting (VSF) problem, where the majority of variables are unavailable in the inference stage of multivariate forecasting, has been an important but under-explored task with broad impacts in many real-world applications. Missing values, absent inter-correlation, and the impracticality of retraining largely hinder the ability of multivariate forecasting models to capture inherent relationships among variables, impacting their performance. However, existing approaches towards these issues either heavily rely on local temporal correlation or face limitations in fully recovering missing information from the unavailable subset, accompanied by notable computational expenses. To address these problems, we propose a novel density estimation solution to recover the information of missing variables via flows-based generative framework. In particular, a novel generative network for time series, namely Time-series Reconstruction Flows (TRF), is proposed to estimate and reconstruct the missing variable subset. In addition, a novel meta-training framework, Variable-Agnostic Meta Learning, has been developed to enhance the generalization ability of TRF, enabling it to adapt to diverse missing variables situations. Finally, extensive experiments are conducted to demonstrate the superiority of our proposed method compared with baseline methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5dcee67fc3a8091cc9cc24057f5c0b9c933f4044" target='_blank'>
              Reconstructing Missing Variables for Multivariate Time Series Forecasting via Conditional Generative Flows
              </a>
            </td>
          <td>
            Xuanming Hu, Wei Fan, Haifeng Chen, Pengyang Wang, Yanjie Fu
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Transformer-based methods have recently demonstrated their potential in time series forecasting problems. However, the mainstream approach, primarily utilizing attention to model inter-step correlation in the time domain, is constrained by two significant issues that lead to ineffective and inefficient multivariate forecasting. The first is that key representations in the time domain are scattered and sparse, resulting in parameter bloat and increased difficulty in capturing time dependencies. The second is that treating time step points as uniformly embedded tokens leads to the erasure of inter-variate correlations. To address these challenges, we propose a frequency-wise and variables-oriented transformer-based method. This method leverages the intrinsic conjugate symmetry in the frequency domain, enabling compact frequency domain representations that naturally mix information across time points while reducing spatio-temporal costs. Multivariate inter-correlations can also be captured from similar frequency domain components, which enhances the variables-oriented attention mechanism modeling capability. Further, we employ both polar and complex domain perspectives to enrich the frequency domain representations and decode complicated temporal patterns. We propose frequency-enhanced independent representation multi-head attention (FIR-Attention) to leverage these advantages for improved multivariate interaction. Techniques such as cutting-off frequency and equivalent mapping are used to ensure the model’s lightweight nature. Extensive experiments on eight mainstream datasets show that our approach achieves first-rate satisfactory results and, importantly, requires only one percent of the spatio-temporal cost of mainstream methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/36f87cbd5187e521184ddb319b132292cb8ae608" target='_blank'>
              Frequency-Enhanced Transformer with Symmetry-Based Lightweight Multi-Representation for Multivariate Time Series Forecasting
              </a>
            </td>
          <td>
            Chenyue Wang, Zhouyuan Zhang, Xin Wang, Mingyang Liu, Lin Chen, Jiatian Pi
          </td>
          <td>2024-06-25</td>
          <td>Symmetry</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="In this paper, we leverage the power of latent diffusion models to generate synthetic time series tabular data. Along with the temporal and feature correlations, the heterogeneous nature of the feature in the table has been one of the main obstacles in time series tabular data modeling. We tackle this problem by combining the ideas of the variational auto-encoder (VAE) and the denoising diffusion probabilistic model (DDPM). Our model named as \texttt{TimeAutoDiff} has several key advantages including (1) Generality: the ability to handle the broad spectrum of time series tabular data from single to multi-sequence datasets; (2) Good fidelity and utility guarantees: numerical experiments on six publicly available datasets demonstrating significant improvements over state-of-the-art models in generating time series tabular data, across four metrics measuring fidelity and utility; (3) Fast sampling speed: entire time series data generation as opposed to the sequential data sampling schemes implemented in the existing diffusion-based models, eventually leading to significant improvements in sampling speed, (4) Entity conditional generation: the first implementation of conditional generation of multi-sequence time series tabular data with heterogenous features in the literature, enabling scenario exploration across multiple scientific and engineering domains. Codes are in preparation for release to the public, but available upon request.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/78a04be6d94e77047c3251e4480cd08d1194dbd1" target='_blank'>
              TimeAutoDiff: Combining Autoencoder and Diffusion model for time series tabular data synthesizing
              </a>
            </td>
          <td>
            Namjoon Suh, Yuning Yang, Din-Yin Hsieh, Qitong Luan, Shi Xu, Shixiang Zhu, Guang Cheng
          </td>
          <td>2024-06-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="In recent years, transformer-based models have gained prominence in multivariate long-term time series forecasting (LTSF), demonstrating significant advancements despite facing challenges such as high computational demands, difficulty in capturing temporal dynamics, and managing long-term dependencies. The emergence of LTSF-Linear, with its straightforward linear architecture, has notably outperformed transformer-based counterparts, prompting a reevaluation of the transformer's utility in time series forecasting. In response, this paper presents an adaptation of a recent architecture termed extended LSTM (xLSTM) for LTSF. xLSTM incorporates exponential gating and a revised memory structure with higher capacity that has good potential for LTSF. Our adopted architecture for LTSF termed as xLSTMTime surpasses current approaches. We compare xLSTMTime's performance against various state-of-the-art models across multiple real-world da-tasets, demonstrating superior forecasting capabilities. Our findings suggest that refined recurrent architectures can offer competitive alternatives to transformer-based models in LTSF tasks, po-tentially redefining the landscape of time series forecasting.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/286a643cebfa8149f02ead76b0be31caa7d8735f" target='_blank'>
              xLSTMTime : Long-term Time Series Forecasting With xLSTM
              </a>
            </td>
          <td>
            Musleh Alharthi, Ausif Mahmood
          </td>
          <td>2024-07-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Sensor data collected from climate stations has been used in various scientific applications and environmental monitoring. Maintaining the data quality is essential to guarantee the reliability and accuracy of science outputs, potentially impacting many critical decision making processes. Existing sensor anomaly detection techniques are mostly designed for general purposes, and may not be suitable for climate sensors which require complex handling of seasonality, spatial relationship and sensor interdependency. Current quality control process is deficient in climate sensor drift detection, which is a slow degradation of sensor accuracy over time. Recent development of anomaly detection in climate sensor domain is limited, it’s often constrained to particular sensor types, and not focused on drift detection. In this paper, we present a new drift-aware time series anomaly detection framework which leverages the spatial-temporal correlation of the climate sensor network and significantly improves climate sensor drift detection capability. Moreover, the proposed semi- supervised learning approach helps to generalise the solution for various types of sensors and anomalies. Our experiments using real-world dataset have demonstrated promising and competitive performance in regards to sensitivity, false alarm control, and computational efficiency suitable for real-time or near-real-time applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/89a3de8e95c92c6cbdb5ccd07c334244703c9ba7" target='_blank'>
              Sensor-Drift-Aware Time-Series Anomaly Detection for Climate Stations
              </a>
            </td>
          <td>
            Bryce Chen, V. Huang, Chen Wang
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE Conference on Artificial Intelligence (CAI)</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="By exploring complex graph information hidden in data from multiple views, multi-view clustering based on graph neural network significantly enhances the clustering performance and has drawn increasing attention in recent years. Although considerable progress has been made, most existing GNN based MVC models merely consider the explicit presence of graph structure in raw data and ignore that latent graphs of different views also provide specific information for the clustering task. We propose dynamic weighted graph fusion for deep multi-view clustering (DFMVC) to address this issue. Specifically, DFMVC learns embedded features via deep autoencoders and then constructs latent graphs for each individual view. Then, it concatenates the embedded features of all views to form a global feature to leverage complementary information, as well as generates a fusion graph via combining all latent graphs to accurately capture the topological information among samples. Based on the informative fusion graph and global features, the graph convolution module is adopted to derive a representation with global comprehensive information, which is further used to generate pseudo-label information. In a self-supervised manner, such information guides each view to dynamically learn discriminative features and latent graphs. Extensive experimental results demonstrate the efficacy of DFMVC.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a7f5a7584d160c0aeb23438010e9889e183b3d7a" target='_blank'>
              Dynamic Weighted Graph Fusion for Deep Multi-View Clustering
              </a>
            </td>
          <td>
            Yazhou Ren, Jingyu Pu, Chenhang Cui, Yan Zheng, Xinyue Chen, X. Pu, Lifang He
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="It is challenging to scale time series forecasting models such that they forecast accurately for multiple distinct domains and datasets, all with potentially different underlying collection procedures (e.g., sample resolution), patterns (e.g., periodicity), and prediction requirements (e.g., reconstruction vs. forecasting). We call this general task universal forecasting. Existing methods usually assume that input data is regularly sampled, and they forecast to pre-determined horizons, resulting in failure to generalise outside of the scope of their training. We propose the DAM - a neural model that takes randomly sampled histories and outputs an adjustable basis composition as a continuous function of time for forecasting to non-fixed horizons. It involves three key components: (1) a flexible approach for using randomly sampled histories from a long-tail distribution, that enables an efficient global perspective of the underlying temporal dynamics while retaining focus on the recent history; (2) a transformer backbone that is trained on these actively sampled histories to produce, as representational output, (3) the basis coefficients of a continuous function of time. We show that a single univariate DAM, trained on 25 time series datasets, either outperformed or closely matched existing SoTA models at multivariate long-term forecasting across 18 datasets, including 8 held-out for zero-shot transfer, even though these models were trained to specialise for each dataset-horizon combination. This single DAM excels at zero-shot transfer and very-long-term forecasting, performs well at imputation, is interpretable via basis function composition and attention, can be tuned for different inference-cost requirements, is robust to missing and irregularly sampled data {by design}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e177ce13d5e3ca4673b2fff9481408cd98bba0d3" target='_blank'>
              DAM: Towards A Foundation Model for Time Series Forecasting
              </a>
            </td>
          <td>
            Luke Darlow, Qiwen Deng, Ahmed Hassan, Martin Asenov, Rajkarn Singh, Artjom Joosen, Adam Barker, A. Storkey
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>43</td>
        </tr>

        <tr id="Time-series classification is vital in health monitoring and human activity recognition, as well as in areas such as financial forecasting, process control, and a wide array of forecasting tasks. Traditional time-series models segment data into windows and assign one label per window, often missing label transitions within those windows. This paper presents a novel many-to-many time-series model and post-processing using hybrid recurrent neural networks with attention mechanisms, which more effectively captures label transitions over traditional many-to-one models. Further, unlike typical other many-to-many models, our approach doesn’t require a decoder. Instead, it employs an RNN, generating a label for every input time step. During inference, a weighted voting scheme consolidates overlapping predictions into one label per time step. Experiments show our model remains effective on time-series with sparse label shifts, but particularly excels in detecting frequent transitions. This model is ideal for tasks demanding accurate pinpointing of rapid label changes in time-series data, such as gesture recognition, making it ideal for fast-paced human activity recognition. 1">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d206e88758c3992f96eaf77f9ddd4f0e63779b32" target='_blank'>
              Many-to-Many Prediction for Effective Modeling of Frequent Label Transitions in Time Series
              </a>
            </td>
          <td>
            Alexander Katrompas, V. Metsis
          </td>
          <td>2024-06-26</td>
          <td>Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="The telecommunications industry is becoming increasingly aware of potential subscriber churn as a result of the growing popularity of smartphones in the mobile Internet era, the quick development of telecommunications services, the implementation of the number portability policy, and the intensifying competition among operators. At the same time, users' consumption preferences and choices are evolving. Excellent churn prediction models must be created in order to accurately predict the churn tendency, since keeping existing customers is far less expensive than acquiring new ones. But conventional or learning-based algorithms can only go so far into a single subscriber's data; they cannot take into consideration changes in a subscriber's subscription and ignore the coupling and correlation between various features. Additionally, the current churn prediction models have a high computational burden, a fuzzy weight distribution, and significant resource economic costs. The prediction algorithms involving network models currently in use primarily take into account the private information shared between users with text and pictures, ignoring the reference value supplied by other users with the same package. This work suggests a user churn prediction model based on Graph Attention Convolutional Neural Network (GAT-CNN) to address the aforementioned issues. The main contributions of this paper are as follows: Firstly, we present a three-tiered hierarchical cloud-edge cooperative framework that increases the volume of user feature input by means of two aggregations at the device, edge, and cloud layers. Second, we extend the use of users' own data by introducing self-attention and graph convolution models to track the relative changes of both users and packages simultaneously. Lastly, we build an integrated offline-online system for churn prediction based on the strengths of the two models, and we experimentally validate the efficacy of cloud-side collaborative training and inference. In summary, the churn prediction model based on Graph Attention Convolutional Neural Network presented in this paper can effectively address the drawbacks of conventional algorithms and offer telecom operators crucial decision support in developing subscriber retention strategies and cutting operational expenses.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/389c1f686431f303fa0095054fe8c8ecdb8f4c61" target='_blank'>
              User churn prediction hierarchical model based on graph attention convolutional neural networks
              </a>
            </td>
          <td>
            Mei Miao, Tang Miao, Zhou Long
          </td>
          <td>2024-07-01</td>
          <td>China Communications</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Graph neural networks (GNNs) for graph classification or representation learning require a pooling operation to convert the nodes' embeddings of each graph to a vector as the graph-level representation and the operation has a significant impact on model accuracy. The paper presents a novel graph pooling method called Kernel Readout (KerRead). KerRead maps the node embeddings from the sample space with limited nodes to an augmented sample space with infinite nodes, and then calculates the inner product between some learnable adaptive centers and the augmented node embeddings, which forms a final graph-level feature vector. We apply the proposed strategy to six supervised and two unsupervised graph neural networks such as GCN, GIN, GUNet, InfoGraph, and GraphCL, and the experiments on eight benchmark datasets show that the proposed readout outperforms classical pooling methods such as Sum and seven state-of-the-art pooling methods such as SRead and Janossy GRU. Code and Appendix are both available at https://github.com/jiajunCAU/KerRead.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/eb7eaa32c2656851204eea6a4172f373fc0c723a" target='_blank'>
              Kernel Readout for Graph Neural Networks
              </a>
            </td>
          <td>
            Jiajun Yu, Zhihao Wu, Jinyu Cai, Adele Lu Jia, Jicong Fan
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="High-frequency trading (HFT) has transformed financial markets by enabling rapid execution of trades, exploiting market inefficiencies, and optimizing trading strategies. However, this speed and complexity also present significant challenges for real-time fraud detection. Deep learning, a subset of machine learning, offers promising solutions to these challenges through its ability to analyze large volumes of data and uncover intricate patterns. This review explores the conceptual challenges and solutions associated with deploying deep learning for fraud detection in HFT environments. One of the primary challenges in implementing deep learning for HFT fraud detection is the sheer volume and velocity of data. HFT systems generate vast amounts of transactional data in milliseconds, necessitating highly efficient and scalable deep learning models. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are particularly suited for this task due to their ability to process and analyze sequential data efficiently. However, these models require substantial computational resources and sophisticated infrastructure to operate in real time. Another significant challenge is the need for high accuracy and low latency in fraud detection. False positives can lead to unnecessary interventions, while false negatives can result in undetected fraudulent activities. Deep learning models must be fine-tuned to balance these risks, employing techniques such as hyperparameter optimization and ensemble learning to enhance their predictive capabilities. Additionally, integrating real-time anomaly detection methods can help identify suspicious activities promptly, reducing the window of opportunity for fraudsters. Data quality and integrity also pose substantial challenges. HFT environments are susceptible to noise and outliers, which can distort model predictions. Ensuring high-quality data through rigorous preprocessing and anomaly filtering is crucial for the accuracy of deep learning models. Techniques such as data augmentation and normalization can further improve model robustness. To address these challenges, a hybrid approach combining deep learning with traditional statistical methods and rule-based systems can be effective. This approach leverages the strengths of each method, providing a comprehensive fraud detection framework that is both accurate and responsive. Additionally, ongoing model retraining and adaptation to evolving fraud patterns are essential to maintain the effectiveness of the system. In conclusion, while deep learning presents significant opportunities for enhancing real-time fraud detection in high-frequency trading, it also requires addressing challenges related to data volume, computational demands, accuracy, and data quality. By employing a hybrid approach and continually refining models, financial institutions can effectively mitigate fraud risks and safeguard their trading operations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/901f480e44654bcf7bf34ae745f63741f584cdcd" target='_blank'>
              Deep learning in high-frequency trading: Conceptual challenges and solutions for real-time fraud detection
              </a>
            </td>
          <td>
            Halima Oluwabunmi, Halima Oluwabunmi Bello, Adebimpe Bolatito, Maxwell Nana Ameyaw
          </td>
          <td>2024-07-30</td>
          <td>World Journal of Advanced Engineering Technology and Sciences</td>
          <td>2</td>
          <td>0</td>
        </tr>

        <tr id="Deep graph clustering (DGC) has been a promising method for clustering graph data in recent years. However, existing research primarily focuses on optimizing clustering outcomes by improving the quality of embedded representations, resulting in slow-speed complex models. Additionally, these methods do not consider changes in node similarity and corresponding adjustments in the original structure during the iterative optimization process after updating node embeddings, which easily falls into the representation collapse issue. We introduce an Efficient Graph Auto-Encoder and a dynamic graph weight updating strategy to address these issues, forming the basis for our proposed Fast Deep Graph Clustering (FastDGC) network. Specifically, we significantly reduce feature dimensions using a linear transformation that preserves the original node similarity. We then employ a single-layer graph convolutional filtering approximation to replace multiple layers of graph convolutional neural network, reducing computational complexity and parameter count. During iteration, we calculate the similarity between nodes using the linearly transformed features and periodically update the original graph structure to reduce edges with low similarity, thereby enhancing the learning of discriminative and cohesive representations. Theoretical analysis confirms that EGAE has lower computational complexity. Extensive experiments on standard datasets demonstrate that our proposed method improves clustering performance and achieves a speedup of 2~3 orders of magnitude compared to state-of-the-art methods, showcasing outstanding performance. The code for our model is available at https://github.com/Marigoldwu/FastDGC. Furthermore, we have organized a portion of the deep graph clustering code into a unified framework, available at https://github.com/Marigoldwu/A-Unified-Framework-for-Deep-Attribute-Graph-Clustering.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e865f36946ec02117a8329f82b69445e265d6229" target='_blank'>
              Towards Faster Deep Graph Clustering via Efficient Graph Auto-Encoder
              </a>
            </td>
          <td>
            Shifei Ding, Benyu Wu, Ling Ding, Xiao Xu, Lili Guo, Hongmei Liao, Xindong Wu
          </td>
          <td>2024-06-28</td>
          <td>ACM Transactions on Knowledge Discovery from Data</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="Multivariate time series modeling has been essential in sensor-based data mining tasks. However, capturing complex dynamics caused by intra-variable (temporal) and inter-variable (spatial) relationships while simultaneously taking into account evolving data distributions is a non-trivial task, which faces accumulated computational overhead and multiple temporal patterns or distribution modes. Most existing methods focus on the former direction without adaptive task-specific learning ability. To this end, we developed a holistic spatial-temporal meta-learning probabilistic inference framework, entitled ST-MeLaPI, for the efficient and versatile learning of complex dynamics. Specifically, first, a multivariate relationship recognition module is utilized to learn task-specific inter-variable dependencies. Then, a multiview meta-learning and probabilistic inference strategy was designed to learn shared parameters while enabling the fast and flexible learning of task-specific parameters for different batches. At the core are spatial dependency-oriented and temporal pattern-oriented meta-learning approximate probabilistic inference modules, which can quickly adapt to changing environments via stochastic neurons at each timestamp. Finally, a gated aggregation scheme is leveraged to realize appropriate information selection for the generative style prediction. We benchmarked our approach against state-of-the-art methods with real-world data. The experimental results demonstrate the superiority of our approach over the baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4dea55e0f4c4c2f82b1128047a54ec3d4fccaa7f" target='_blank'>
              Multiview Spatial-Temporal Meta-Learning for Multivariate Time Series Forecasting
              </a>
            </td>
          <td>
            Liang Zhang, Jianping Zhu, Bo Jin, Xiaopeng Wei
          </td>
          <td>2024-07-01</td>
          <td>Sensors (Basel, Switzerland)</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Time series generation is a crucial research topic in the area of deep learning, which can be used for data augmentation, imputing missing values, and forecasting. Currently, latent diffusion models are ascending to the forefront of generative modeling for many important data representations. Being the most pivotal in the computer vision domain, latent diffusion models have also recently attracted interest in other communities, including NLP, Speech, and Geometric Space. In this work, we propose TimeLDM, a novel latent diffusion model for high-quality time series generation. TimeLDM is composed of a variational autoencoder that encodes time series into an informative and smoothed latent content and a latent diffusion model operating in the latent space to generate latent information. We evaluate the ability of our method to generate synthetic time series with simulated and realistic datasets, benchmark the performance against existing state-of-the-art methods. Qualitatively and quantitatively, we find that the proposed TimeLDM persistently delivers high-quality generated time series. Sores from Context-FID and Discriminative indicate that TimeLDM consistently and significantly outperforms current state-of-the-art benchmarks with an average improvement of 3.4$\times$ and 3.8$\times$, respectively. Further studies demonstrate that our method presents better performance on different lengths of time series data generation. To the best of our knowledge, this is the first study to explore the potential of the latent diffusion model for unconditional time series generation and establish a new baseline for synthetic time series.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c127c4b179c7e5d61f7a4347e0003557a02536f1" target='_blank'>
              TimeLDM: Latent Diffusion Model for Unconditional Time Series Generation
              </a>
            </td>
          <td>
            Jian Qian, Miao Sun, Sifan Zhou, Biao Wan, Minhao Li, Patrick Chiang
          </td>
          <td>2024-07-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Many current fault diagnosis methods tend to ignore the temporal correlation in signals, leading to a loss of critical fault information. Additionally, traditional diagnostic models often face challenges in terms of noise immunity, generalization, and handling non-Euclidean structured data. To address these issues, we propose a novel fault diagnosis approach that combines graph neural networks (GNNs) with the Markov transform field (MTF). We first use the MTF to convert vibration signals into 2-D images, preserving temporal correlation and preventing the loss of crucial fault information. Next, we use a graph convolutional neural network (GCN) to process graph-structured data, capturing global structural information. Finally, we introduce the graph attention network (GAT) to dynamically adjust node weights based on their relative importance, enhancing the overall model performance. In this article, we introduce a new fault diagnosis model, GCN-GAT, and evaluate it using the CWRU bearing dataset and a custom-built planetary gearbox dataset. The results show that our model maintains high fault detection accuracy even in the presence of significant noise and variable load conditions. This indicates that our approach demonstrates strong robustness and generalization, providing an effective solution for complex fault diagnosis tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/67b40db25bf26b14664b0de0f0c9ef7c5d9daf51" target='_blank'>
              A Gearbox Fault Diagnosis Method Based on Graph Neural Networks and Markov Transform Fields
              </a>
            </td>
          <td>
            Haitao Wang, Zelin Liu, Mingjun Li, Xiyang Dai, Ruihua Wang, Lichen Shi
          </td>
          <td>2024-08-01</td>
          <td>IEEE Sensors Journal</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Recurrent neural network-based sequence-to-sequence models have been extensively applied for multi-step-ahead time series forecasting. These models typically involve a decoder trained using either its previous forecasts or the actual observed values as the decoder inputs. However, relying on self-generated predictions can lead to the rapid accumulation of errors over multiple steps, while using the actual observations introduces exposure bias as these values are unavailable during the extrapolation stage. In this regard, this study proposes a novel training approach called reinforced decoder, which introduces auxiliary models to generate alternative decoder inputs that remain accessible when extrapolating. Additionally, a reinforcement learning algorithm is utilized to dynamically select the optimal inputs to improve accuracy. Comprehensive experiments demonstrate that our approach outperforms representative training methods over several datasets. Furthermore, the proposed approach also exhibits promising performance when generalized to self-attention-based sequence-to-sequence forecasting models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ecdcef5ef150ad893f080cafe211f505c559753a" target='_blank'>
              Reinforced Decoder: Towards Training Recurrent Neural Networks for Time Series Forecasting
              </a>
            </td>
          <td>
            Qi Sima, Xinze Zhang, Yukun Bao, Siyue Yang, Liang Shen
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/27f9fcb6735817e02fa4201e2f161c1213de1d4e" target='_blank'>
              An adaptive composite time series forecasting model for short-term traffic flow
              </a>
            </td>
          <td>
            Qitan Shao, Xinglin Piao, Xiangyu Yao, Yuqiu Kong, Yongli Hu, Baocai Yin, Yong Zhang
          </td>
          <td>2024-08-03</td>
          <td>Journal of Big Data</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="
 Graph Neural Networks (GNNs) are deep learning models specifically designed for analyzing graph-structured data, capturing complex relationships and structures to improve analysis and prediction. A common task in GNNs is node classification, where each node in the graph is assigned a predefined category. The Graph Attention Network (GAT) is a popular variant of GNNs known for its ability to capture complex dependencies by assigning importance weights to nodes during information aggregation. However, the GAT’s reliance on local attention mechanisms limits its effectiveness in capturing global information and long-range dependencies. To address this limitation, we propose a new attention mechanism called Global-Local Graph Attention (GLGA). Our mechanism enables the GAT to capture long-range dependencies and global graph structures while maintaining its ability to focus on local interactions. We evaluate our algorithm on three citation datasets (Cora, Citeseer, and Pubmed) using multiple metrics, demonstrating its superiority over other baseline models. The proposed GLGA mechanism has been proven to be an effective solution for improving node classification tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c1f387e285df6163f69689ed5ed40e0e4c8fcc01" target='_blank'>
              Global-local graph attention: unifying global and local attention for node classification
              </a>
            </td>
          <td>
            Keao Lin, Xiao-Zhu Xie, Wei Weng, Xiaofeng Du
          </td>
          <td>2024-07-11</td>
          <td>The Computer Journal</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Hierarchical time-series forecasting (HTSF) is an important problem for many real-world business applications where the goal is to simultaneously forecast multiple time-series that are related to each other via a hierarchical relation. Recent works, however, do not address two important challenges that are typically observed in many demand forecasting applications at large companies. First, many time-series at lower levels of the hierarchy have high sparsity i.e., they have a significant number of zeros. Most HTSF methods do not address this varying sparsity across the hierarchy. Further, they do not scale well to the large size of the real-world hierarchy typically unseen in benchmarks used in literature. We resolve both these challenges by proposing HAILS, a novel probabilistic hierarchical model that enables accurate and calibrated probabilistic forecasts across the hierarchy by adaptively modeling sparse and dense time-series with different distributional assumptions and reconciling them to adhere to hierarchical constraints. We show the scalability and effectiveness of our methods by evaluating them against real-world demand forecasting datasets. We deploy HAILS at a large chemical manufacturing company for a product demand forecasting application with over ten thousand products and observe a significant 8.5\% improvement in forecast accuracy and 23% better improvement for sparse time-series. The enhanced accuracy and scalability make HAILS a valuable tool for improved business planning and customer experience.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e7c76a66b9e8e91e00f152d5a723fd37775613e4" target='_blank'>
              Large Scale Hierarchical Industrial Demand Time-Series Forecasting incorporating Sparsity
              </a>
            </td>
          <td>
            Harshavardhan Kamarthi, Aditya B. Sasanur, Xinjie Tong, Xingyu Zhou, James Peters, Joe Czyzyk, B. A. Prakash
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Accurate evaluation of forecasting models is essential for ensuring reliable predictions. Current practices for evaluating and comparing forecasting models focus on summarising performance into a single score, using metrics such as SMAPE. We hypothesize that averaging performance over all samples dilutes relevant information about the relative performance of models. Particularly, conditions in which this relative performance is different than the overall accuracy. We address this limitation by proposing a novel framework for evaluating univariate time series forecasting models from multiple perspectives, such as one-step ahead forecasting versus multi-step ahead forecasting. We show the advantages of this framework by comparing a state-of-the-art deep learning approach with classical forecasting techniques. While classical methods (e.g. ARIMA) are long-standing approaches to forecasting, deep neural networks (e.g. NHITS) have recently shown state-of-the-art forecasting performance in benchmark datasets. We conducted extensive experiments that show NHITS generally performs best, but its superiority varies with forecasting conditions. For instance, concerning the forecasting horizon, NHITS only outperforms classical approaches for multi-step ahead forecasting. Another relevant insight is that, when dealing with anomalies, NHITS is outperformed by methods such as Theta. These findings highlight the importance of aspect-based model evaluation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/11c0631ee5b9502a277f7561c809f6a420e0df60" target='_blank'>
              Forecasting with Deep Learning: Beyond Average of Average of Average Performance
              </a>
            </td>
          <td>
            Vítor Cerqueira, Luis Roque, Carlos Soares
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Dynamic graphs (DG) are often used to describe evolving interactions between nodes in real-world applications. Temporal patterns are a natural feature of DGs and are also key to representation learning. However, existing dynamic GCN models are mostly composed of static GCNs and sequence modules, which results in the separation of spatiotemporal information and cannot effectively capture complex temporal patterns in DGs. To address this problem, this study proposes a spatial-temporal graph convolutional networks with diversified transformation (STGCNDT), which includes three aspects: a) constructing a unified graph tensor convolutional network (GTCN) using tensor M-products without the need to represent spatiotemporal information separately; b) introducing three transformation schemes in GTCN to model complex temporal patterns to aggregate temporal information; and c) constructing an ensemble of diversified transformation schemes to obtain higher representation capabilities. Empirical studies on four DGs that appear in communication networks show that the proposed STGCNDT significantly outperforms state-of-the-art models in solving link weight estimation tasks due to the diversified transformations.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/645c7ccb4c62510922512fdba7088d9493b9fd62" target='_blank'>
              Spatial-temporal Graph Convolutional Networks with Diversified Transformation for Dynamic Graph Representation Learning
              </a>
            </td>
          <td>
            Ling Wang, Yixiang Huang, Hao Wu
          </td>
          <td>2024-08-05</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Ensemble forecasts from numerical weather prediction models show systematic errors that require correction via post-processing. While there has been substantial progress in flexible neural network-based post-processing methods over the past years, most station-based approaches still treat every input data point separately which limits the capabilities for leveraging spatial structures in the forecast errors. In order to improve information sharing across locations, we propose a graph neural network architecture for ensemble post-processing, which represents the station locations as nodes on a graph and utilizes an attention mechanism to identify relevant predictive information from neighboring locations. In a case study on 2-m temperature forecasts over Europe, the graph neural network model shows substantial improvements over a highly competitive neural network-based post-processing method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1fe4b1eeac90174d97c76b0869c2c1e6f138febe" target='_blank'>
              Graph Neural Networks and Spatial Information Learning for Post-Processing Ensemble Weather Forecasts
              </a>
            </td>
          <td>
            Moritz Feik, Sebastian Lerch, Jan Stuhmer
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="As a pivotal subfield within the domain of time series forecasting, runoff forecasting plays a crucial role in water resource management and scheduling. Recent advancements in the application of artificial neural networks (ANNs) and attention mechanisms have markedly enhanced the accuracy of runoff forecasting models. This article introduces an innovative hybrid model, ResTCN-DAM, which synergizes the strengths of deep residual network (ResNet), temporal convolutional networks (TCNs), and dual attention mechanisms (DAMs). The proposed ResTCN-DAM is designed to leverage the unique attributes of these three modules: TCN has outstanding capability to process time series data in parallel. By combining with modified ResNet, multiple TCN layers can be densely stacked to capture more hidden information in the temporal dimension. DAM module adeptly captures the interdependencies within both temporal and feature dimensions, adeptly accentuating relevant time steps/features while diminishing less significant ones with minimal computational cost. Furthermore, the snapshot ensemble method is able to obtain the effect of training multiple models through one single training process, which ensures the accuracy and robustness of the forecasts. The deep integration and collaborative cooperation of these modules comprehensively enhance the model's forecasting capability from various perspectives. Ablation studies conducted validate the efficacy of each module, and through multiple sets of comparative experiments, it is shown that the proposed ResTCN-DAM has exceptional and consistent performance across varying lead times. We also employ visualization techniques to display heatmaps of the model's weights, thereby enhancing the interpretability of the model. When compared with the prevailing neural network-based runoff forecasting models, ResTCN-DAM exhibits state-of-the-art accuracy, temporal robustness, and interpretability, positioning it at the forefront of contemporary research.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/079c42245248c40b0b391ca3bab0fb7ff75c9d0f" target='_blank'>
              Residual Temporal Convolutional Network With Dual Attention Mechanism for Multilead-Time Interpretable Runoff Forecasting.
              </a>
            </td>
          <td>
            Ziyu Sheng, Yuting Cao, Yin Yang, Zhong-Kai Feng, Kaibo Shi, Tingwen Huang, Shiping Wen
          </td>
          <td>2024-06-13</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Implicit graph neural networks have gained popularity in recent years as they capture long-range dependencies while improving predictive performance in static graphs. Despite the tussle between performance degradation due to the oversmoothing of learned embeddings and long-range dependency being more pronounced in dynamic graphs, as features are aggregated both across neighborhood and time, no prior work has proposed an implicit graph neural model in a dynamic setting. In this paper, we present Implicit Dynamic Graph Neural Network (IDGNN) a novel implicit neural network for dynamic graphs which is the first of its kind. A key characteristic of IDGNN is that it demonstrably is well-posed, i.e., it is theoretically guaranteed to have a fixed-point representation. We then demonstrate that the standard iterative algorithm often used to train implicit models is computationally expensive in our dynamic setting as it involves computing gradients, which themselves have to be estimated in an iterative manner. To overcome this, we pose an equivalent bilevel optimization problem and propose an efficient single-loop training algorithm that avoids iterative computation by maintaining moving averages of key components of the gradients. We conduct extensive experiments on real-world datasets on both classification and regression tasks to demonstrate the superiority of our approach over the state-of-the-art baselines. We also demonstrate that our bi-level optimization framework maintains the performance of the expensive iterative algorithm while obtaining up to \textbf{1600x} speed-up.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3093322772df05295e0448968c691f9fa9b30d06" target='_blank'>
              Efficient and Effective Implicit Dynamic Graph Neural Network
              </a>
            </td>
          <td>
            Yongjian Zhong, Hieu Vu, Tianbao Yang, Bijaya Adhikari
          </td>
          <td>2024-06-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="A methodology based on deep recurrent models for maritime surveillance, over publicly available Automatic Identification System (AIS) data, is presented in this paper. The setup employs a deep Recurrent Neural Network (RNN)-based model, for encoding and reconstructing the observed ships’ motion patterns. Our approach is based on a thresholding mechanism, over the calculated errors between observed and reconstructed motion patterns of maritime vessels. Specifically, a deep-learning framework, i.e. an encoder-decoder architecture, is trained using the observed motion patterns, enabling the models to learn and predict the expected trajectory, which will be compared to the effective ones. Our models, particularly the bidirectional GRU with recurrent dropouts, showcased superior performance in capturing the temporal dynamics of maritime data, illustrating the potential of deep learning to enhance maritime surveillance capabilities. Our work lays a solid foundation for future research in this domain, highlighting a path toward improved maritime safety through the innovative application of technology.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/8d9575f41168214cda28f53658d441ec2b0c996d" target='_blank'>
              Outlier detection in maritime environments using AIS data and deep recurrent architectures
              </a>
            </td>
          <td>
            Constantine Maganaris, Eftychios E. Protopapadakis, N. Doulamis
          </td>
          <td>2024-06-14</td>
          <td>Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments</td>
          <td>0</td>
          <td>42</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ab6298680ff5025317e83f15e4256ff1c0149c29" target='_blank'>
              Multi-channel anomaly detection using graphical models
              </a>
            </td>
          <td>
            Bernadin Namoano, Christina Latsou, J. Erkoyuncu
          </td>
          <td>2024-07-13</td>
          <td>Journal of Intelligent Manufacturing</td>
          <td>0</td>
          <td>29</td>
        </tr>

        <tr id="Graphs have emerged as critical data structures for content analysis in various domains, such as social network analysis, bioinformatics, and recommendation systems. Node classification, a fundamental task in this context, is typically tackled using graph neural networks (GNNs). Unfortunately, conventional GNNs still face challenges in scenarios with few labeled nodes, despite the prevalence of few-shot node classification tasks in real-world applications. To address this challenge, various approaches have been proposed, including graph meta-learning, transfer learning, and methods based on Large Language Models (LLMs). However, traditional meta-learning and transfer learning methods often require prior knowledge from base classes or fail to exploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based methods may overlook the zero-shot capabilities of LLMs and rely heavily on the quality of generated contexts. In this paper, we propose a novel approach that integrates LLMs and GNNs, leveraging the zero-shot inference and reasoning capabilities of LLMs and employing a Graph-LLM-based active learning paradigm to enhance GNNs' performance. Extensive experiments demonstrate the effectiveness of our model in improving node classification accuracy with considerably limited labeled data, surpassing state-of-the-art baselines by significant margins.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d41329cb9c08fef08379c558db5a09f412a506ad" target='_blank'>
              Enhancing Data-Limited Graph Neural Networks by Actively Distilling Knowledge from Large Language Models
              </a>
            </td>
          <td>
            Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, Suhang Wang
          </td>
          <td>2024-07-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="
 Load forecasting has perennially stood as a fundamental pillar within the power industry, with precision in prognostication constituting a paramount necessity for critical functions including grid management, optimization of energy storage, and response to demand fluctuations. In the present study, we introduce a comprehensive global modeling prediction framework aimed at forecasting large-scale power demand within time series data, alongside the development of a novel short-term power load forecasting model. By integrating all time series data pertaining to the same regression task and employing a unified univariate prediction function across the entire time series dataset, we efficaciously forecast large-scale heterogeneous loads aggregated at various levels within distribution networks. The proposed framework leverages the cutting-edge DARTS (Differentiable Architecture Search) deep learning framework for implementation. Through comprehensive experimentation on real-world data, our model framework demonstrates superior performance in terms of overall prediction accuracy, scalability, and robustness in handling missing and non-stationary time series data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/42aaaf069fb1f234843001c98ef50b7c55dbc902" target='_blank'>
              Short-term load forecasting of mixed industrial users in distribution networks using DARTS-LSTM method
              </a>
            </td>
          <td>
            Wenyu Shi, Dechang Yang, Payman Dehghaniang
          </td>
          <td>2024-08-01</td>
          <td>Journal of Physics: Conference Series</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="As attention to recorded data grows in the realm of automotive testing and manual evaluation reaches its limits, there is a growing need for automatic online anomaly detection. This real-world data is complex in many ways and requires the modelling of testee behaviour. To address this, we propose a temporal variational autoencoder (TeVAE) that can detect anomalies with minimal false positives when trained on unlabelled data. Our approach also avoids the bypass phenomenon and introduces a new method to remap individual windows to a continuous time series. Furthermore, we propose metrics to evaluate the detection delay and root-cause capability of our approach and present results from experiments on a real-world industrial data set. When properly configured, TeVAE flags anomalies only 6% of the time wrongly and detects 65% of anomalies present. It also has the potential to perform well with a smaller training and validation subset but requires a more sophisticated threshold estimation method.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c3ae8a87673bb36b0506af55a306ca5cc65d4446" target='_blank'>
              TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly Detection in Variable-state Multivariate Time-series Data
              </a>
            </td>
          <td>
            Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Back, Anna V. Kononova
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="In the rapidly evolving field of sensor technology, efficient and accurate anomaly detection is critical across applications from environmental monitoring to cybersecurity. Traditional approaches often fail in real-time sensor data scenarios due to high computational requirements and lack of labeled datasets. This article presents a lightweight, unsupervised anomaly detection framework that combines continuous wavelet transform (CWT) with support vector clustering (SVC), aiming to reduce computational complexity and dynamically adapt to the data flow. Extensive validation on the Intel Berkeley Research Laboratory (IBRL) dataset demonstrates that our method not only handles sensor aberrations effectively, but also achieves a significant detection accuracy of 93.2% for drift readings, confirming its robustness and efficiency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1be9d983144f4eeeea269c88a5785f13b3971a1f" target='_blank'>
              A Reliable Approach for Lightweight Anomaly Detection in Sensors Using Continuous Wavelet Transform and Vector Clustering
              </a>
            </td>
          <td>
            Rami Ahmad, Waseem Alhasan, Raniyah Wazirali, Rania Almajalid
          </td>
          <td>2024-08-01</td>
          <td>IEEE Sensors Journal</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="
 Graph neural network (GNN) has the proven ability to learn feature representations from graph data, and has been utilized for the tasks of predicting the machinery remaining useful life (RUL). However, existing methods only focus on a single graph structure and cannot integrate the correlation information contained in multi-graph structures. To address these issues, a multi-graph structure GNN prediction method with attention fusion (MGAFGNN) is proposed in this paper for GNN-based bearing RUL prediction. Specifically, a multi-channel graph attention module (MCGAM) is designed to effectively learn the similar features of node neighbors from different graph data and capture the multi-scale latent features of nodes through the nonlinear transformation. Furthermore, a multi-graph attention fusion module (MGAFM) is proposed to extract the collaborative features from the interaction graph, thereby fusing the feature embeddings from different graph structures. The fused feature representation is sent to the long short-term memory (LSTM) network to further learn the temporal features and achieve RUL prediction. The experimental results on two bearing datasets demonstrate that MGAFGNN outperforms existing methods in terms of prediction performance by effectively incorporating multi-graph structural information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/fd2674e2befbd01e7d09b8390264b47a82a8fa77" target='_blank'>
              Multi-graph attention fusion graph neural network for remaining useful life prediction of rolling bearings
              </a>
            </td>
          <td>
            Yongchang Xiao, Lingli Cui, Dongdong Liu
          </td>
          <td>2024-07-02</td>
          <td>Measurement Science and Technology</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances. In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse fields. Although massive LLM based approaches are proposed for time series tasks, these methods require to load the whole LLM in both training and reference. This high computational demands limit practical applications in resource-constrained settings, like edge-computing and IoT devices. To address this issue, we propose Knowledge Pruning (KP), a novel paradigm for time series learning in this paper. For a specific downstream task, we argue that the world knowledge learned by LLMs is much redundant and only the related knowledge termed as"pertinent knowledge"is useful. Unlike other methods, our KP targets to prune the redundant knowledge and only distill the pertinent knowledge into the target model. This reduces model size and computational costs significantly. Additionally, different from existing LLM based approaches, our KP does not require to load the LLM in the process of training and testing, further easing computational burdens. With our proposed KP, a lightweight network can effectively learn the pertinent knowledge, achieving satisfactory performances with a low computation cost. To verify the effectiveness of our KP, two fundamental tasks on edge-computing devices are investigated in our experiments, where eight diverse environments or benchmarks with different networks are used to verify the generalization of our KP. Through experiments, our KP demonstrates effective learning of pertinent knowledge, achieving notable performance improvements in regression (19.7% on average) and classification (up to 13.7%) tasks, showcasing state-of-the-art results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/818750ab091d9a798766931f53aad7bf0581bccb" target='_blank'>
              LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices
              </a>
            </td>
          <td>
            Ruibing Jin, Qing Xu, Min Wu, Yuecong Xu, Dan Li, Xiaoli Li, Zhenghua Chen
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4ccc940a891f74aa82e4a858d918d21d59774433" target='_blank'>
              A binary-domain recurrent-like architecture-based dynamic graph neural network
              </a>
            </td>
          <td>
            Zi-chao Chen, Sui Lin
          </td>
          <td>2024-06-25</td>
          <td>Auton. Intell. Syst.</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In this paper, we introduce a novel theoretical framework for multi-task regression, applying random matrix theory to provide precise performance estimations, under high-dimensional, non-Gaussian data distributions. We formulate a multi-task optimization problem as a regularization technique to enable single-task models to leverage multi-task learning information. We derive a closed-form solution for multi-task optimization in the context of linear models. Our analysis provides valuable insights by linking the multi-task learning performance to various model statistics such as raw data covariances, signal-generating hyperplanes, noise levels, as well as the size and number of datasets. We finally propose a consistent estimation of training and testing errors, thereby offering a robust foundation for hyperparameter optimization in multi-task regression scenarios. Experimental validations on both synthetic and real-world datasets in regression and multivariate time series forecasting demonstrate improvements on univariate models, incorporating our method into the training loss and thus leveraging multivariate information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/83734272d78baa3517416ebc962bcbb5accc3346" target='_blank'>
              Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting
              </a>
            </td>
          <td>
            Romain Ilbert, Malik Tiomoko, Cosme Louart, Ambroise Odonnat, Vasilii Feofanov, Themis Palpanas, I. Redko
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>49</td>
        </tr>

        <tr id="Time series forecasting has played an important role in different industries, including economics, energy, weather, and healthcare. RNN-based methods have shown promising potential due to their strong ability to model the interaction of time and variables. However, they are prone to gradient issues like gradient explosion and vanishing gradients. And the prediction accuracy is not high. To address the above issues, this paper proposes a Fractional-order Lipschitz Recurrent Neural Network with a Frequency-domain Gated Attention mechanism (FLRNN-FGA). There are three major components: the Fractional-order Lipschitz Recurrent Neural Network (FLRNN), frequency module, and gated attention mechanism. In the FLRNN, fractional-order integration is employed to describe the dynamic systems accurately. It can capture long-term dependencies and improve prediction accuracy. Lipschitz weight matrices are applied to alleviate the gradient issues. In the frequency module, temporal data are transformed into the frequency domain by Fourier transform. Frequency domain processing can reduce the computational complexity of the model. In the gated attention mechanism, the gated structure can regulate attention information transmission to reduce the number of model parameters. Extensive experimental results on five real-world benchmark datasets demonstrate the effectiveness of FLRNN-FGA compared with the state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/df8abef5d3a79df25da8495db7434b86114030b4" target='_blank'>
              FLRNN-FGA: Fractional-Order Lipschitz Recurrent Neural Network with Frequency-Domain Gated Attention Mechanism for Time Series Forecasting
              </a>
            </td>
          <td>
            Chunna Zhao, Junjie Ye, Zelong Zhu, Yaqun Huang
          </td>
          <td>2024-07-22</td>
          <td>Fractal and Fractional</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Effective imputation is a crucial preprocessing step for time series analysis. Despite the development of numerous deep learning algorithms for time series imputation, the community lacks standardized and comprehensive benchmark platforms to effectively evaluate imputation performance across different settings. Moreover, although many deep learning forecasting algorithms have demonstrated excellent performance, whether their modeling achievements can be transferred to time series imputation tasks remains unexplored. To bridge these gaps, we develop TSI-Bench, the first (to our knowledge) comprehensive benchmark suite for time series imputation utilizing deep learning techniques. The TSI-Bench pipeline standardizes experimental settings to enable fair evaluation of imputation algorithms and identification of meaningful insights into the influence of domain-appropriate missingness ratios and patterns on model performance. Furthermore, TSI-Bench innovatively provides a systematic paradigm to tailor time series forecasting algorithms for imputation purposes. Our extensive study across 34,804 experiments, 28 algorithms, and 8 datasets with diverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse downstream tasks and potential to unlock future directions in time series imputation research and analysis. The source code and experiment logs are available at https://github.com/WenjieDu/AwesomeImputation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/74049a723c1c52b4f2dc01b28bef137771cb1148" target='_blank'>
              TSI-Bench: Benchmarking Time Series Imputation
              </a>
            </td>
          <td>
            Wenjie Du, Jun Wang, Linglong Qian, Yiyuan Yang, Fanxing Liu, Zepu Wang, Zina Ibrahim, Haoxin Liu, Zhiyuan Zhao, Yingjie Zhou, Wenjia Wang, Kaize Ding, Yuxuan Liang, B. A. Prakash, Qingsong Wen
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>4</td>
        </tr>

        <tr id="Modeling time series data has become a very at tractive research topic due to its wide application, such as human activity recognition, financial forecasting and sensor-based automatic system monitoring. Recently deep learning models have shown great advances in modeling the time series data but they heavily depend on a large amount of labeled data. To avoid costly labeling, this paper explores domain adaptation from a labeled source domain to the unlabeled target domain on time series data. To achieve the goal, we propose a disentangled representation learning framework named CADT to disentangle the domain-invariant features from the domain-specific ones. Particularly, CADT is injected with a novel class-wise hypersphere loss to improve the generalization of the classifier from the source domain to the target domain. Intuitively, it restricts the source data of the same class within the same hypersphere and minimizes the radius of it, which in turn enlarges the margin between different classes and makes the decision boundary of both domains easier. We further devise several kinds of domain-preserving data augmentation methods to better capture the domain-specific patterns. Extensive experiments on two public datasets and two real-world applications demonstrate the effectiveness of the proposed model against several state-of-the-art baselines.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a21870156817614d3af47fda7b03e99e6181a852" target='_blank'>
              Disentangling Domain and General Representations for Time Series Classification
              </a>
            </td>
          <td>
            Youmin Chen, Xinyu Yan, Yang Yang, Jianfeng Zhang, Jing Zhang, Lujia Pan, Juren Li
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The prediction of time series is a challenging task relevant in such diverse applications as analyzing financial data, forecasting flow dynamics or understanding biological processes. Especially chaotic time series that depend on a long history pose an exceptionally difficult problem. While machine learning has shown to be a promising approach for predicting such time series, it either demands long training time and much training data when using deep recurrent neural networks. Alternative, when using a reservoir computing approach it comes with high uncertainty and typically a high number of random initializations and extensive hyper-parameter tuning when using a reservoir computing approach. In this paper, we focus on the reservoir computing approach and propose a new mapping of input data into the reservoir's state space. Furthermore, we incorporate this method in two novel network architectures increasing parallelizability, depth and predictive capabilities of the neural network while reducing the dependence on randomness. For the evaluation, we approximate a set of time series from the Mackey-Glass equation, inhabiting non-chaotic as well as chaotic behavior and compare our approaches in regard to their predictive capabilities to echo state networks and gated recurrent units. For the chaotic time series, we observe an error reduction of up to $85.45\%$ and up to $87.90\%$ in contrast to echo state networks and gated recurrent units respectively. Furthermore, we also observe tremendous improvements for non-chaotic time series of up to $99.99\%$ in contrast to existing approaches.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d488897997df288927a73befa218f42726e3f36d" target='_blank'>
              Temporal Convolution Derived Multi-Layered Reservoir Computing
              </a>
            </td>
          <td>
            Johannes Viehweg, Dominik Walther, Prof. Dr.-Ing. Patrick Mader
          </td>
          <td>2024-07-09</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Recent works have demonstrated the potential of Graph Neural Networks (GNN) for network intrusion detection. Despite their advantages, a significant gap persists between real-world scenarios, where detection speed is critical, and existing proposals, which operate on large graphs representing several hours of traffic. This gap results in unrealistic operational conditions and impractical detection delays. Moreover, existing models do not generalize well across different networks, hampering their deployment in production environments. To address these issues, we introduce PPTGNN, a practical spatio-temporal GNN for intrusion detection. PPTGNN enables near real-time predictions, while better capturing the spatio-temporal dynamics of network attacks. PPTGNN employs self-supervised pre-training for improved performance and reduced dependency on labeled data. We evaluate PPTGNN on three public datasets and show that it significantly outperforms state-of-the-art models, such as E-ResGAT and E-GraphSAGE, with an average accuracy improvement of 10.38%. Finally, we show that a pre-trained PPTGNN can easily be fine-tuned to unseen networks with minimal labeled examples. This highlights the potential of PPTGNN as a general, large-scale pre-trained model that can effectively operate in diverse network environments.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f5f8d70fd421eea7999a2a530f094c90a24eeaac" target='_blank'>
              PPT-GNN: A Practical Pre-Trained Spatio-Temporal Graph Neural Network for Network Security
              </a>
            </td>
          <td>
            Louis Van Langendonck, Ismael Castell-Uroz, P. Barlet-Ros
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>27</td>
        </tr>

        <tr id="Graph Neural Network (GNN) achieves great success for node-level and graph-level tasks via encoding meaningful topological structures of networks in various domains, ranging from social to biological networks. However, repeated aggregation operations lead to excessive mixing of node representations, particularly in dense regions with multiple GNN layers, resulting in nearly indistinguishable embeddings. This phenomenon leads to the oversmoothing problem that hampers downstream graph analytics tasks. To overcome this issue, we propose a novel and flexible truss-based graph sparsification model that prunes edges from dense regions of the graph. Pruning redundant edges in dense regions helps to prevent the aggregation of excessive neighborhood information during hierarchical message passing and pooling in GNN models. We then utilize our sparsification model in the state-of-the-art baseline GNNs and pooling models, such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL, DMonPool, and AdamGNN. Extensive experiments on different real-world datasets show that our model significantly improves the performance of the baseline GNN models in the graph classification task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3c6b535bf90c88c1001770c3872f3caf3e8e11bb" target='_blank'>
              Tackling Oversmoothing in GNN via Graph Sparsification: A Truss-based Approach
              </a>
            </td>
          <td>
            Tanvir Hossain, Khaled Mohammed Saifuddin, Muhammad Ifte Khairul Islam, Farhan Tanvir, Esra Akbas
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="Machine learning approaches, such as artificial neural networks (ANN), effectively perform various tasks and provide new predictive models for complicated physiological systems. Examples of Robotics applications involving direct human engagement, such as controlling prosthetic arms, athletic training, and investigating muscle physiology. It is now time for automated systems to take over modelling and monitoring tasks. However, there is a problem with the massive amount of time series data collected to build accurate forecasting systems. There may be inconsistencies in forecasting muscle forces due to the enormous amount of data. As a result, anomaly detection techniques play a significant role in detecting anomalous data. Detecting anomalies can help reduce redundancy and free up large storage space for storing relevant time‐series data. This paper employs several anomaly detection techniques, including Isolation Forest (iforest), K‐Nearest Neighbour (KNN), Open Support Vector Machine (OSVM), Histogram, and Local Outlier Factor (LOF). These techniques have been used by Long Short‐Term Memory (LSTM), Auto‐Regressive Integrated Moving Average (ARIMA), and Prophet models. The dataset used in this study contained raw measurements of body movements (kinematics) and the forces generated during walking (kinetics) of 57 healthy people (29 Female, 28 Male) without walking abnormalities or recent leg injuries. To increase the data samples, we used TimeGAN that generates synthetic time series data with temporal dependencies, aiding in training robust predictive models for muscle force prediction. The results are then compared with different evaluation metrics for five different samples. It is found that anomaly detection techniques with LSTM, ARIMA, and Prophet models provided better performance in forecasting muscle forces. The iforest method achieved the best Pearson's Correlation Coefficient (r) of 0.95, which is a competitive score with existing systems that perform between 0.7 and 0.9. The methodology provides a foundation for precision medicine, enhancing prognostic capability over relying solely on population averages.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/037043014df1953716ded03eeee758b86ba3a9ba" target='_blank'>
              Time series generative adversarial network for muscle force prognostication using statistical outlier detection
              </a>
            </td>
          <td>
            Hunish Bansal, Basavraj Chinagundi, P. Rana, Neeraj Kumar
          </td>
          <td>2024-06-23</td>
          <td>Expert Systems</td>
          <td>0</td>
          <td>16</td>
        </tr>

        <tr id="Inspired by Large Language Models (LLMs), Time Series Forecasting (TSF), a long-standing task in time series analysis, is undergoing a transition towards Large Time Series Models (LTSMs), aiming to train universal transformer-based models for TSF. However, training LTSMs on heterogeneous time series data poses unique challenges, including diverse frequencies, dimensions, and patterns across datasets. Recent endeavors have studied and evaluated various design choices aimed at enhancing LTSM training and generalization capabilities, spanning pre-processing techniques, model configurations, and dataset configurations. In this work, we comprehensively analyze these design choices and aim to identify the best practices for training LTSM. Moreover, we propose \emph{time series prompt}, a novel statistical prompting strategy tailored to time series data. Furthermore, based on the observations in our analysis, we introduce \texttt{LTSM-bundle}, which bundles the best design choices we have identified. Empirical results demonstrate that \texttt{LTSM-bundle} achieves superior zero-shot and few-shot performances compared to state-of-the-art LSTMs and traditional TSF methods on benchmark datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/631c05be8806b43fac7aaf4d041de59562ac17f9" target='_blank'>
              Understanding Different Design Choices in Training Large Time Series Models
              </a>
            </td>
          <td>
            Yu-Neng Chuang, Songchen Li, Jiayi Yuan, Guanchu Wang, Kwei-Herng Lai, Leisheng Yu, Sirui Ding, Chia-yuan Chang, Qiaoyu Tan, D. Zha, Xia Hu
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>22</td>
        </tr>

        <tr id="Time series forecasting has been an essential field in many different application areas, including economic analysis, meteorology, and so forth. The majority of time series forecasting models are trained using the mean squared error (MSE). However, this training based on MSE causes a limitation known as prediction delay. The prediction delay, which implies the ground-truth precedes the prediction, can cause serious problems in a variety of fields, e.g., finance and weather forecasting -- as a matter of fact, predictions succeeding ground-truth observations are not practically meaningful although their MSEs can be low. This paper proposes a new perspective on traditional time series forecasting tasks and introduces a new solution to mitigate the prediction delay. We introduce a continuous-time gated recurrent unit (GRU) based on the neural ordinary differential equation (NODE) which can supervise explicit time-derivatives. We generalize the GRU architecture in a continuous-time manner and minimize the prediction delay through our time-derivative regularization. Our method outperforms in metrics such as MSE, Dynamic Time Warping (DTW) and Time Distortion Index (TDI). In addition, we demonstrate the low prediction delay of our method in a variety of datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb1274dd33e80fab3c983829e9b94abcebe9e2a8" target='_blank'>
              Addressing Prediction Delays in Time Series Forecasting: A Continuous GRU Approach with Derivative Regularization
              </a>
            </td>
          <td>
            Sheo Yon Jhin, Seojin Kim, Noseong Park
          </td>
          <td>2024-06-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Automated Driving Systems (ADS) development relies on utilizing real-world vehicle data. The volume of data generated by modern vehicles presents transmission, storage, and computational challenges. Focusing on Dynamic Behavior (DB) offers a promising approach to distinguish relevant from irrelevant information for ADS functionalities, thereby reducing data. Time series pattern recognition is beneficial for this task as it can analyze the temporal context of vehicle driving behavior. However, existing state-of-the-art methods often lack the adaptability to identify variable-length patterns or provide analytical descriptions of discovered patterns. This contribution proposes a Behavior Forest framework for real-time data selection by constructing a Behavior Graph during vehicle operation, facilitating analytical descriptions without pre-training. The method demonstrates its performance using a synthetically generated and electrocardiogram data set. An automotive time series data set is used to evaluate the data reduction capabilities, in which this method discarded 96.01% of the incoming data stream, while relevant DB remain included.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0f9405920808feb31e2d3725dcb785e8bac96acb" target='_blank'>
              Behavior Forests: Real-Time Discovery of Dynamic Behavior for Data Selection
              </a>
            </td>
          <td>
            Philipp Reis, Philipp Rigoll, Eric Sax
          </td>
          <td>2024-07-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Time series anomaly detection is the process of identifying anomalies within time series data. The primary challenge of this task lies in the necessity for the model to comprehend the characteristics of time-independent and abnormal data patterns. In this study, a novel algorithm called adaptive memory broad learning system (AdaMemBLS) is proposed for time series anomaly detection. This algorithm leverages the rapid inference capabilities of the broad learning algorithm and the memory bank's capacity to differentiate between normal and abnormal data. Furthermore, an incremental algorithm based on multiple data augmentation techniques is introduced and applied to multiple ensemble learners, thereby enhancing the model's effectiveness in learning the characteristics of time series data. To bolster the model's anomaly detection capabilities, a more diverse ensemble approach and a discriminative anomaly score are recommended. Extensive experiments conducted on various real-world datasets demonstrate that the proposed method exhibits superior inference speed and more accurate anomaly detection compared to the existing competitors. A detailed experimental investigation is presented to elucidate the effectiveness of the proposed method and the underlying reasons for its efficacy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/36aa8bca7afd76f17a62c6192b1808a1a39462e8" target='_blank'>
              Adaptive Memory Broad Learning System for Unsupervised Time Series Anomaly Detection.
              </a>
            </td>
          <td>
            Zhijie Zhong, Zhiwen Yu, Ziwei Fan, C. L. P. Chen, Kaixiang Yang
          </td>
          <td>2024-06-26</td>
          <td>IEEE transactions on neural networks and learning systems</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We explore various neural network architectures for modeling the dynamics of the cryptocurrency market. Traditional linear models often fall short in accurately capturing the unique and complex dynamics of this market. In contrast, Deep Neural Networks (DNNs) have demonstrated considerable proficiency in time series forecasting. This papers introduces novel neural network framework that blend the principles of econometric state space models with the dynamic capabilities of Recurrent Neural Networks (RNNs). We propose state space models using Long Short Term Memory (LSTM), Gated Residual Units (GRU) and Temporal Kolmogorov-Arnold Networks (TKANs). According to the results, TKANs, inspired by Kolmogorov-Arnold Networks (KANs) and LSTM, demonstrate promising outcomes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f2be9285dd976d1dd4c063f13369b73af652e881" target='_blank'>
              Deep State Space Recurrent Neural Networks for Time Series Forecasting
              </a>
            </td>
          <td>
            Hugo Inzirillo
          </td>
          <td>2024-07-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>0</td>
        </tr>

        <tr id="An overview of various open-source Python libraries for time series analysis and forecasting is presented. It covers such tools as Prophet, Kats, Merlion, as well as ARIMA, LSTM algorithms, which allow to study seasonality, trends and anomalies in time series data. The capabilities of each library, their advantages and applications in time series data analysis are discussed in detail.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7bdeb61d1e26117d5e63a3cb3e86b8b03e7303b1" target='_blank'>
              Review of Open-Source Libraries for Solving Time Series Forecasting Problems
              </a>
            </td>
          <td>
            E.A. Svekolnikova, V.N. Panovskiy
          </td>
          <td>2024-07-01</td>
          <td>Моделирование и анализ данных</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This work examines the potential of predictive analysis to characterize the behavior of VoIP traffic in a mobile environment where approximately 40, 000 packets of voice traffic have been collected, processed, and analyzed. Starting with the construction of 6 specific QoS/QoE metrics extracted from a VoIP measurement campaign in an LTE-A environment, we face the problem of predicting the behavior of such metrics across time. A preliminary stage involves estimating a Vector AutoRegressive (VAR) model to capture correlations among the involved time series. This stage also involves statistical checks, such as stationarity and residual autocorrelations, in order to build a consistent model to be used for prediction. In the second stage, we employ a set of recurrent neural networks (simple RNN, LSTM, and GRU) to predict the behavior of selected QoS/QoE metrics. This choice is motivated by the fact that such techniques are able to handle temporal sequences, owing to their cell memory structure. Then, the employed techniques are contrasted in terms of both their offered performance and required computational time. Results provide valuable insights for constructing realistic traffic models (not artificially simulated ones) and useful information for network providers looking to optimize their resources based on usage patterns.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/368e7983211492c90c5c524aa54495a3c31d7530" target='_blank'>
              Evaluating Recurrent Neural Networks for prediction of Multi- Variate time series VoIP metrics
              </a>
            </td>
          <td>
            M. Mauro, G. Galatro, Fabio Postiglione, Wei Song, Antonio Liotta
          </td>
          <td>2024-06-11</td>
          <td>2024 22nd Mediterranean Communication and Computer Networking Conference (MedComNet)</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Efficient representation of complex infrastructure systems is crucial for system‐level management tasks, such as edge prediction, component classification, and decision‐making. However, the complex interactions between the infrastructure systems and their spatial environments increased the complexity of network representation learning. This study introduces a novel geometric‐based multimodal deep learning model for spatially embedded network representation learning, namely the regional spatial graph convolutional network (RSGCN). The developed RSGCN model simultaneously learns from the node's multimodal spatial features. To evaluate the network representation performance, the introduced RSGCN model is used to embed different infrastructure networks into latent spaces and then reconstruct the networks. A synthetic network dataset, a California Highway Network, and a New Jersey Power Network were used as testbeds. The performance of the developed model is compared with two other state‐of‐the‐art geometric deep learning models, GraphSAGE and Spatial Graph Convolutional Network. The results demonstrate the importance of considering regional information and the effectiveness of using novel graph convolutional neural networks for a more accurate representation of complex infrastructure systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6eefffcdebbdf1dd6cb24f14d87797386c0c6010" target='_blank'>
              Modeling of spatially embedded networks via regional spatial graph convolutional networks
              </a>
            </td>
          <td>
            Xudong Fan, Jürgen Hackl
          </td>
          <td>2024-06-20</td>
          <td>Computer-Aided Civil and Infrastructure Engineering</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Large language models (LLMs) are being applied to time series tasks, particularly time series forecasting. However, are language models actually useful for time series? After a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade the forecasting results -- in most cases the results even improved. We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and reveal that patching and attention structures perform similarly to state-of-the-art LLM-based forecasters.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/df0d604b8e8e3b2947d9865d735f204c08635012" target='_blank'>
              Are Language Models Actually Useful for Time Series Forecasting?
              </a>
            </td>
          <td>
            Mingtian Tan, Mike A. Merrill, Vinayak Gupta, Tim Althoff, Tom Hartvigsen
          </td>
          <td>2024-06-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Lost circulation, a common risk during the drilling process, significantly impacts drilling safety and efficiency. The presence of data noise and temporal evolution characteristics pose significant challenges to the accurate monitoring of lost circulation. Traditional supervised intelligent monitoring methods rely on large amounts of labeled data, which often do not consider temporal fluctuations in data, leading to insufficient accuracy and transferability. To address these issues, this paper proposes an unsupervised time series autoencoder (BiLSTM-AE) intelligent monitoring model for lost circulation, aiming to overcome the limitations of supervised algorithms. The BiLSTM-AE model employs BiLSTM for both the encoder and decoder, enabling it to comprehensively capture the temporal features and dynamic changes in the data. It learns the patterns of normal data sequences, thereby automatically identifying anomalous risk data points that deviate from the normal patterns during testing. Results show that the proposed model can efficiently identify and monitor lost circulation risks, achieving an accuracy of 92.51%, a missed alarm rate of 6.87%, and a false alarm rate of 7.71% on the test set. Compared to other models, the BiLSTM-AE model has higher accuracy and better timeliness, which is of great significance for improving drilling efficiency and ensuring drilling safety.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7fcaadd2590a96052d351370048cb028c6094a31" target='_blank'>
              Intelligent Monitoring Model for Lost Circulation Based on Unsupervised Time Series Autoencoder
              </a>
            </td>
          <td>
            Liwei Wu, Xiaopeng Wang, Ziyue Zhang, Guowei Zhu, Qilong Zhang, Pinghua Dong, Jiangtao Wang, Zhaopeng Zhu
          </td>
          <td>2024-06-22</td>
          <td>Processes</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Recently, the incorporation of both temporal features and the correlation across time series has become an effective approach in time series prediction. Spatio-Temporal Graph Neural Networks (STGNNs) demonstrate good performance on many Temporal-correlation Forecasting Problem. However, when applied to tasks lacking periodicity, such as stock data prediction, the effectiveness and robustness of STGNNs are found to be unsatisfactory. And STGNNs are limited by memory savings so that cannot handle problems with a large number of nodes. In this paper, we propose a novel approach called the Temporal-Correlation Graph Pre-trained Network (TCGPN) to address these limitations. TCGPN utilize Temporal-correlation fusion encoder to get a mixed representation and pre-training method with carefully designed temporal and correlation pre-training tasks. Entire structure is independent of the number and order of nodes, so better results can be obtained through various data enhancements. And memory consumption during training can be significantly reduced through multiple sampling. Experiments are conducted on real stock market data sets CSI300 and CSI500 that exhibit minimal periodicity. We fine-tune a simple MLP in downstream tasks and achieve state-of-the-art results, validating the capability to capture more robust temporal correlation patterns.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1184b61fe584f056e3f7156ed3fe740a346b75a1" target='_blank'>
              TCGPN: Temporal-Correlation Graph Pre-trained Network for Stock Forecasting
              </a>
            </td>
          <td>
            Wenbo Yan, Ying Tan
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Distributed databases are fundamental infrastructures of today's large-scale software systems such as cloud systems. Detecting anomalies in distributed databases is essential for maintaining software availability. Existing approaches, predominantly developed using Loghub-a comprehensive collection of log datasets from various systems-lack datasets specifically tailored to distributed databases, which exhibit unique anomalies. Additionally, there's a notable absence of datasets encompassing multi-anomaly, multi-node logs. Consequently, models built upon these datasets, primarily designed for standalone systems, are inadequate for distributed databases, and the prevalent method of deeming an entire cluster anomalous based on irregularities in a single node leads to a high false-positive rate. This paper addresses the unique anomalies and multivariate nature of logs in distributed databases. We expose the first open-sourced, comprehensive dataset with multivariate logs from distributed databases. Utilizing this dataset, we conduct an extensive study to identify multiple database anomalies and to assess the effectiveness of state-of-the-art anomaly detection using multivariate log data. Our findings reveal that relying solely on logs from a single node is insufficient for accurate anomaly detection on distributed database. Leveraging these insights, we propose MultiLog, an innovative multivariate log-based anomaly detection approach tailored for distributed databases. Our experiments, based on this novel dataset, demonstrate MultiLog's superiority, outperforming existing state-of-the-art methods by approximately 12%.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7b1ce3e66e47fcfad22f8d59504b656762b1bc32" target='_blank'>
              Multivariate Log-based Anomaly Detection for Distributed Database
              </a>
            </td>
          <td>
            Lingzhe Zhang, Tong Jia, Mengxi Jia, Ying Li, Yong Yang, Zhonghai Wu
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>7</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2a2d930d01bd781f6b52c085e333a3895ad406f5" target='_blank'>
              ASOD: an adaptive stream outlier detection method using online strategy
              </a>
            </td>
          <td>
            Zhichao Hu, Xiangzhan Yu, Likun Liu, Yu Zhang, Haining Yu
          </td>
          <td>2024-07-05</td>
          <td>J. Cloud Comput.</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The emerging graph convolutional network (GCN) provides a way to extract additional relationship information and has demonstrated superior performance in fault diagnosis. However, the GCN-based methods have some limitations when applied to cross-domain fault diagnostics. First, the graph topology constructed according to certain criteria remains fixed while it may contain domain-specific information, which could hamper the extraction of domain-invariant features. Second, the non-Euclidean structure of the graph hinders its augmentation through mixing and interpolation, bringing its application challenges in scenarios with small samples. To address these limitations, this article proposes an adaptive topology-aware siamese network (ATA-SN) for cross-domain fault diagnosis with small samples. Specifically, the multichannel data are constructed as a complete graph as input. The adaptive topology-aware module updates and augments the graph simultaneously through biased uncertainty edge perturbation in a nonparametric manner. In combination with a contrastive loss, augmented data with clear class boundaries can be obtained. A statistical metrics-based style loss is used for domain-invariant feature learning. A pair of updated graphs is then fed into the siamese framework, which utilizes the semantic consistency of the paired graphs to recalibrate the contribution of the augmented data to the gradient descent, thus ensuring the effectiveness of the graph augmentation and domain adaptation. Comparative studies and cross-domain fault diagnosis experiments indicate the superiority and effectiveness of the proposed method even in scenarios with extremely limited training data. The code of ATA-SN is released at https://github.com/CQU-ZixuChen/ATA-SN.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/65415f2d9909e8321b7099af8288d9c77f827c26" target='_blank'>
              Adaptive Topology-Aware Siamese Network for Cross-Domain Fault Diagnosis With Small Samples
              </a>
            </td>
          <td>
            Zixu Chen, J. Ji, Kai Chen, Qing Ni, Xiaoxi Ding, Wennian Yu
          </td>
          <td>2024-08-01</td>
          <td>IEEE Sensors Journal</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Machine learning (ML) models have emerged as powerful tools for accelerating materials discovery and design by enabling accurate predictions of properties from compositional and structural data. These capabilities are vital for developing advanced technologies across fields such as energy, electronics, and biomedicine, potentially reducing the time and resources needed for new material exploration and promoting rapid innovation cycles. Recent efforts have focused on employing advanced ML algorithms, including deep learning - based graph neural network, for property prediction. Additionally, ensemble models have proven to enhance the generalizability and robustness of ML and DL. However, the use of such ensemble strategies in deep graph networks for material property prediction remains underexplored. Our research provides an in-depth evaluation of ensemble strategies in deep learning - based graph neural network, specifically targeting material property prediction tasks. By testing the Crystal Graph Convolutional Neural Network (CGCNN) and its multitask version, MT-CGCNN, we demonstrated that ensemble techniques, especially prediction averaging, substantially improve precision beyond traditional metrics for key properties like formation energy per atom ($\Delta E^{f}$), band gap ($E_{g}$) and density ($\rho$) in 33,990 stable inorganic materials. These findings support the broader application of ensemble methods to enhance predictive accuracy in the field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/afeb0f922c4ca4409341e544b270897d9ba61451" target='_blank'>
              Enhancing material property prediction with ensemble deep graph convolutional networks
              </a>
            </td>
          <td>
            Chowdhury Mohammad Abid Rahman, Ghadendra B. Bhandari, Nasser M. Nasrabadi, Aldo H. Romero, P. Gyawali
          </td>
          <td>2024-07-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="In sequential event prediction, which finds applications in finance, retail, social networks, and healthcare, a crucial task is forecasting multiple future events within a specified time horizon. Traditionally, this has been addressed through autoregressive generation using next-event prediction models, such as Marked Temporal Point Processes. However, autoregressive methods use their own output for future predictions, potentially reducing quality as the prediction horizon extends. In this paper, we challenge traditional approaches by introducing a novel benchmark, HoTPP, specifically designed to evaluate a model's ability to predict event sequences over a horizon. This benchmark features a new metric inspired by object detection in computer vision, addressing the limitations of existing metrics in assessing models with imprecise time-step predictions. Our evaluations on established datasets employing various models demonstrate that high accuracy in next-event prediction does not necessarily translate to superior horizon prediction, and vice versa. HoTPP aims to serve as a valuable tool for developing more robust event sequence prediction methods, ultimately paving the way for further advancements in the field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5c0701f646c8eaac8073e437a46b0dddf2f8bc4f" target='_blank'>
              HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?
              </a>
            </td>
          <td>
            Ivan Karpukhin, F. Shipilov, Andrey Savchenko
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="aeon is a unified Python 3 library for all machine learning tasks involving time series. The package contains modules for time series forecasting, classification, extrinsic regression and clustering, as well as a variety of utilities, transformations and distance measures designed for time series data. aeon also has a number of experimental modules for tasks such as anomaly detection, similarity search and segmentation. aeon follows the scikit-learn API as much as possible to help new users and enable easy integration of aeon estimators with useful tools such as model selection and pipelines. It provides a broad library of time series algorithms, including efficient implementations of the very latest advances in research. Using a system of optional dependencies, aeon integrates a wide variety of packages into a single interface while keeping the core framework with minimal dependencies. The package is distributed under the 3-Clause BSD license and is available at https://github.com/ aeon-toolkit/aeon. This version was submitted to the JMLR journal on 02 Nov 2023 for v0.5.0 of aeon. At the time of this preprint aeon has released v0.9.0, and has had substantial changes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6664110bc4fee7c79dd1928d2a37dd9222295260" target='_blank'>
              aeon: a Python toolkit for learning from time series
              </a>
            </td>
          <td>
            Matthew Middlehurst, Ali Ismail-Fawaz, Antoine Guillaume, Christopher Holder, David Guijo Rubio, Guzal Bulatova, Leonidas Tsaprounis, Lukasz Mentel, Martin Walter, Patrick Schäfer, Anthony J. Bagnall
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>2</td>
          <td>8</td>
        </tr>

        <tr id="Tabular data is ubiquitous in many real-life systems. In particular, time-dependent tabular data, where rows are chronologically related, is typically used for recording historical events, e.g., financial transactions, healthcare records, or stock history. Recently, hierarchical variants of the attention mechanism of transformer architectures have been used to model tabular time-series data. At first, rows (or columns) are encoded separately by computing attention between their fields. Subsequently, encoded rows (or columns) are attended to one another to model the entire tabular time-series. While efficient, this approach constrains the attention granularity and limits its ability to learn patterns at the field-level across separate rows, or columns. We take a first step to address this gap by proposing Fieldy, a fine-grained hierarchical model that contextualizes fields at both the row and column levels. We compare our proposal against state of the art models on regression and classification tasks using public tabular time-series datasets. Our results show that combining row-wise and column-wise attention improves performance without increasing model size. Code and data are available at https://github.com/raphaaal/fieldy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a9e5c4649872cb3a4b417acdce4fae0ea8d72ebf" target='_blank'>
              Fine-grained Attention in Hierarchical Transformers for Tabular Time-series
              </a>
            </td>
          <td>
            Raphaël Azorin, Zied Ben-Houidi, Massimo Gallo, A. Finamore, Pietro Michiardi
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>24</td>
        </tr>

        <tr id="We develop a deep learning surrogate model to accelerate parallel discrete event simulation (PDES) in high-performance computing (HPC) networks, addressing the computational challenges of traditional methods as HPC scales to larger and more complex systems. Our architecture stacks a 1D Convolutional Network, Long Short-Term Memory (LSTM), and a dense layer, trained using synthetic multivariate time series data from the CODES simulation framework. Preliminary results demonstrate the model’s promising performance in predicting application iteration times and its potential for generalization to other HPC workloads. With RSME metrics at an 81.5% improvement and MAE metrics up to 25.6% over baseline statistical methods, our deep learning surrogate approach signifies a shift towards rapid and less resource-intensive predictive models in the HPC simulation landscape.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/457ba0ded500b0bea788a7ccc997a477ca36e498" target='_blank'>
              Deep Learning Surrogate Models for Network Simulation
              </a>
            </td>
          <td>
            M. Dearing
          </td>
          <td>2024-06-24</td>
          <td>Proceedings of the 38th ACM SIGSIM Conference on Principles of Advanced Discrete Simulation</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="This paper presents a sequence of two approaches for the data-driven control-oriented modeling of networked systems, i.e., the systems that involve many interacting dynamical components. First, a novel deep learning approach named the weak Latent Dynamics Model (wLDM) is developed for learning generic nonlinear dynamics with control. Leveraging the weak form, the wLDM enables more numerically stable and computationally efficient training as well as more accurate prediction, when compared to conventional methods such as neural ordinary differential equations. Building upon the wLDM framework, we propose the weak Graph Koopman Bilinear Form (wGKBF) model, which integrates geometric deep learning and Koopman theory to learn latent space dynamics for networked systems, especially for the challenging cases having multiple timescales. The effectiveness of the wLDM framework and wGKBF model are demonstrated on three example systems of increasing complexity - a controlled double pendulum, the stiff Brusselator dynamics, and an electrified aircraft energy system. These numerical examples show that the wLDM and wGKBF achieve superior predictive accuracy and training efficiency as compared to baseline models. Parametric studies provide insights into the effects of hyperparameters in the weak form. The proposed framework shows the capability to efficiently capture control-dependent dynamics in these systems, including stiff dynamics and multi-physics interactions, offering a promising direction for learning control-oriented models of complex networked systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c2168dc84b931e82498692c0f9277bed180e91df" target='_blank'>
              Learning Networked Dynamical System Models with Weak Form and Graph Neural Networks
              </a>
            </td>
          <td>
            Yin Yu, Daning Huang, Seho Park, H. Pangborn
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>11</td>
        </tr>

        <tr id="A common problem of classical neural network architectures is that additional information or expert knowledge cannot be naturally integrated into the learning process. To overcome this limitation, we propose a two-step approach consisting of (1) generating rule functions from knowledge and (2) using these rules to define rule based layers -- a new type of dynamic neural network layer. The focus of this work is on the second step, i.e., rule based layers that are designed to dynamically arrange learnable parameters in the weight matrices and bias vectors depending on the input samples. Indeed, we prove that our approach generalizes classical feed-forward layers such as fully connected and convolutional layers by choosing appropriate rules. As a concrete application we present rule based graph neural networks (RuleGNNs) that overcome some limitations of ordinary graph neural networks. Our experiments show that the predictive performance of RuleGNNs is comparable to state-of-the-art graph classifiers using simple rules based on Weisfeiler-Leman labeling and pattern counting. Moreover, we introduce new synthetic benchmark graph datasets to show how to integrate expert knowledge into RuleGNNs making them more powerful than ordinary graph neural networks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/323f07171520382a593da0b618df114653ea4452" target='_blank'>
              Rule Based Learning with Dynamic (Graph) Neural Networks
              </a>
            </td>
          <td>
            Florian Seiffarth
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Short-Term Load Forecasting (STLF) plays a crucial role in energy management for planning and managing the operation strategy of hybrid energy sources. A common STLF approach involves making predictions based on multiple input variables which affect the load. However, additional data besides historical load data often are not available, making STLF a challenging univariate task. Two approaches are discussed in this paper for the univariate day-ahead STLF: the first integrates temporal information into the input regression vector, with Seasonal Auto-Regressive Moving Average (SARMA) and Prophet being the best representatives for the application considered; the second integrates temporal information directly into its architecture through the feedback of the internal state, with Long Short-Term Memory (LSTM) and vanilla Recurrent Neural Network (RNN) chosen for this purpose. The models were evaluated on a real case study involving a university building in Italy. The results validate the effectiveness of the LSTM model. LSTM performed significantly better than SARIMA, and slightly better than RNN and Prophet, achieving a Mean Absolute Error (MAE) equal to 13.12 kW, a Root Mean Square Error (RMSE) equal to 25.09 kW, a Mean Absolute Percentage Error (MAPE) equal to 10.97% and an R2 score equal to 0.85. This work provides a foundation for the improvement and deployment of such time series forecasting techniques in the emerging field of STLF for energy management.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e087a93b258393dffd6c5f3ffccb50ae4ffd55ee" target='_blank'>
              A Comparison of Univariate Methods for Day-Ahead Short-Term Load Forecasting
              </a>
            </td>
          <td>
            Giorgia Ghione, Malik Ali Judge, Marco Badami, E. Pasero, Vincenzo Franzitta, G. Cirrincione
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE 22nd Mediterranean Electrotechnical Conference (MELECON)</td>
          <td>0</td>
          <td>23</td>
        </tr>

        <tr id="Aiming at the problems of high stochasticity and volatility of power loads as well as the difficulty of accurate load forecasting, this paper proposes a power load forecasting method based on CEEMDAN (Completely Integrated Empirical Modal Decomposition) and TCN-LSTM (Temporal Convolutional Networks and Long-Short-Term Memory Networks). The method combines the decomposition of raw load data by CEEMDAN and the spatio-temporal modeling capability of TCN-LSTM model, aiming to improve the accuracy and stability of forecasting. First, the raw load data are decomposed into multiple linearly stable subsequences by CEEMDAN, and then the sample entropy is introduced to reorganize each subsequence. Then the reorganized sequences are used as inputs to the TCN-LSTM model to extract sequence features and perform training and prediction. The modeling prediction is carried out by selecting the electricity compliance data of New South Wales, Australia, and compared with the traditional prediction methods. The experimental results show that the algorithm proposed in this paper has higher accuracy and better prediction effect on load forecasting, which can provide a partial reference for electricity load forecasting methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/649e6e09e48b7c2a24d84425c09acc51e4accd7f" target='_blank'>
              Load forecasting method based on CEEMDAN and TCN-LSTM
              </a>
            </td>
          <td>
            Luo Heng, Cheng Hao, Liu Chen Nan
          </td>
          <td>2024-07-05</td>
          <td>PLOS ONE</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper proposes a new approach to classify and evaluate defects in concrete structures automatically. To overcome the limitations of defect detection methods that traditionally relied on expert visual observation, the reflection signal of electromagnetic pulses is extracted as time-series data and used to analyze the propagation characteristics of each defect. This study uses deep learning models to analyze these time-series data and classify defects. Since anomaly detection data has more normal data than anomaly data, data augmentation methods such as Time Warping, Noise Injection, Smoothing, Trend Shifting, etc., were applied to solve the problem of data imbalance and overfitting. Among them, Noise Injection showed the best performance. The generalization performance of the proposed method was evaluated through performance evaluation using LSTM, GRU, and TCN models, and LSTM models showed the highest performance. The study results show that the proposed method effectively classifies defect types in concrete structures and can solve the limitations of existing methods by automatic classification through deep learning models. In addition, it was confirmed that the model's performance could be improved by improving the amount and diversity of data by selecting and applying appropriate data augmentation methods. The contribution of the research is to present a new approach that automates the defect detection and classification task of concrete structures and provides high accuracy and efficiency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6c27c2e9eb2439808d402546380cd44a835817be" target='_blank'>
              Time-Series Data Augmentation for Improving Multi-Class Classification Performance
              </a>
            </td>
          <td>
            Woo-Hyeon Kim, Geon-Woo Kim, Jaeyoon Ahn, Kyungyong Chung
          </td>
          <td>2024-06-14</td>
          <td>International Journal on Advanced Science, Engineering and Information Technology</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a38bf5e77fa031380b4d6055f1501f99673fb6d1" target='_blank'>
              HEN: a novel hybrid explainable neural network based framework for robust network intrusion detection
              </a>
            </td>
          <td>
            Wei Wei, Sijin Chen, Cen Chen, Heshi Wang, Jing Liu, Zhongyao Cheng, Xiaofeng Zou
          </td>
          <td>2024-06-28</td>
          <td>Science China Information Sciences</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="The increasing adoption of rooftop photovoltaic (PV) power generation systems in residential areas necessitates accurate monitoring and disaggregation of behind-the-meter (BTM) load and PV power. Despite recent advancements, existing BTM disaggregation approaches suffer from three major drawbacks: neglecting task-relevant spatiotemporal features, overfitting, and lack of a sparse neural architecture which leads to high sample complexity. This paper addresses them by introducing a deep sparse attention graph recurrent framework. This framework conceptualizes a set of neighboring residential units as a graph where the nodes are the net load values of the units and the edges show the mutual information (MI) of these measurements. We develop an Attention Gated Recurrent Unit (AGRU) to capture enhanced temporal characteristics of the net load. We employ a novel low-rank Dictionary Learning (DL) method to discern spatiotemporal features of these measurements and further utilize a Rectified Linear Unit (ReLU) neural network that incorporates an MI-based dropout to provide a sparse model for the estimation of the BTM load and PV. Experimental results validate the effectiveness of our proposed model, exhibiting superior performance on the Ausgrid dataset in BTM load and PV power estimation compared to state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ba52197a4d7f5071efd214b6546a45314b15bb13" target='_blank'>
              Sparse Attention Graph Gated Recurrent Unit for Spatiotemporal Behind-The-Meter Load and PV Disaggregation
              </a>
            </td>
          <td>
            Mahdi Khodayar, Ali Farajzadeh Bavil, M. Saffari
          </td>
          <td>2024-06-27</td>
          <td>2024 16th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Many of today's data is time-series data originating from various sources, such as sensors, transaction systems, or production systems. Major challenges with such data include privacy and business sensitivity. Generative time-series models have the potential to overcome these problems, allowing representative synthetic data, such as people's movement in cities, to be shared openly and be used to the benefit of society at large. However, contemporary approaches are limited to prohibitively short sequences and small scales. Aside from major memory limitations, the models generate less accurate and less representative samples the longer the sequences are. This issue is further exacerbated by the lack of a comprehensive and accessible benchmark. Furthermore, a common need in practical applications is what-if analysis and dynamic adaptation to data distribution changes, for usage in decision making and to manage a changing world: What if this road is temporarily blocked or another road is added? The focus of this paper is on mobility data, such as people's movement in cities, requiring all these issues to be addressed. To this end, we propose a transformer-based diffusion model, TDDPM, for time-series which outperforms and scales substantially better than state-of-the-art. This is evaluated in a new comprehensive benchmark across several sequence lengths, standard datasets, and evaluation measures. We also demonstrate how the model can be conditioned on a prior over spatial occupancy frequency information, allowing the model to generate mobility data for previously unseen environments and for hypothetical scenarios where the underlying road network and its usage changes. This is evaluated by training on mobility data from part of a city. Then, using only aggregate spatial information as prior, we demonstrate out-of-distribution generalization to the unobserved remainder of the city.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1d7a486a59e6327be349015918f17ee84f6c4776" target='_blank'>
              Deep Temporal Deaggregation: Large-Scale Spatio-Temporal Generative Models
              </a>
            </td>
          <td>
            David Bergstrom, Mattias Tiger, Fredrik Heintz
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="$\textbf{This is the conference version of our paper: Spatiotemporal Implicit Neural Representation as a Generalized Traffic Data Learner}$. Spatiotemporal Traffic Data (STTD) measures the complex dynamical behaviors of the multiscale transportation system. Existing methods aim to reconstruct STTD using low-dimensional models. However, they are limited to data-specific dimensions or source-dependent patterns, restricting them from unifying representations. Here, we present a novel paradigm to address the STTD learning problem by parameterizing STTD as an implicit neural representation. To discern the underlying dynamics in low-dimensional regimes, coordinate-based neural networks that can encode high-frequency structures are employed to directly map coordinates to traffic variables. To unravel the entangled spatial-temporal interactions, the variability is decomposed into separate processes. We further enable modeling in irregular spaces such as sensor graphs using spectral embedding. Through continuous representations, our approach enables the modeling of a variety of STTD with a unified input, thereby serving as a generalized learner of the underlying traffic dynamics. It is also shown that it can learn implicit low-rank priors and smoothness regularization from the data, making it versatile for learning different dominating data patterns. We validate its effectiveness through extensive experiments in real-world scenarios, showcasing applications from corridor to network scales. Empirical results not only indicate that our model has significant superiority over conventional low-rank models, but also highlight that the versatility of the approach. We anticipate that this pioneering modeling perspective could lay the foundation for universal representation of STTD in various real-world tasks. $\textbf{The full version can be found at:}$ https://doi.org/10.48550/arXiv.2405.03185.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bfa0182b430cbb508346d06c6e1e9d4390a866f6" target='_blank'>
              Generalizable Implicit Neural Representation As a Universal Spatiotemporal Traffic Data Learner
              </a>
            </td>
          <td>
            Tong Nie, Guoyang Qin, Wei Ma, Jiangming Sun
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Large language models (LLMs) have shown their potential in long-context understanding and mathematical reasoning. In this paper, we study the problem of using LLMs to detect tabular anomalies and show that pre-trained LLMs are zero-shot batch-level anomaly detectors. That is, without extra distribution-specific model fitting, they can discover hidden outliers in a batch of data, demonstrating their ability to identify low-density data regions. For LLMs that are not well aligned with anomaly detection and frequently output factual errors, we apply simple yet effective data-generating processes to simulate synthetic batch-level anomaly detection datasets and propose an end-to-end fine-tuning strategy to bring out the potential of LLMs in detecting real anomalies. Experiments on a large anomaly detection benchmark (ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art transductive learning-based anomaly detection methods and ii) the efficacy of our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e8ef63be9a9a07e55656bd6372cd96cc4ef37f90" target='_blank'>
              Anomaly Detection of Tabular Data Using LLMs
              </a>
            </td>
          <td>
            Aodong Li, Yunhan Zhao, Chen Qiu, M. Kloft, P. Smyth, Maja Rudolph, S. Mandt
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>39</td>
        </tr>

        <tr id="Accurately predicting PM2.5 is a crucial task for protecting public health and making policy decisions. In the meanwhile, it is also a challenging task, given the complex spatio-temporal patterns of PM2.5 concentrations. Recently, the utilization of graph neural network (GNN) models has emerged as a promising approach, demonstrating significant advantages in capturing the spatial and temporal dependencies associated with PM2.5 concentrations. In this work, we collected a comprehensive dataset spanning 308 cities in China, encompassing data on seven pollutants as well as meteorological variables from January 2015 to September 2022. To effectively predict the PM2.5 concentrations, we propose a graph attention recurrent neural network (GARNN) model by taking into account both meteorological and geographical information. Extensive experiments validated the efficiency of the proposed GARNN model, revealing its superior performance compared to other existing methods in terms of predictive capabilities. This study contributes to advancing the understanding and prediction of PM2.5 concentrations, providing a valuable tool for addressing environmental challenges.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f0d3185a50e22682b8c650ceac52e70dd3a9849f" target='_blank'>
              A Graph Attention Recurrent Neural Network Model for PM2.5 Prediction: A Case Study in China from 2015 to 2022
              </a>
            </td>
          <td>
            Rui Pan, Tuozhen Liu, Lingfei Ma
          </td>
          <td>2024-07-03</td>
          <td>Atmosphere</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Weakly-supervised anomaly detection can outperform existing unsupervised methods with the assistance of a very small number of labeled anomalies, which attracts increasing attention from researchers. However, existing weakly-supervised anomaly detection methods are limited as these methods do not factor in the multimodel nature of the real-world data distribution. To mitigate this, we propose the Weakly-supervised Variational-mixture-model-based Anomaly Detector (WVAD). WVAD excels in multimodal datasets. It consists of two components: a deep variational mixture model, and an anomaly score estimator. The deep variational mixture model captures various features of the data from different clusters, then these features are delivered to the anomaly score estimator to assess the anomaly levels. Experimental results on three real-world datasets demonstrate WVAD's superiority.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/801c3e22d6b8f9b7d8284e864c12306bd0a9d2f8" target='_blank'>
              Weakly-supervised anomaly detection for multimodal data distributions
              </a>
            </td>
          <td>
            Xu Tan, Junqi Chen, S. Rahardja, Jiawei Yang, S. Rahardja
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>37</td>
        </tr>

        <tr id="
Purpose
With the introduction of graph structure learning into service classification, more accurate graph structures can significantly improve the precision of service classification. However, existing graph structure learning methods tend to rely on a single information source when attempting to eliminate noise in the original graph structure and lack consideration for the graph generation mechanism. To address this problem, this paper aims to propose a graph structure estimation neural network-based service classification (GSESC) model.


Design/methodology/approach
First, this method uses the local smoothing properties of graph convolutional networks (GCN) and combines them with the stochastic block model to serve as the graph generation mechanism. Next, it constructs a series of observation sets reflecting the intrinsic structure of the service from different perspectives to minimize biases introduced by a single information source. Subsequently, it integrates the observation model with the structural model to calculate the posterior distribution of the graph structure. Finally, it jointly optimizes GCN and the graph estimation process to obtain the optimal graph.


Findings
The authors conducted a series of experiments on the API data set and compared it with six baseline methods. The experimental results demonstrate the effectiveness of the GSESC model in service classification.


Originality/value
This paper argues that the data set used for service classification exhibits a strong community structure. In response to this, the paper innovatively applies a graph-based learning model that considers the underlying generation mechanism of the graph to the field of service classification and achieves good results.
">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bf894a391c3d807ee0681ba4298f9d1a4d22ad2a" target='_blank'>
              Graph structure estimation neural network-based service classification
              </a>
            </td>
          <td>
            Yanxinwen Li, Ziming Xie, Buqing Cao, Hua Lou
          </td>
          <td>2024-06-24</td>
          <td>International Journal of Web Information Systems</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cc40ef5fcacd95049db40e8f30973bbc30e18c03" target='_blank'>
              DGCPPISP: a PPI site prediction model based on dynamic graph convolutional network and two-stage transfer learning
              </a>
            </td>
          <td>
            Zijian Feng, Weihong Huang, Haohao Li, Hancan Zhu, Yanlei Kang, Zhong Li
          </td>
          <td>2024-07-31</td>
          <td>BMC Bioinformatics</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Objectives. To build neural network models of time series (LSTM, GRU, RNN) and compare the results of forecasting with their mutual help and the results of standard models (ARIMA, ETS), in order to ascertain in which cases a certain group of models should be used.Methods. The paper provides a review of neural network models and considers the structure of RNN, LSTM, and GRU models. They are used for modeling time series in Russian macroeconomic statistics. The quality of model adjustment to the data and the quality of forecasts are compared experimentally. Neural network and standard models can be used both for the entire series and for its parts (trend and seasonality). When building a forecast for several time intervals in the future, two approaches are considered: building a forecast for the entire interval at once, and step-by-step forecasting. In this way there are several combinations of models that can be used for forecasting. These approaches are analyzed in the computational experiment.Results. Several experiments have been conducted in which standard (ARIMA, ETS, LOESS) and neural network models (LSTM, GRU, RNN) are built and compared in terms of proximity of the forecast to the series data in the test period.Conclusions. In the case of seasonal time series, models based on neural networks surpassed the standard ARIMA and ETS models in terms of forecast accuracy for the test period. The single-step forecast is computationally less efficient than the integral forecast for the entire target period. However, it is not possible to accurately indicate which approach is the best in terms of quality for a given series. Combined models (neural networks for trend, ARIMA for seasonality) almost always give good results. When forecasting a non-seasonal heteroskedastic series of share price, the standard approaches (LOESS method and ETS model) showed the best results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/18b38c6aa6bc97b32d3a4d813c85a98d07c92494" target='_blank'>
              Neural network analysis in time series forecasting
              </a>
            </td>
          <td>
            B. Pashshoev, D. Petrusevich
          </td>
          <td>2024-08-05</td>
          <td>Russian Technological Journal</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Online ride-hailing services play a crucial role in daily transportation, However, challenges persist in certain regions with limited access, and drivers encounter difficulties in receiving orders. Accurate prediction of short-term origin-destination (OD) demand is crucial for addressing these issues. This study leverages recent advancements in artificial intelligence and big data to introduce a spatiotemporal encoder-decoder network with a residual feature extractor (RF-STED) for short-term OD demand prediction in online ride-hailing services. The RF-STED model, built on deep learning models such as graph convolutional networks and convolutional long short-term memory (Conv-LSTM), includes spatiotemporal networks, encoding layers, and a residual feature extractor. The spatiotemporal network has two branches: branch one processes multi-pattern OD data using a multi-pattern temporal feature extraction module, utilizing a multi-channel Conv-LSTM to capture temporal correlations. Branch two utilizes a multi-spatial feature extraction module to convert OD pair associations into a spatial topology, extracting multi-spatial correlations. The encoding layer captures spatiotemporal dependencies, while the residual feature extractor decodes compressed vectors back into an OD graph for forecasting future demand. Experiments with a Manhattan taxi dataset in the U.S. show the RF-STED model outperforms 10 baseline models and four ablation models. The results emphasize the model’s strength and robustness in short-term OD flow prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bc47a33b6b82630673ca7bacf081d0236a8624bd" target='_blank'>
              Short-Term Origin-Destination Demand Prediction Based on Spatiotemporal Encoder-Decoder Network with a Residual Feature Extractor
              </a>
            </td>
          <td>
            Xiaohui Zhong, Jinlei Zhang, Qiang Hua, Lixing Yang, Ziyou Gao
          </td>
          <td>2024-07-24</td>
          <td>Transportation Research Record: Journal of the Transportation Research Board</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Graph neural networks (GNNs) have gained significant attention in diverse domains, ranging from urban planning to pandemic management. Ensuring both accuracy and robustness in GNNs remains a challenge due to insufficient quality data that contains sufficient features. With sufficient training data where all spatiotemporal patterns are well-represented, existing GNN models can make reasonably accurate predictions. However, existing methods fail when the training data are drawn from different circumstances (e.g., traffic patterns on regular days) than test data (e.g., traffic patterns after a natural disaster). Such challenges are usually classified under domain generalization. In this work, we show that one way to address this challenge in the context of spatiotemporal prediction is by incorporating domain differential equations into graph convolutional networks (GCNs). We theoretically derive conditions where GCNs incorporating such domain differential equations are robust to mismatched training and testing data compared to baseline domain agnostic models. To support our theory, we propose two domain-differential-equation-informed networks: Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates differential equations for traffic speed evolution, and the Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which incorporates a disease propagation model. Both RDGCN and SIRGCN are based on reliable and interpretable domain differential equations that allow the models to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN are more robust with mismatched testing data than state-of-the-art deep learning methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0ce74ab023edbff0d9a6fad7e1a98a69c76287cb" target='_blank'>
              On the generalization discrepancy of spatiotemporal dynamics-informed graph convolutional networks
              </a>
            </td>
          <td>
            Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S. Blum, Parv Venkitasubramaniam
          </td>
          <td>2024-07-12</td>
          <td>Frontiers in Mechanical Engineering</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ca70c38270fe1f47a4e82ae81496c70c0e541ccf" target='_blank'>
              Multivariate Bayesian Time-Series Model with Multi-temporal Convolution Network for Forecasting Stock Market During COVID-19 Pandemic
              </a>
            </td>
          <td>
            Paramita Ray, B. Ganguli, Amlan Chakrabarti
          </td>
          <td>2024-06-27</td>
          <td>Int. J. Comput. Intell. Syst.</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Forecasting multivariate time series is a computationally intensive task challenged by extreme or redundant samples. Recent resampling methods aim to increase training efficiency by reweighting samples based on their running losses. However, these methods do not solve the problems caused by heavy-tailed distribution losses, such as overfitting to outliers. To tackle these issues, we introduce a novel approach: a Gaussian loss-weighted sampler that multiplies their running losses with a Gaussian distribution weight. It reduces the probability of selecting samples with very low or very high losses while favoring those close to average losses. As it creates a weighted loss distribution that is not heavy-tailed theoretically, there are several advantages to highlight compared to existing methods: 1) it relieves the inefficiency in learning redundant easy samples and overfitting to outliers, 2) It improves training efficiency by preferentially learning samples close to the average loss. Application on real-world time series forecasting datasets demonstrate improvements in prediction quality for 1%-4% using mean square error measurements in channel-independent settings. The code will be available online after 1 the review.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ad56f787b7bab71041c092d434e98cae941c807c" target='_blank'>
              Robust Time Series Forecasting with Non-Heavy-Tailed Gaussian Loss-Weighted Sampler
              </a>
            </td>
          <td>
            Jiang You, A. Çela, Ren'e Natowicz, Jacob Ouanounou, Patrick Siarry
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>20</td>
        </tr>

        <tr id="
 One of the core assumptions of most deep-learning-based data-driven models is that samples are independent. However, this assumption poses a key challenge in production forecasting—performance is influenced by well interference and reservoir connectivity. Most shale gas wells are hydraulically fractured and exist in complex fracture systems, and the neighboring well characteristics should also be considered when constructing data-driven forecast models. Researchers have explored using the graph convolutional network (GCN) to address this issue by incorporating neighboring well characteristics into production forecasting models. However, applying GCN to field-scale studies is problematic, as it requires training on a full batch, leading to gigantic cache allocation. In addition, the transductive nature of GCN poses challenges for direct generalization to unseen nodes. To overcome these limitations, we adopt the graph sampling and aggregation (GraphSAGE) network architecture, which allows training large graphs with batches and generalizing predictions for previously unseen nodes. By utilizing the gated recurrent unit (GRU) network, the proposed spatial-temporal (ST)-GraphSAGE model can capture cross-time relationships between the target and the neighboring wells and generate promising prediction time series for the target wells, even if they are newly drilled wells.
 The proposed approach is validated and tested using the field data from 2,240 Montney shale gas wells, including formation properties, hydraulic fracture parameters, production history, and operational data. The algorithm aggregates the first-hop information to the target node for each timestep. The encoder-decoder (ED) architecture is used to generate forecasts for the subsequent 3-year production rate by using the 1-year production history of the wells. The trained model enables the evaluation of production predictions for newly developed wells at any location. We evaluate the model’s performance using P10, P50, and P90 of the test data set’s root mean square error (RMSE). Our method preserves the topological characteristics of wells and generalizes the prediction to unseen nodes while significantly reducing training complexity, making it applicable to larger data sets. By incorporating information from adjacent wells and integrating ST data, our ST-GraphSAGE model outperforms the traditional GRU-ED model and shows enhanced interpretability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3acb14c12dea5fe005f993df58cb0487b23127f2" target='_blank'>
              Shale Gas Production Forecasting with Well Interference Based on Spatial-Temporal Graph Convolutional Network
              </a>
            </td>
          <td>
            Ziming Xu, Juliana Y. Leung
          </td>
          <td>2024-07-01</td>
          <td>SPE Journal</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Rethink convolution-based graph neural networks (GNN) -- they characteristically suffer from limited expressiveness, over-smoothing, and over-squashing, and require specialized sparse kernels for efficient computation. Here, we design a simple graph learning module entirely free of convolution operators, coined random walk with unifying memory (RUM) neural network, where an RNN merges the topological and semantic graph features along the random walks terminating at each node. Relating the rich literature on RNN behavior and graph topology, we theoretically show and experimentally verify that RUM attenuates the aforementioned symptoms and is more expressive than the Weisfeiler-Lehman (WL) isomorphism test. On a variety of node- and graph-level classification and regression tasks, RUM not only achieves competitive performance, but is also robust, memory-efficient, scalable, and faster than the simplest convolutional GNNs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9b451516b9432318d81aef2a5bdc0135d2285a5d" target='_blank'>
              Non-convolutional Graph Neural Networks
              </a>
            </td>
          <td>
            Yuanqing Wang, Kyunghyun Cho
          </td>
          <td>2024-07-31</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Temporal data modelling techniques with neural networks are useful in many domain applications, including time-series forecasting and control engineering. This paper aims at developing a recurrent version of stochastic configuration networks (RSCNs) for problem solving, where we have no underlying assumption on the dynamic orders of the input variables. Given a collection of historical data, we first build an initial RSCN model in the light of a supervisory mechanism, followed by an online update of the output weights by using a projection algorithm. Some theoretical results are established, including the echo state property, the universal approximation property of RSCNs for both the offline and online learnings, and the convergence of the output weights. The proposed RSCN model is remarkably distinguished from the well-known echo state networks (ESNs) in terms of the way of assigning the input random weight matrix and a special structure of the random feedback matrix. A comprehensive comparison study among the long short-term memory (LSTM) network, the original ESN, and several state-of-the-art ESN methods such as the simple cycle reservoir (SCR), the polynomial ESN (PESN), the leaky-integrator ESN (LIESN) and RSCN is carried out. Numerical results clearly indicate that the proposed RSCN performs favourably over all of the datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/46e3c55f0b0e77192fb6cc422ecf599a47334d73" target='_blank'>
              Recurrent Stochastic Configuration Networks for Temporal Data Analytics
              </a>
            </td>
          <td>
            Dianhui Wang, Gang Dang
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="This paper presents a novel anomaly detection methodology termed Statistical Aggregated Anomaly Detection (SAAD). The SAAD approach integrates advanced statistical techniques with machine learning, and its efficacy is demonstrated through validation on real sensor data from a Hardware-in-the-Loop (HIL) environment within the automotive domain. The key innovation of SAAD lies in its ability to significantly enhance the accuracy and robustness of anomaly detection when combined with Fully Connected Networks (FCNs) augmented by dropout layers. Comprehensive experimental evaluations indicate that the standalone statistical method achieves an accuracy of 72.1%, whereas the deep learning model alone attains an accuracy of 71.5%. In contrast, the aggregated method achieves a superior accuracy of 88.3% and an F1 score of 0.921, thereby outperforming the individual models. These results underscore the effectiveness of SAAD, demonstrating its potential for broad application in various domains, including automotive systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2370c64d4952ea9a3426ac33a375058f5d5b22c1" target='_blank'>
              Enhanced Anomaly Detection in Automotive Systems Using SAAD: Statistical Aggregated Anomaly Detection
              </a>
            </td>
          <td>
            Dacian Goina, Eduard Hogea, George Matieş
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The ever-growing speed at which data are generated nowadays, together with the substantial cost of labeling processes cause Machine Learning models to face scenarios in which data are partially labeled. The extreme case where such a supervision is indefinitely unavailable is referred to as extreme verification latency. On the other hand, in streaming setups data flows are affected by exogenous factors that yield non-stationarities in the patterns (concept drift), compelling models learned incrementally from the data streams to adapt their modeled knowledge to the concepts within the stream. In this work we address the casuistry in which these two conditions occur together, by which adaptation mechanisms to accommodate drifts within the stream are challenged by the lack of supervision, requiring further mechanisms to track the evolution of concepts in the absence of verification. To this end we propose a novel approach, AiGAS-dEVL (Adaptive Incremental neural GAS model for drifting Streams under Extreme Verification Latency), which relies on growing neural gas to characterize the distributions of all concepts detected within the stream over time. Our approach exposes that the online analysis of the behavior of these prototypical points over time facilitates the definition of the evolution of concepts in the feature space, the detection of changes in their behavior, and the design of adaptation policies to mitigate the effect of such changes in the model. We assess the performance of AiGAS-dEVL over several synthetic datasets, comparing it to that of state-of-the-art approaches proposed in the recent past to tackle this stream learning setup. Our results reveal that AiGAS-dEVL performs competitively with respect to the rest of baselines, exhibiting a superior adaptability over several datasets in the benchmark while ensuring a simple and interpretable instance-based adaptation strategy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f5844c538f91ea5b207c4697abcca3340a5aac57" target='_blank'>
              AiGAS-dEVL: An Adaptive Incremental Neural Gas Model for Drifting Data Streams under Extreme Verification Latency
              </a>
            </td>
          <td>
            Maria Arostegi, Miren Nekane Bilbao, J. Lobo, J. Ser
          </td>
          <td>2024-07-07</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>42</td>
        </tr>

        <tr id="Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET's superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b21605b67376e0377590d3cdfb99e3c43495b865" target='_blank'>
              JANET: Joint Adaptive predictioN-region Estimation for Time-series
              </a>
            </td>
          <td>
            Eshant English, Eliot Wong-Toi, Matteo Fontana, Stephan Mandt, P. Smyth, Christoph Lippert
          </td>
          <td>2024-07-08</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Time series data is widely available in a variety of industries. By forecasting time series, decision-makers can better grasp future trends and make more effective decisions. Financial time series data exhibit non-stationarity and high volatility. High-frequency fluctuations in financial products such as exchange rates, bonds and equities may reflect external shocks and risks in global financial markets, which are potentially dangerous and may threaten national economic security or even trigger financial crises. For financial time series data, a deep recurrent neural network first progressively processes each data point in the time series through its recurrent unit. Each recurring unit can adjust its own weights to better predict or analyze future values. Over time, these recurrent units continuously update their internal state, resulting in a comprehensive understanding of the characteristics of the entire data sequence. In addition, we add a gating mechanism to further improve the network's ability to control the flow of information, so that the model is more effective when retaining long-term dependencies, so as to improve the accuracy of prediction and the stability of the model. Experimental results show that our recurrent neural network model shows higher prediction accuracy and stability than other baseline models on financial time series datasets.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6f5f866e6e62fae9396ebd199496590a68d45ac8" target='_blank'>
              Research on time-series financial data prediction and analysis based on deep recurrent neural network
              </a>
            </td>
          <td>
            Feng Yuan
          </td>
          <td>2024-07-25</td>
          <td>Applied and Computational Engineering</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="This paper introduces new methodology based on the field of Topological Data Analysis for detecting anomalies in multivariate time series, that aims to detect global changes in the dependency structure between channels. The proposed approach is lean enough to handle large scale datasets, and extensive numerical experiments back the intuition that it is more suitable for detecting global changes of correlation structures than existing methods. Some theoretical guarantees for quantization algorithms based on dependent time sequences are also provided.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e2e901eccd1f97fc280f7ea803690f095d687fc7" target='_blank'>
              Topological Analysis for Detecting Anomalies (TADA) in Time Series
              </a>
            </td>
          <td>
            Fr'ed'eric Chazal, Martin Royer, Clément Levrard
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Generative artificial intelligence (AI) has developed as an effective tool for time series predicting, revolutionizing the typical methods of prediction. Different classical approaches that depend on existing approaches and assumptions, generative AI controls advanced deep learning (DL) approaches like generative adversarial networks (GANs) and recurrent neural networks (RNNs), to identify designs and connections in time series data. DL has accomplished major success in optimizing performances connected with AI. In the financial area, it can be extremely utilized for the stock market predictive, trade implementation approaches, and set of optimizers. Stock market predictive is the most important use case in this field. GANs with advanced AI approaches have become more significant in recent times. However, it can be utilized in image‐image‐translation and other computer vision (CV) conditions. GANs could not utilized greatly for stock market prediction because of their effort to establish the proper set of hyperparameters. This study develops an integrated spotted hyena optimization algorithm with generative artificial intelligence for time series forecasting (SHOAGAI‐TSF) technique. The purpose of the SHOAGAI‐TSF technique is to accomplish a forecasting process for the utilization of stock price prediction. The SHOAGAI‐TSF technique uses probabilistic forecasting with a conditional GAN (CGAN) approach for the prediction of stock prices. The CGAN model learns the data generation distribution and determines the probabilistic prediction from it. To boost the prediction results of the CGAN approach, the hyperparameter tuning can be performed by the use of the SHOA. The simulation result analysis of the SHOAGAI‐TSF technique takes place on the stock market dataset. The experimental outcomes determine the significant solution of the SHOAGAI‐TSF algorithm with other compared methods in terms of distinct metrics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cf996280ce1c70670f339e7e20d6248fb2ceaa21" target='_blank'>
              Integrating spotted hyena optimization technique with generative artificial intelligence for time series forecasting
              </a>
            </td>
          <td>
            Reda Salama
          </td>
          <td>2024-08-01</td>
          <td>Expert Systems</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The detection of abnormal or critical system states is essential in condition monitoring. While much attention is given to promptly identifying anomalies, a retrospective analysis of these anomalies can significantly enhance our comprehension of the underlying causes of observed undesired behavior. This aspect becomes particularly critical when the monitored system is deployed in a vital environment. In this study, we delve into anomalies within the domain of Bio-Regenerative Life Support Systems (BLSS) for space exploration and analyze anomalies found in telemetry data stemming from the EDEN ISS space greenhouse in Antarctica. We employ time series clustering on anomaly detection results to categorize various types of anomalies in both uni- and multivariate settings. We then assess the effectiveness of these methods in identifying systematic anomalous behavior. Additionally, we illustrate that the anomaly detection methods MDI and DAMP produce complementary results, as previously indicated by research.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ab350916e6aea74a31cb7ea03c853050908b6fd5" target='_blank'>
              Unraveling Anomalies in Time: Unsupervised Discovery and Isolation of Anomalous Behavior in Bio-regenerative Life Support System Telemetry
              </a>
            </td>
          <td>
            Ferdinand Rewicki, J. Gawlikowski, Julia Niebling, Joachim Denzler
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85961a97dd0769d04e19fd1588cb93092c4d8280" target='_blank'>
              Multi-view Heterogeneous Graph Neural Networks for Node Classification
              </a>
            </td>
          <td>
            Xi Zeng, Fangyuan Lei, Changdong Wang, Qing-Yun Dai
          </td>
          <td>2024-06-24</td>
          <td>Data Science and Engineering</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="This study addresses the prediction of CAN bus data, a lesser-explored aspect within unsupervised anomaly detection research. We propose the Fast-Gated Attention (FGA) Transformer, a novel approach designed for accurate and efficient prediction of CAN bus data. This model utilizes a cross-attention window to optimize computational scale and feature extraction, a gated single-head attention mechanism in place of multi-head attention, and shared parameters to minimize model size. Additionally, a generalized unbiased linear attention approximation technique speeds up attention block computation. On three datasets—Car-Hacking, SynCAN, and Automotive Sensors—the FGA Transformer achieves predicted root mean square errors of 1.86 × 10−3, 3.03 × 10−3, and 30.66 × 10−3, with processing speeds of 2178, 2768, and 3062 frames per second, respectively. The FGA Transformer provides the best or comparable accuracy with a speed improvement ranging from 6 to 170 times over existing methods, underscoring its potential for CAN bus data prediction.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/29ae49c10fdafb110a22645dbea2e2c775a56ed9" target='_blank'>
              A Multivariate Time Series Prediction Method for Automotive Controller Area Network Bus Data
              </a>
            </td>
          <td>
            Dan Yang, Shuya Yang, Junsuo Qu, Ke Wang
          </td>
          <td>2024-07-10</td>
          <td>Electronics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="
Purpose
This study aims to introduce an innovative approach to predictive maintenance by integrating time-series sensor data with event logs, leveraging the synergistic potential of deep learning models. The primary goal is to enhance the accuracy of equipment failure predictions, thereby minimizing operational downtime.


Design/methodology/approach
The methodology uses a dual-model architecture, combining the patch time series transformer (PatchTST) model for analyzing time-series sensor data and bidirectional encoder representations from transformers for processing textual event log data. Two distinct fusion strategies, namely, early and late fusion, are explored to integrate these data sources effectively. The early fusion approach merges data at the initial stages of processing, while late fusion combines model outputs toward the end. This research conducts thorough experiments using real-world data from wind turbines to validate the approach.


Findings
The results demonstrate a significant improvement in fault prediction accuracy, with early fusion strategies outperforming traditional methods by 2.6% to 16.9%. Late fusion strategies, while more stable, underscore the benefit of integrating diverse data types for predictive maintenance. The study provides empirical evidence of the superiority of the fusion-based methodology over singular data source approaches.


Originality/value
This research is distinguished by its novel fusion-based approach to predictive maintenance, marking a departure from conventional single-source data analysis methods. By incorporating both time-series sensor data and textual event logs, the study unveils a comprehensive and effective strategy for fault prediction, paving the way for future advancements in the field.
">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/93fff1cb1c0641b2ba19f4a98796d9fd975baded" target='_blank'>
              Advancing predictive maintenance: a deep learning approach to sensor and event-log data fusion
              </a>
            </td>
          <td>
            Zengkun Liu, Justine Hui
          </td>
          <td>2024-07-09</td>
          <td>Sensor Review</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="High-resolution time series data are crucial for operation and planning in energy systems such as electrical power systems and heating systems. However, due to data collection costs and privacy concerns, such data is often unavailable or insufficient for downstream tasks. Data synthesis is a potential solution for this data scarcity. With the recent development of generative AI, we propose EnergyDiff, a universal data generation framework for energy time series data. EnergyDiff builds on state-of-the-art denoising diffusion probabilistic models, utilizing a proposed denoising network dedicated to high-resolution time series data and introducing a novel Marginal Calibration technique. Our extensive experimental results demonstrate that EnergyDiff achieves significant improvement in capturing temporal dependencies and marginal distributions compared to baselines, particularly at the 1-minute resolution. Additionally, EnergyDiff consistently generates high-quality time series data across diverse energy domains, time resolutions, and at both customer and transformer levels with reduced computational need.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/069d06dea29e8386626d706e3527becdc39fcdfe" target='_blank'>
              EnergyDiff: Universal Time-Series Energy Data Generation using Diffusion Models
              </a>
            </td>
          <td>
            Nan Lin, Peter Palensky, Pedro P. Vergara
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id=" Classification of sensor data plays a crucial role in different fields, aiding in important tasks like detecting faults, recognizing events and predicting maintenance needs. This research paper thoroughly explores the use of machine learning methods, in specific Multi-Layer Perceptron (MLP) networks, to classify sensor data. The dataset used in this study includes raw measurements from all 24 ultrasound sensors alongside their corresponding class labels indicating the robots actions (such as moving or turning left). The study tackles challenges like imbalanced classes, noisy signals, and striving for classification through a case study employing MLP models. Through conducting experiments and analysis, we fine-tuned the MLP models setup to achieve a 93.04% accuracy on the test dataset. Additionally evaluation metrics like precision, recall and F1 score highlighted the models effectiveness across different classes. A comparison with Support Vector Machines (SVM) and Logistic Regression models highlighted the performance of the MLP model. These results not only show the effectiveness of MLP networks but also provide valuable insights into best practices, for classifying sensor data. Through examining the intricacies of sensor data analysis and classification this study contributes to enhancing our knowledge of applying machine learning to real world challenges.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cdae3caaf8ae8e2350b1f016723151109bb56210" target='_blank'>
              Forecasting Robot Movement with Sensor Readings and Multi-Layer Perceptron Models
              </a>
            </td>
          <td>
            Sarah Sabeeh
          </td>
          <td>2024-07-07</td>
          <td>Misan Journal of Engineering Sciences</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Temporal networks are effective in capturing the evolving interactions of networks over time, such as social networks and e-commerce networks. In recent years, researchers have primarily concentrated on developing specific model architectures for Temporal Graph Neural Networks (TGNNs) in order to improve the representation quality of temporal nodes and edges. However, limited attention has been given to the quality of negative samples during the training of TGNNs. When compared with static networks, temporal networks present two specific challenges for negative sampling: positive sparsity and positive shift. Positive sparsity refers to the presence of a single positive sample amidst numerous negative samples at each timestamp, while positive shift relates to the variations in positive samples across different timestamps. To robustly address these challenges in training TGNNs, we introduce Curriculum Negative Mining (CurNM), a model-aware curriculum learning framework that adaptively adjusts the difficulty of negative samples. Within this framework, we first establish a dynamically updated negative pool that balances random, historical, and hard negatives to address the challenges posed by positive sparsity. Secondly, we implement a temporal-aware negative selection module that focuses on learning from the disentangled factors of recently active edges, thus accurately capturing shifting preferences. Extensive experiments on 12 datasets and 3 TGNNs demonstrate that our method outperforms baseline methods by a significant margin. Additionally, thorough ablation studies and parameter sensitivity experiments verify the usefulness and robustness of our approach. Our code is available at https://github.com/zziyue83/CurNM.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/56efc5b54a9beda670261028d60fe6056619863a" target='_blank'>
              Curriculum Negative Mining For Temporal Networks
              </a>
            </td>
          <td>
            Ziyue Chen, Tongya Zheng, Mingli Song
          </td>
          <td>2024-07-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Deep neural networks (DNNs) are prominent in predictive analytics for accurately forecasting target variables. However, inherent uncertainties necessitate constructing prediction intervals for reliability. The existing literature often lacks practical methodologies for creating predictive intervals, especially for time series with trends and seasonal patterns. This paper explicitly details a practical approach integrating dual-output Monte Carlo Dropout (MCDO) with DNNs to approximate predictive means and variances within a Bayesian framework, enabling forecast interval construction. The dual-output architecture employs a custom loss function, combining mean squared error with Softplus-derived predictive variance, ensuring non-negative variance values. Hyperparameter optimization is performed through a grid search exploring activation functions, dropout rates, epochs, and batch sizes. Empirical distributions of predictive means and variances from the MCDO demonstrate the results of the dual-output MCDO DNNs. The proposed method achieves a significant improvement in forecast accuracy, with an RMSE reduction of about 10% compared to the seasonal autoregressive integrated moving average model. Additionally, the method provides more reliable forecast intervals, as evidenced by a higher coverage proportion and narrower interval widths. A case study on Thailand’s durian export data showcases the method’s utility and applicability to other datasets with trends and/or seasonal components.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/48e9c93416cccc0d5c4d1f0646a11cd372d3ec5b" target='_blank'>
              Time-Series Interval Forecasting with Dual-Output Monte Carlo Dropout: A Case Study on Durian Exports
              </a>
            </td>
          <td>
            Unyamanee Kummaraka, Patchanok Srisuradetchai
          </td>
          <td>2024-08-02</td>
          <td>Forecasting</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Macroeconomic data are crucial for monitoring countries' performance and driving policy. However, traditional data acquisition processes are slow, subject to delays, and performed at a low frequency. We address this 'ragged-edge' problem with a two-step framework. The first step is a supervised learning model predicting observed low-frequency figures. We propose a neural-network-based nowcasting model that exploits mixed-frequency, high-dimensional data. The second step uses the elasticities derived from the previous step to interpolate unobserved high-frequency figures. We apply our method to nowcast countries' yearly research and development (R&D) expenditure series. These series are collected through infrequent surveys, making them ideal candidates for this task. We exploit a range of predictors, chiefly Internet search volume data, and document the relevance of these data in improving out-of-sample predictions. Furthermore, we leverage the high frequency of our data to derive monthly estimates of R&D expenditures, which are currently unobserved. We compare our results with those obtained from the classical regression-based and the sparse temporal disaggregation methods. Finally, we validate our results by reporting a strong correlation with monthly R&D employment data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/755d4e35574496bb86e54a9124bcf609f8663ce7" target='_blank'>
              Nowcasting R&D Expenditures: A Machine Learning Approach
              </a>
            </td>
          <td>
            Atin Aboutorabi, Gaétan de Rassenfosse
          </td>
          <td>2024-07-16</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="In order to capture and integrate structural features and temporal features contained in social graph and diffusion cascade more effectively, an information diffusion prediction model based on Transformer and Relational Graph Convolutional Network (TRGCN) is proposed. Firstly, a dynamic heterogeneous graph composed of the social network graph and the diffusion cascade graph was constructed, and it was input into the Relational Graph Convolutional Network (RGCN) to extract the structural features of each node. Secondly, the time embedding of each node was re-encoded using Bi-directional Long Short-Term Memory (Bi-LSTM). The time decay function was introduced to give different weights to nodes at different time positions, so as to obtain the temporal features of nodes. Finally, structural features and temporal features were input into Transformer and then merged. The spatial-temporal features are obtained for information diffusion prediction. The experimental results on three real data sets of Twitter, Douban and Memetracker show that compared with the optimal model in the comparison experiment, the TRGCN model has an average increase of 4.16% in Hits@100 metric and 13.26% in map@100 metric. The validity and rationality of the model are proved.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e30c4768b013727c2b2172051248f77f7b36e273" target='_blank'>
              TRGCN: A Prediction Model for Information Diffusion Based on Transformer and Relational Graph Convolutional Network
              </a>
            </td>
          <td>
            Jinghua Zhao, Xiting Lyu, Haiying Rong, Jiale Zhao
          </td>
          <td>2024-07-26</td>
          <td>ACM Transactions on Asian and Low-Resource Language Information Processing</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We present TinySNN, a novel unsupervised spiking neural network hardware designed for real-time anomaly detection. TinySNN provides an energy-efficient edge computing solution for detecting anomalies in industrial settings. TinySNN can model and extract spatio-temporal features in sensory data, thereby enabling it to identify anomalous inputs with high accuracy. During inference, the spike rate of the TinySNN model acts as an indicator of anomalous patterns within these features. We demonstrate TinySNN’s potential using publicly available vibration datasets, achieving impressive anomaly detection results. TinySNN demonstrates exceptional sensitivity to subtle deviations from normal operation, and can dynamically adapt during online, unsupervised training. We provide a digital implementation of TinySNN on an FPGA for hardware efficiency. The TinySNN hardware can be trained online on real industrial data without requiring power hungry computing architectures like GPUs. This approach presents a promising and practical solution that allows for the dynamic learning of normal industrial operations and assists in mitigating risks through continuous monitoring. TinySNN can help ensure the safe and reliable operation of critical industrial systems through neuromorphic processing.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0429efce75ae73f49c4af9c8a6aa7dea32802b29" target='_blank'>
              Real-Time Anomaly Detection Using Hardware-based Unsupervised Spiking Neural Network (TinySNN)
              </a>
            </td>
          <td>
            Ali Mehrabi, Nik Dennler, Y. Bethi, A. Schaik, Saeed Afshar
          </td>
          <td>2024-06-18</td>
          <td>2024 IEEE 33rd International Symposium on Industrial Electronics (ISIE)</td>
          <td>0</td>
          <td>31</td>
        </tr>

        <tr id="In the rapidly evolving domain of finance, quantitative stock selection strategies have gained prominence, driven by the pursuit of maximizing returns while mitigating risks through sophisticated data analysis and algorithmic models. Yet, prevailing models frequently neglect the fluid dynamics of asset relationships and market shifts, a gap that undermines their predictive and risk management efficacy. This oversight renders them vulnerable to market volatility, adversely affecting investment decision quality and return consistency. Addressing this critical gap, our study proposes the Graph Learning Spatial–Temporal Encoder Network (GL-STN), a pioneering model that seamlessly integrates graph theory and spatial–temporal encoding to navigate the intricacies and variabilities of financial markets. By harnessing the inherent structural knowledge of stock markets, the GL-STN model adeptly captures the nonlinear interactions and temporal shifts among assets. Our innovative approach amalgamates graph convolutional layers, attention mechanisms, and long short-term memory (LSTM) networks, offering a comprehensive analysis of spatial–temporal data features. This integration not only deciphers complex stock market interdependencies but also accentuates crucial market insights, enabling the model to forecast market trends with heightened precision. Rigorous evaluations across diverse market boards—Main Board, SME Board, STAR Market, and ChiNext—underscore the GL-STN model’s exceptional ability to withstand market turbulence and enhance profitability, affirming its substantial utility in quantitative stock selection.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3a699ab84f65e499eee3551ba3251ab0abb0f4ce" target='_blank'>
              Quantitative Stock Selection Model Using Graph Learning and a Spatial–Temporal Encoder
              </a>
            </td>
          <td>
            Tianyi Cao, Xinrui Wan, Huanhuan Wang, Xin Yu, Libo Xu
          </td>
          <td>2024-07-15</td>
          <td>Journal of Theoretical and Applied Electronic Commerce Research</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="The rapid expansion of large urban areas underscores the critical importance of road infrastructure. An accurate understanding of traffic flow on road networks is essential for enhancing civil services and reducing fuel consumption. However, traffic flow is influenced by a complex array of factors and perpetually changing conditions, making comprehensive prediction of road network behavior challenging. Recent research has leveraged deep learning techniques to identify and forecast traffic flow and road network conditions, enhancing prediction accuracy by extracting key features from diverse factors. In this study, we performed short-term traffic speed predictions for road networks using data from Mobileye sensors mounted on taxis in Daegu City, Republic of Korea. These sensors capture the road network flow environment and the driver’s intentions. Utilizing these data, we integrated convolutional neural networks (CNNs) with spatio-temporal graph convolutional networks (STGCNs). Our experimental results demonstrated that the combined STGCN and CNN model outperformed the standalone STGCN and CNN models. The findings of this study contribute to the advancement of short-term traffic speed prediction models, thereby improving road network flow management.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/283f298a2f818d14f7e8f226f0c6cc82f33a66c7" target='_blank'>
              Integrating Spatio-Temporal Graph Convolutional Networks with Convolutional Neural Networks for Predicting Short-Term Traffic Speed in Urban Road Networks
              </a>
            </td>
          <td>
            S. Jeon, Myeong-Hun Jeong
          </td>
          <td>2024-07-12</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1e7d935a1fd72ec0c78f76e9366ac22a96bd7a77" target='_blank'>
              A Meta-learner approach to multistep-ahead time series prediction
              </a>
            </td>
          <td>
            Fouad Bahrpeyma, V. M. Ngo, M. Roantree, A. Mccarren
          </td>
          <td>2024-07-09</td>
          <td>International Journal of Data Science and Analytics</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0fcb3cc6350fd633d21c02eb2218f3bf4d8d98aa" target='_blank'>
              Application of Deep Learning Techniques for the Optimization of Industrial Processes Through the Fusion of Sensory Data
              </a>
            </td>
          <td>
            W. Villegas-Ch., Walter Gaibor-Naranjo, Santiago Sánchez-Viteri
          </td>
          <td>2024-07-18</td>
          <td>Int. J. Comput. Intell. Syst.</td>
          <td>0</td>
          <td>12</td>
        </tr>

        <tr id="Image anomaly detection plays a pivotal role in industrial inspection. Traditional approaches often demand distinct models for specific categories, resulting in substantial deployment costs. This raises concerns about multi-class anomaly detection, where a unified model is developed for multiple classes. However, applying conventional methods, particularly reconstruction-based models, directly to multi-class scenarios encounters challenges such as identical shortcut learning, hindering effective discrimination between normal and abnormal instances. To tackle this issue, our study introduces the Prior Normality Prompt Transformer (PNPT) method for multi-class image anomaly detection. PNPT strategically incorporates normal semantics prompting to mitigate the"identical mapping"problem. This entails integrating a prior normality prompt into the reconstruction process, yielding a dual-stream model. This innovative architecture combines normal prior semantics with abnormal samples, enabling dual-stream reconstruction grounded in both prior knowledge and intrinsic sample characteristics. PNPT comprises four essential modules: Class-Specific Normality Prompting Pool (CS-NPP), Hierarchical Patch Embedding (HPE), Semantic Alignment Coupling Encoding (SACE), and Contextual Semantic Conditional Decoding (CSCD). Experimental validation on diverse benchmark datasets and real-world industrial applications highlights PNPT's superior performance in multi-class industrial anomaly detection.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/09794d56d295554d66571a061a756057e6ba6611" target='_blank'>
              Prior Normality Prompt Transformer for Multi-class Industrial Image Anomaly Detection
              </a>
            </td>
          <td>
            Haiming Yao, Yunkang Cao, Wei Luo, Weihang Zhang, Wenyong Yu, Weiming Shen
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6aee1f022642bde0a16f9beb44cbc7c9b67c2d34" target='_blank'>
              Stacking for Probabilistic Short-Term Load Forecasting
              </a>
            </td>
          <td>
            Grzegorz Dudek
          </td>
          <td>2024-06-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="AI systems are notorious for their fragility; minor input changes can potentially cause major output swings. When such systems are deployed in critical areas like finance, the consequences of their uncertain behavior could be severe. In this paper, we focus on multi-modal time-series forecasting, where imprecision due to noisy or incorrect data can lead to erroneous predictions, impacting stakeholders such as analysts, investors, and traders. Recently, it has been shown that beyond numeric data, graphical transformations can be used with advanced visual models to achieve better performance. In this context, we introduce a rating methodology to assess the robustness of Multi-Modal Time-Series Forecasting Models (MM-TSFM) through causal analysis, which helps us understand and quantify the isolated impact of various attributes on the forecasting accuracy of MM-TSFM. We apply our novel rating method on a variety of numeric and multi-modal forecasting models in a large experimental setup (six input settings of control and perturbations, ten data distributions, time series from six leading stocks in three industries over a year of data, and five time-series forecasters) to draw insights on robust forecasting models and the context of their strengths. Within the scope of our study, our main result is that multi-modal (numeric + visual) forecasting, which was found to be more accurate than numeric forecasting in previous studies, can also be more robust in diverse settings. Our work will help different stakeholders of time-series forecasting understand the models` behaviors along trust (robustness) and accuracy dimensions to select an appropriate model for forecasting using our rating method, leading to improved decision-making.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1d6d01816a36ea78b2c97fe4fbd6ed4dddec8d71" target='_blank'>
              Rating Multi-Modal Time-Series Forecasting Models (MM-TSFM) for Robustness Through a Causal Lens
              </a>
            </td>
          <td>
            Kausik Lakkaraju, Rachneet Kaur, Zhen Zeng, Parisa Zehtabi, Sunandita Patra, Biplav Srivastava, Marco Valtorta
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/debe0ee50d9dc6388d2e21ad87cbf672e2865ff3" target='_blank'>
              Randomnet: clustering time series using untrained deep neural networks
              </a>
            </td>
          <td>
            Xiaosheng Li, Wenjie Xi, Jessica Lin
          </td>
          <td>2024-06-22</td>
          <td>Data Mining and Knowledge Discovery</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="We present a knowledge-guided machine learning (KGML) framework for modeling multi-scale processes, and study its performance in the context of streamflow forecasting in hydrology. Specifically, we propose a novel hierarchical recurrent neural architecture that factorizes the system dynamics at multiple temporal scales and captures their interactions. This framework consists of an inverse and a forward model. The inverse model is used to empirically resolve the system's temporal modes from data (physical model simulations, observed data, or a combination of them from the past), and these states are then used in the forward model to predict streamflow. In a hydrological system, these modes can represent different processes, evolving at different temporal scales (e.g., slow: groundwater recharge and baseflow vs. fast: surface runoff due to extreme rainfall). A key advantage of our framework is that once trained, it can incorporate new observations into the model's context (internal state) without expensive optimization approaches (e.g., EnKF) that are traditionally used in physical sciences for data assimilation. Experiments with several river catchments from the NWS NCRFC region show the efficacy of this ML-based data assimilation framework compared to standard baselines, especially for basins that have a long history of observations. Even for basins that have a shorter observation history, we present two orthogonal strategies of training our FHNN framework: (a) using simulation data from imperfect simulations and (b) using observation data from multiple basins to build a global model. We show that both of these strategies (that can be used individually or together) are highly effective in mitigating the lack of training data. The improvement in forecast accuracy is particularly noteworthy for basins where local models perform poorly because of data sparsity.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/520332f72d1a6deaabdd7dd965c0cf76e89053b5" target='_blank'>
              Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems
              </a>
            </td>
          <td>
            Rahul Ghosh, Zac McEachran, Arvind Renganathan, Kelly Lindsay, Somya Sharma, M. Steinbach, John L. Nieber, Christopher J. Duffy, Vipin Kumar
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>44</td>
        </tr>

        <tr id="Graph neural networks (GNNs) perform well in text analysis tasks. Their unique structure allows them to capture complex patterns and dependencies in text, making them ideal for processing natural language tasks. At the same time, XGBoost (version 1.6.2.) outperforms other machine learning methods on heterogeneous tabular data. However, traditional graph neural networks mainly study isomorphic and sparse data features. Therefore, when dealing with tabular data, traditional graph neural networks encounter challenges such as data structure mismatch, feature selection, and processing difficulties. To solve these problems, we propose a novel architecture, XGNN, which combines the advantages of XGBoost and GNNs to deal with heterogeneous features and graph structures. In this paper, we use GAT for our graph neural network model. We can train XGBoost and GNN end-to-end to fit and adjust the new tree in XGBoost based on the gradient information from the GNN. Extensive experiments on node prediction and node classification tasks demonstrate that the performance of our proposed new model is significantly improved for both prediction and classification tasks and performs particularly well on heterogeneous tabular data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/21dce0407d0ee3bec185b0361593d73bb26a532e" target='_blank'>
              XGBoost-Enhanced Graph Neural Networks: A New Architecture for Heterogeneous Tabular Data
              </a>
            </td>
          <td>
            Liuxi Yan, Yaoqun Xu
          </td>
          <td>2024-07-03</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="In this paper, we apply a supervised machine-learning approach to solve a fundamental problem in queueing theory: estimating the transient distribution of the number in the system for a G(t)/GI/1. We develop a neural network mechanism that provides a fast and accurate predictor of these distributions for moderate horizon lengths and practical settings. It is based on using a Recurrent Neural Network (RNN) architecture based on the first several moments of the time-dependant inter-arrival and the stationary service time distributions; we call it the Moment-Based Recurrent Neural Network (RNN) method (MBRNN ). Our empirical study suggests MBRNN requires only the first four inter-arrival and service time moments. We use simulation to generate a substantial training dataset and present a thorough performance evaluation to examine the accuracy of our method using two different test sets. We show that even under the configuration with the worst performance errors, the mean number of customers over the entire timeline has an error of less than 3%. While simulation modeling can achieve high accuracy, the advantage of the MBRNN over simulation is runtime, while the MBRNN analyzes hundreds of systems within a fraction of a second. This paper focuses on a G(t)/GI/1; however, the MBRNN approach demonstrated here can be extended to other queueing systems, as the training data labeling is based on simulations (which can be applied to more complex systems) and the training is based on deep learning, which can capture very complex time sequence tasks. In summary, the MBRNN can potentially revolutionize our ability to perform transient analyses of queueing systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f691dfd4dff3b3fb9b91bf5e89d6572368165f58" target='_blank'>
              Approximating G(t)/GI/1 queues with deep learning
              </a>
            </td>
          <td>
            Eliran Sherzer, Opher Baron, Dmitry Krass, Yehezkel S. Resheff
          </td>
          <td>2024-07-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="In the realm of transportation science, the advent of deep learning has propelled
 advancements in predicting longitudinal driving behavior. This study explores
 the application of deep neural network architectures, specifically
 long–short-term memory (LSTM) and convolutional neural networks (CNNs),
 recognized for their effectiveness in handling sequential data. Using a 3-s
 temporal window that includes past vehicle progress, speed, and acceleration,
 the proposed model, a hybrid LSTM–CNN architecture, predicts the vehicle’s speed
 and progress for the next 6 s. The approach achieves state-of-the-art
 performance, particularly within a 4 s horizon, but remains competitive even for
 longer-term predictions. This is achieved despite the simplicity of its input
 space, which does not include information about vehicles other than the target
 vehicle. As a result, while its performance may decrease slightly for
 longer-term predictions due to the lack of environmental information, it still
 offers reliable predictions and can be applied effectively in scenarios with
 partial observability. The comparative analysis of multilayer perceptron (MLP),
 LSTM, and one-dimensional CNN architectures highlights the challenges faced by
 MLP in capturing the complex nonlinearity of driving behavior. LSTM and CNN
 demonstrate superior performance, with model complexity influencing outcomes. No
 statistically significant difference is observed in the performance between LSTM
 and CNN models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b89149034ce871272b45a99ef39fd0aeb3d0fa0e" target='_blank'>
              Deep Learning Algorithms for Longitudinal Driving Behavior
 Prediction: A Comparative Analysis of Convolutional Neural Network and
 Long–Short-Term Memory Models
              </a>
            </td>
          <td>
            Giovanni Lucente, Mikkel Skov Maarssoe, Iris Kahl, Julian Schindler
          </td>
          <td>2024-06-13</td>
          <td>SAE International Journal of Connected and Automated Vehicles</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Container marking anomaly detection is a pivotal component within container production processes. Due to the diversity and variability of container markings, manual detection is inefficient and prone to errors. While mainstream deep learning-based anomaly detection methods typically focus on products with consistent appearances, container markings from different production batches often exhibit variations. To address this, we propose a Cellular Data Generator Based Siamese Container Marking Anomaly Detection Network (GS-CMAD). GS-CMAD is trained on data generated using our method during the training phase and employs a siamese network for anomaly detection, addressing issues of scarce anomaly data and detection of unknown class markings. Additionally, we propose a novel Cellular Data Generator (CDG), utilizing Cellular noise to generate data that closely resemble the distribution of real container marking data compared to methods using Perlin noise. To enable the generated data for training siamese networks, we propose a novel method, Data Generation for Siamese Network (DGSN). DGSN consists of two CDGs with identical parameters, capable of simultaneously generating normal samples and anomaly samples, which can be directly used for siamese network training. Subsequently, we propose the Siamese Container Marking Anomaly Detection Network (SADN), utilizing siamese networks to extract difference features between template and target images for anomaly detection. Additionally, due to the absence of corresponding datasets, we proposed the Container Marking Anomaly Detection and Localization Dataset (CM-ADL). Finally, through extensive experimentation, we have validated the effectiveness of SADN and CDG, thus demonstrating the advancement of GS-CMAD.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/344d2b7c96f83ebaa1984ab9ecb8ea9c4fb006cd" target='_blank'>
              Cellular Data Generator Based Siamese Container Marking Anomaly Detection Network
              </a>
            </td>
          <td>
            Wenfeng Pan, Zhihao Long, Xinru Li, Gaoyang Li, Tangrong Huang, Yanyang Liang, Yikui Zhai
          </td>
          <td>2024-06-14</td>
          <td>2024 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA)</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Anomaly detection involves identifying instances within a dataset that deviate from the norm and occur infrequently. Current benchmarks tend to favor methods biased towards low diversity in normal data, which does not align with real-world scenarios. Despite advancements in these benchmarks, contemporary anomaly detection methods often struggle with out-of-distribution generalization, particularly in classifying samples with subtle transformations during testing. These methods typically assume that normal samples during test time have distributions very similar to those in the training set, while anomalies are distributed much further away. However, real-world test samples often exhibit various levels of distribution shift while maintaining semantic consistency. Therefore, effectively generalizing to samples that have undergone semantic-preserving transformations, while accurately detecting normal samples whose semantic meaning has changed after transformation as anomalies, is crucial for the trustworthiness and reliability of a model. For example, although it is clear that rotation shifts the meaning for a car in the context of anomaly detection but preserves the meaning for a bird, current methods are likely to detect both as abnormal. This complexity underscores the necessity for dynamic learning procedures rooted in the intrinsic concept of outliers. To address this issue, we propose new testing protocols and a novel method called Knowledge Exposure (KE), which integrates external knowledge to comprehend concept dynamics and differentiate transformations that induce semantic shifts. This approach enhances generalization by utilizing insights from a pre-trained CLIP model to evaluate the significance of anomalies for each concept. Evaluation on CIFAR-10, CIFAR-100, and SVHN with the new protocols demonstrates superior performance compared to previous methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e9b09fc9f3189d6d87ea55f2a53f01d08076d82c" target='_blank'>
              Enhancing Anomaly Detection Generalization through Knowledge Exposure: The Dual Effects of Augmentation
              </a>
            </td>
          <td>
            Mohammad Akhavan Anvari, Rojina Kashefi, Vahid Reza Khazaie, Mohammad Khalooei, Mohammad Sabokrou
          </td>
          <td>2024-06-15</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Our work focuses on the exploration of the internal relationships of signals in an individual sensor. In particular, we address the problem of not being able to evaluate such intrasensor relationships due to missing rich and explicit feature representation. To solve this problem, we propose GraphSensor, a graph attention network, with a shared-weight convolution feature encoder to generate the signal segments and learn the internal relationships between them. Furthermore, we enrich the representation of the features by utilizing a multi-head approach when creating the internal relationship graph. Compared with traditional multi-head approaches, we propose a more efficient convolution-based multi-head mechanism, which only requires 56% of model parameters compared with the best multi-head baseline as demonstrated in the experiments. Moreover, GraphSensor is capable of achieving state-of-the-art performance in the electroencephalography dataset and improving the accuracy by 13.8% compared to the best baseline in an inertial measurement unit (IMU) dataset.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e918df8828422967298b9aec4ac1ee02d2ddb68c" target='_blank'>
              GraphSensor: A Graph Attention Network for Time-Series Sensor
              </a>
            </td>
          <td>
            Jiaqi Ge, Gaochao Xu, Jianchao Lu, Xu Xu, Xiangyu Meng
          </td>
          <td>2024-06-11</td>
          <td>Electronics</td>
          <td>1</td>
          <td>9</td>
        </tr>

        <tr id="Time series data are ubiquitous across a wide range of real-world domains. While real-world time series analysis (TSA) requires human experts to integrate numerical series data with multimodal domain-specific knowledge, most existing TSA models rely solely on numerical data, overlooking the significance of information beyond numerical series. This oversight is due to the untapped potential of textual series data and the absence of a comprehensive, high-quality multimodal dataset. To overcome this obstacle, we introduce Time-MMD, the first multi-domain, multimodal time series dataset covering 9 primary data domains. Time-MMD ensures fine-grained modality alignment, eliminates data contamination, and provides high usability. Additionally, we develop MM-TSFlib, the first multimodal time-series forecasting (TSF) library, seamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth analyses. Extensive experiments conducted on Time-MMD through MM-TSFlib demonstrate significant performance enhancements by extending unimodal TSF to multimodality, evidenced by over 15% mean squared error reduction in general, and up to 40% in domains with rich textual data. More importantly, our datasets and library revolutionize broader applications, impacts, research topics to advance TSA. The dataset and library are available at https://github.com/AdityaLab/Time-MMD and https://github.com/AdityaLab/MM-TSFlib.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ef5ed58b37b4760ed1f5f89a06442f47d376aa90" target='_blank'>
              Time-MMD: A New Multi-Domain Multimodal Dataset for Time Series Analysis
              </a>
            </td>
          <td>
            Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Kamarthi, Aditya B. Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao Zhang, B. A. Prakash
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="The world surrounding us is subject to constant change. These changes, frequently described as concept drift, influence many industrial and technical processes. As they can lead to malfunctions and other anomalous behavior, which may be safety-critical in many scenarios, detecting and analyzing concept drift is crucial. In this study, we provide a literature review focusing on concept drift in unsupervised data streams. While many surveys focus on supervised data streams, so far, there is no work reviewing the unsupervised setting. However, this setting is of particular relevance for monitoring and anomaly detection which are directly applicable to many tasks and challenges in engineering. This survey provides a taxonomy of existing work on unsupervised drift detection. In addition to providing a comprehensive literature review, it offers precise mathematical definitions of the considered problems and contains standardized experiments on parametric artificial datasets allowing for a direct comparison of different detection strategies. Thus, the suitability of different schemes can be analyzed systematically, and guidelines for their usage in real-world scenarios can be provided.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a608f01f407ffdd4bd52e99b487aacdea23e3295" target='_blank'>
              One or two things we know about concept drift—a survey on monitoring in evolving environments. Part A: detecting concept drift
              </a>
            </td>
          <td>
            Fabian Hinder, Valerie Vaquet, Barbara Hammer
          </td>
          <td>2024-06-19</td>
          <td>Frontiers in Artificial Intelligence</td>
          <td>1</td>
          <td>7</td>
        </tr>

        <tr id="This paper proposes a deep recurrent neural network (DRNN) approach to model the one-hour-ahead wind speed forecasting by using various meteorological sensory data from the North Wyke farm platform (NWFP). To refine model input, mutual information analysis is applied to eliminate irrelevant sensory data. The DRNN architecture employs three recurrent layers Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU), and simple Recurrent Neural Network (RNN) to capture temporal relationships. The proposed networks are tested using real-life, one-year data from the NWFP. The results showed a strong correlation between the actual and predicted wind speed for LSTM, GRU, and RNN layers-based DRNN, however, simple RNN slightly outperformed the other two recurrent layers. The distribution of the network errors over the year is also analyzed. Although the observed meteorological data between the years was from different distributions, the proposed network generalized well even though these data were altered due to global warming.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85208f740e1c9925156ec820fb1175b1cb13df77" target='_blank'>
              Wind Speed Prediction Using Deep Recurrent Neural Networks and Farm Platform Features for One-Hour-Ahead Forecast
              </a>
            </td>
          <td>
            Emre Özbilge, Yonal Kırsal
          </td>
          <td>2024-06-27</td>
          <td>Çukurova Üniversitesi Mühendislik Fakültesi Dergisi</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Concept Drift is a phenomenon in which the underlying data distribution and statistical properties of a target domain change over time, leading to a degradation of the model's performance. Consequently, models deployed in production require continuous monitoring through drift detection techniques. Most drift detection methods to date are supervised, i.e., based on ground-truth labels. However, true labels are usually not available in many real-world scenarios. Although recent efforts have been made to develop unsupervised methods, they often lack the required accuracy, have a complexity that makes real-time implementation in production environments difficult, or are unable to effectively characterize drift. To address these challenges, we propose DriftLens, an unsupervised real-time concept drift detection framework. It works on unstructured data by exploiting the distribution distances of deep learning representations. DriftLens can also provide drift characterization by analyzing each label separately. A comprehensive experimental evaluation is presented with multiple deep learning classifiers for text, image, and speech. Results show that (i) DriftLens performs better than previous methods in detecting drift in $11/13$ use cases; (ii) it runs at least 5 times faster; (iii) its detected drift value is very coherent with the amount of drift (correlation $\geq 0.85$); (iv) it is robust to parameter changes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2d495b1929a236708b82988d1b44ca17d9a93e95" target='_blank'>
              Unsupervised Concept Drift Detection from Deep Learning Representations in Real-time
              </a>
            </td>
          <td>
            Salvatore Greco, Bartolomeo Vacchetti, D. Apiletti, Tania Cerquitelli
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="In the era of rapid globalization and digitalization, accurate identification of similar stocks has become increasingly challenging due to the non-stationary nature of financial markets and the ambiguity in conventional regional and sector classifications. To address these challenges, we examine SimStock, a novel temporal self-supervised learning framework that combines techniques from self-supervised learning (SSL) and temporal domain generalization to learn robust and informative representations of financial time series data. The primary focus of our study is to understand the similarities between stocks from a broader perspective, considering the complex dynamics of the global financial landscape. We conduct extensive experiments on four real-world datasets with thousands of stocks and demonstrate the effectiveness of SimStock in finding similar stocks, outperforming existing methods. The practical utility of SimStock is showcased through its application to various investment strategies, such as pairs trading, index tracking, and portfolio optimization, where it leads to superior performance compared to conventional methods. Our findings empirically examine the potential of data-driven approach to enhance investment decision-making and risk management practices by leveraging the power of temporal self-supervised learning in the face of the ever-changing global financial landscape.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c259bc9adf3019870613cbb7888ea4fac4e1929f" target='_blank'>
              Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management
              </a>
            </td>
          <td>
            Yoon-Jeong Hwang, Stefan Zohren, Yongjae Lee
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Generating synthetic financial time series data that accurately reflects real-world market dynamics holds tremendous potential for various applications, including portfolio optimization, risk management, and large scale machine learning. We present an approach for training generative models for financial time series using the maximum mean discrepancy (MMD) with a signature kernel. Our method leverages the expressive power of the signature transform to capture the complex dependencies and temporal structures inherent in financial data. We employ a moving average model to model the variance of the noise input, enhancing the model's ability to reproduce stylized facts such as volatility clustering. Through empirical experiments on S&P 500 index data, we demonstrate that our model effectively captures key characteristics of financial time series and outperforms a comparable GAN-based approach. In addition, we explore the application of the synthetic data generated to train a reinforcement learning agent for portfolio management, achieving promising results. Finally, we propose a method to add robustness to the generative model by tweaking the noise input so that the generated sequences can be adjusted to different market environments with minimal data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/492d244da8647554a7439883786cfe931ec4c342" target='_blank'>
              Generative model for financial time series trained with MMD using a signature kernel
              </a>
            </td>
          <td>
            Chung I Lu, Julian Sester
          </td>
          <td>2024-07-29</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Time series analysis is an important research area and has wide real applications. Time series modeling has been identified and well known as the key fundamental problem in time series analysis, and it has received lots of research attentions. The hardness of achieving sophisticated and efficient time series models depends on the characteristic of the data met. In practical, time series data are usually generated by the physical process with several working states, which are determined by the system design. In such cases, the data characteristics of each state is quite different from the other states, where the modeling problem becomes much harder and it is more challenging to design modeling methods for stateful time series data. Therefore, this paper focuses on the stateful time series data, and studies the state change detection problem, which is identified as a key fundamental problem for modeling stateful data. To achieve effective and efficient state change detection, the proposed method utilizes two essential ideas. Since the discrepancy (a.k.a. histogram) based method has been proposed and verified to be more effective by previous works, the first idea is to improve its performance by involving more temporal information and utilizing the encoder–decoder techniques to reform the data. In view of the efficiency issue of the discrepancy‐based method, the second idea is to integrate both the Bayesian and discrepancy‐based detection methods, such that a perfect tradeoff between time efficiency and detection performance can be achieved. Experiments on both real and synthetic datasets show that the proposed methods are both effective and efficient, and the state change detection problem can be solved well by them on stateful time series data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/44d60851e92e6ebe2118be825de319b722ed89f3" target='_blank'>
              Efficient state change detection method for modeling stateful time series data
              </a>
            </td>
          <td>
            Xianmin Liu, Ruixin Luo, Wenbo Li, Qiang Bi
          </td>
          <td>2024-06-24</td>
          <td>International Journal of Robust and Nonlinear Control</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Log-based anomaly detection has been widely studied in the literature as a way to increase the dependability of software-intensive systems. In reality, logs can be unstable due to changes made to the software during its evolution. This, in turn, degrades the performance of downstream log analysis activities, such as anomaly detection. The critical challenge in detecting anomalies on these unstable logs is the lack of information about the new logs, due to insufficient log data from new software versions. The application of Large Language Models (LLMs) to many software engineering tasks has revolutionized various domains. In this paper, we report on an experimental comparison of a fine-tuned LLM and alternative models for anomaly detection on unstable logs. The main motivation is that the pre-training of LLMs on vast datasets may enable a robust understanding of diverse patterns and contextual information, which can be leveraged to mitigate the data insufficiency issue in the context of software evolution. Our experimental results on the two-version dataset of LOGEVOL-Hadoop show that the fine-tuned LLM (GPT-3) fares slightly better than supervised baselines when evaluated on unstable logs. The difference between GPT-3 and other supervised approaches tends to become more significant as the degree of changes in log sequences increases. However, it is unclear whether the difference is practically significant in all cases. Lastly, our comparison of prompt engineering (with GPT-4) and fine-tuning reveals that the latter provides significantly superior performance on both stable and unstable logs, offering valuable insights into the effective utilization of LLMs in this domain.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5b4685a3efaeee062b898f052a3870c9aa19cab7" target='_blank'>
              Anomaly Detection on Unstable Logs with GPT Models
              </a>
            </td>
          <td>
            Fateme Hadadi, Qinghua Xu, D. Bianculli, Lionel C. Briand
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>21</td>
        </tr>

        <tr id="Identification of critical nodes is a prominent topic in the study of complex networks. Numerous methods have been proposed, yet most exhibit inherent limitations. Traditional approaches primarily analyze specific structural features of the network; however, node influence is typically the result of a combination of multiple factors. Machine learning-based methods struggle to effectively represent the complex characteristics of network structures through suitable embedding techniques and require substantial data for training, rendering them prohibitively costly for large-scale networks. To address these challenges, this paper presents an active learning model based on GraphSAGE and Transformer, named GNNTAL. This model is initially pre-trained on random or synthetic networks and subsequently fine-tuned on real-world networks by selecting a few representative nodes using K-Means clustering and uncertainty sampling. This approach offers two main advantages: (1) it significantly reduces training costs; (2) it simultaneously incorporates both local and global features. A series of comparative experiments conducted on twelve real-world networks demonstrate that GNNTAL achieves superior performance. Additionally, this paper proposes an influence maximization method based on the predictions of the GNNTAL model, which achieves optimal performance without the need for complex computations. Finally, the paper analyses certain limitations of the GNNTAL model and suggests potential solutions.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f2c2569fd1bd2320230e824c982a501344641e9c" target='_blank'>
              GNNTAL:A Novel Model for Identifying Critical Nodes in Complex Networks
              </a>
            </td>
          <td>
            Hao Wang, Ting Luo, Shuang-ping Yang, Ming Jing, Jian Wang, Na Zhao
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Stock price forecasting has been an important area of interest for economists and computer scientists. In addition to traditional statistical methods, advanced artificial intelligence techniques such as machine learning can stand out with their ability to process complex data sets and adapt to historical data. In recent years, hybrid models combining deep learning and time series methods have demonstrated superior performance in stock selection and portfolio optimisation. This study comparatively analyses the performance of LSTM and ARIMA models in time series forecasting. In the study, the stock prices of Oracle company are predicted using two different models, LSTM and ARIMA. Model performance is evaluated using metrics like MSE, MAE, RMSE, and MAPE. Both models have been found to be successful in different metrics. The LSTM model has lower error values; meanwhile, the ARIMA model produced proportionally more accurate forecasts. The study concludes that given the potential offered by deep learning, models such as LSTM are essential for time series forecasting. The flexibility of deep learning allows the development of customized models for different data types and time series problems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/7fa13189dabbbc094e6e28d8bc9e13d6f6bccac1" target='_blank'>
              Comparative Analysis of LSTM and ARIMA Models in Stock Price Prediction: A Technology Company Example
              </a>
            </td>
          <td>
            Y. Kırelli
          </td>
          <td>2024-07-30</td>
          <td>Black Sea Journal of Engineering and Science</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Long-term time series forecasting is a long-standing challenge in various applications. A central issue in time series forecasting is that methods should expressively capture long-term dependency. Furthermore, time series forecasting methods should be flexible when applied to different scenarios. Although Fourier analysis offers an alternative to effectively capture reusable and periodic patterns to achieve long-term forecasting in different scenarios, existing methods often assume high-frequency components represent noise and should be discarded in time series forecasting. However, we conduct a series of motivation experiments and discover that the role of certain frequencies varies depending on the scenarios. In some scenarios, removing high-frequency components from the original time series can improve the forecasting performance, while in others scenarios, removing them is harmful to forecasting performance. Therefore, it is necessary to treat the frequencies differently according to specific scenarios. To achieve this, we first reformulate the time series forecasting problem as learning a transfer function of each frequency in the Fourier domain. Further, we design Frequency Dynamic Fusion (FreDF), which individually predicts each Fourier component, and dynamically fuses the output of different frequencies. Moreover, we provide a novel insight into the generalization ability of time series forecasting and propose the generalization bound of time series forecasting. Then we prove FreDF has a lower bound, indicating that FreDF has better generalization ability. Extensive experiments conducted on multiple benchmark datasets and ablation studies demonstrate the effectiveness of FreDF.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/180860f60e6655bebc9757b695b4fbc18a0c39ee" target='_blank'>
              Not All Frequencies Are Created Equal:Towards a Dynamic Fusion of Frequencies in Time-Series Forecasting
              </a>
            </td>
          <td>
            Xingyu Zhang, Siyu Zhao, Zeen Song, Huijie Guo, Jianqi Zhang, Changwen Zheng, Wenwen Qiang
          </td>
          <td>2024-07-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="In this paper, we present a novel method to significantly enhance the computational efficiency of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived from the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star topology as a GWT prior to training, we balance edge reduction with efficient information propagation, reducing computational demands while maintaining high model performance. Both the time and memory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from $\mathcal{O}(N^2)$ to $\mathcal{O}(N)$. Our approach streamlines the ASTGNN deployment by eliminating the need for exhaustive training, pruning, and retraining cycles, and demonstrates empirically across various datasets that it is possible to achieve comparable performance to full models with substantially lower computational costs. Specifically, our approach enables training ASTGNNs on the largest scale spatial-temporal dataset using a single A6000 equipped with 48 GB of memory, overcoming the out-of-memory issue encountered during original training and even achieving state-of-the-art performance. Furthermore, we delve into the effectiveness of the GWT from the perspective of spectral graph theory, providing substantial theoretical support. This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability of the LTH in resource-constrained settings, marking a significant step forward in the field of graph neural networks. Code is available at https://anonymous.4open.science/r/paper-1430.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/581ca5b19238ebb117930d96e6c5799d0e0484cf" target='_blank'>
              Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks
              </a>
            </td>
          <td>
            Wenying Duan, Tianxiang Fang, Hong Rao, Xiaoxi He
          </td>
          <td>2024-06-12</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="This study investigates the efficacy of machine learning models, specifically Random Forest, in anomaly detection systems when trained on complete flow records and tested on partial flow data. We explore the performance disparity that arises when models are applied to incomplete data typical in real-world, real-time network environments. Our findings demonstrate a significant decline in model performance, with precision and recall dropping by up to 30\% under certain conditions when models trained on complete flows are tested against partial flows. Conversely, models trained and tested on consistently complete or partial datasets maintain robustness, highlighting the importance of dataset consistency in training. The study reveals that a minimum of 7 packets in the test set is required for maintaining reliable detection rates. These results underscore the need for tailored training strategies that can effectively adapt to the dynamics of partial data, enhancing the practical applicability of anomaly detection systems in operational settings.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c43432d254acc1f68df6f2c86411e5b76267045b" target='_blank'>
              Early-Stage Anomaly Detection: A Study of Model Performance on Complete vs. Partial Flows
              </a>
            </td>
          <td>
            Adrián Pekár, Richard Jozsa
          </td>
          <td>2024-07-03</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="We propose a novel approach for time series forecasting with many predictors, referred to as the GO-sdPCA, in this paper. The approach employs a variable selection method known as the group orthogonal greedy algorithm and the high-dimensional Akaike information criterion to mitigate the impact of irrelevant predictors. Moreover, a novel technique, called peeling, is used to boost the variable selection procedure so that many factor-relevant predictors can be included in prediction. Finally, the supervised dynamic principal component analysis (sdPCA) method is adopted to account for the dynamic information in factor recovery. In simulation studies, we found that the proposed method adapts well to unknown degrees of sparsity and factor strength, which results in good performance, even when the number of relevant predictors is large compared to the sample size. Applying to economic and environmental studies, the proposed method consistently performs well compared to some commonly used benchmarks in one-step-ahead out-sample forecasts.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f5f975a9b0c2b07495fe9526e67b916e2eccdc95" target='_blank'>
              Time Series Forecasting with Many Predictors
              </a>
            </td>
          <td>
            Shuo-chieh Huang, R. Tsay
          </td>
          <td>2024-06-13</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>50</td>
        </tr>

        <tr id="A Marked Temporal Point Process (MTPP) is a stochastic process whose realization is a set of event-time data. MTPP is often used to understand complex dynamics of asynchronous temporal events such as money transaction, social media, healthcare, etc. Recent studies have utilized deep neural networks to capture complex temporal dependencies of events and generate embedding that aptly represent the observed events. While most previous studies focus on the inter-event dependencies and their representations, how individual events influence the overall dynamics over time has been under-explored. In this regime, we propose a Decoupled MTPP framework that disentangles characterization of a stochastic process into a set of evolving influences from different events. Our approach employs Neural Ordinary Differential Equations (Neural ODEs) to learn flexible continuous dynamics of these influences while simultaneously addressing multiple inference problems, such as density estimation and survival rate computation. We emphasize the significance of disentangling the influences by comparing our framework with state-of-the-art methods on real-life datasets, and provide analysis on the model behavior for potential applications.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/01528db95c399e0822e827b108710fdf76fd5f21" target='_blank'>
              Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations
              </a>
            </td>
          <td>
            Yujee Song, Donghyun Lee, Rui Meng, Won Hwa Kim
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Literature abounds with various statistical and machine learning techniques for stock market forecasting. However, Reinforcement Learning (RL) is conspicuous by its absence in this field and is little explored despite its potential to address the dynamic and uncertain nature of the stock market. In a first-of-its-kind study, this research precisely bridges this gap, by forecasting stock prices using RL, in the static as well as streaming contexts using deep RL techniques. In the static context, we employed three deep RL algorithms for forecasting the stock prices: Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimisation (PPO) and Recurrent Deterministic Policy Gradient (RDPG) and compared their performance with Multi-Layer Perceptron (MLP), Support Vector Regression (SVR) and General Regression Neural Network (GRNN). In addition, we proposed a generic streaming analytics-based forecasting approach leveraging the real-time processing capabilities of Spark streaming for all six methods. This approach employs a sliding window technique for real-time forecasting or nowcasting using the above-mentioned algorithms. We demonstrated the effectiveness of the proposed approach on the daily closing prices of four different financial time series dataset as well as the Mackey–Glass time series, a benchmark chaotic time series dataset. We evaluated the performance of these methods using three metrics: Symmetric Mean Absolute Percentage (SMAPE), Directional Symmetry statistic (DS) and Theil’s U Coefficient. The results are promising for DDPG in the static context and GRNN turned out to be the best in streaming context. We performed the Diebold–Mariano (DM) test to assess the statistical significance of the best-performing models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/399a8108ea6cff24099de6a2388ecbb9069a00fa" target='_blank'>
              Deep Reinforcement Learning for Financial Forecasting in Static and Streaming Cases
              </a>
            </td>
          <td>
            Aravilli Atchuta Ram, Sandarbh Yadav, Yelleti Vivek, V. Ravi
          </td>
          <td>2024-06-27</td>
          <td>Journal of Information &amp; Knowledge Management</td>
          <td>0</td>
          <td>40</td>
        </tr>

        <tr id="Structural health monitoring (SHM) systems are widely used for civil infrastructure monitoring. Data acquired from the SHM systems play an important role in assessing structural integrity and determining further maintenance activities. Considering that sensors in the SHM systems are installed in a harsh environment for long-term measurements, some sensors can malfunction and produce faulty data. As a large amount of measured data is often desired to be automatically processed and can adversely affect structural assessments, identifying such abnormal data is important. This paper provides critical investigations of the automated detection of data anomalies using existing deep-learning-based classification in conjunction with a simple rule-based approach. The issues investigated in this study include (1) the presence of ambiguous data that cannot be categorized as an anomaly class in the literature, (2) information loss during the conversion of time-series data into images for the deep-learning-based approach, and (3) additional issues, such as misclassification by trained models and requirements of the threshold section in the rule-based approach. The results of these key investigations can be utilized to develop an effective anomaly detection process.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5b9c54590e700fe163d480dca6dcd6f176ac344f" target='_blank'>
              Investigation of Issues in Data Anomaly Detection Using Deep-Learning- and Rule-Based Classifications for Long-Term Vibration Measurements
              </a>
            </td>
          <td>
            Imdad Ullah Khan, Seunghoo Jeong, Sung-Han Sim
          </td>
          <td>2024-06-24</td>
          <td>Applied Sciences</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Climaate prediction plays a vital role in various sectors, including agriculture, disaster management, and urban planning. Traditional methods for climate forecasting often rely on complex physical models, which require substantial computational resources and may not accurately capture local weather patterns. This study explores the potential of Long Short-Term Memory (LSTM) networks, a type of recurrent neural network, for predicting daily climate variables such as temperature, precipitation, and humidity. Utilizing historical climate data from the city of Delhi, we developed an LSTM model to forecast short-term climate trends. The model consists of two LSTM layers followed by three Dense layers and is compiled with the Adam optimizer, mean squared error loss, and mean absolute error as a metric. Our results demonstrate the model's capability to capture temporal dependencies in climate data, achieving a satisfactory level of accuracy in temperature forecasting. This research underscores the potential of machine learning techniques, particularly LSTM networks, in enhancing climate prediction and contributing to more informed decision-making in weather-sensitive sectors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3b6d06cad165e6e886fa69a86af54e02218c3b53" target='_blank'>
              Prediction of Daily Climate Using Long Short-Term Memory (LSTM) Model
              </a>
            </td>
          <td>
            Jinxin Xu, Zhuoyue Wang, Xinjin Li, Zichao Li, Zhenglin Li
          </td>
          <td>2024-07-12</td>
          <td>International Journal of Innovative Science and Research Technology (IJISRT)</td>
          <td>2</td>
          <td>2</td>
        </tr>

        <tr id="Detecting violations within fishing activity reports is crucial for ensuring the sustainable utilization of fish resources, and employing machine learning methods holds promise for uncovering hidden patterns within this complex dataset. Given that these violations are infrequent occurrences, as fishermen generally adhere to regulations, identifying them becomes akin to an anomaly outlier detection task. Since labeled data distinguishing between normal and anomalous instances is not available for catch reports from Norwegian waters, we have opted for more conventional approaches, such as clustering methods, to identify potential clusters and outliers. Moreover, the catch reports inherently exhibit randomness and noise due to environmental factors and potential errors made by fishermen during report registration which complicates the processes of scaling, clustering, and anomaly detection. Through experimentation with various scaling and clustering techniques, we have observed that many of these methods tend to group the data based on the species caught, exhibiting a high level of agreement in cluster formation, indicating the stability of the clusters. Anomaly detection methods, however, yield varying potential outliers as it is a more challenging task.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b5a57883d5dd81a134bfc40453ff1c1a9ff2136b" target='_blank'>
              Analysing Unlabeled Data with Randomness and Noise: The Case of Fishery Catch Reports
              </a>
            </td>
          <td>
            Aida Ashrafi, B. Tessem, Katja Enberg
          </td>
          <td>2024-06-14</td>
          <td>Linköping Electronic Conference Proceedings</td>
          <td>0</td>
          <td>18</td>
        </tr>

        <tr id="Time series forecasting plays a crucial role in various fields, including economics, finance, and weather prediction. This study explores the application of time series forecasting techniques to predict future values based on historical data patterns. Different models, such as ARIMA, SARIMA, and exponential smoothing, are commonly employed to capture the underlying trends, seasonality, and irregularities in time series data. The accuracy of the forecasts depends on selecting the appropriate model and fine-tuning its parameters. Regular validation and refinement of the forecasting models are essential to ensure their reliability and adaptability to changing conditions. Overall, time series forecasting serves as a valuable tool for decision-making, resource allocation, and planning in numerous domains. The Autoregressive Integrated Moving Average (ARIMA) model was used in this research to predict Albanian exports. The ARIMA models provided reliable forecasts for the near future and placed more importance on recent observations rather than distant past data. However, a limitation of this study is the lack of using multiple models on new data to improve future forecasts. Considering the significance of exports in Albania, it is crucial to periodically validate and refine the model-building exercises. Based on the findings, the study recommends using the ARIMA (3,2,3) model for forecasting. Furthermore, the forecast errors were statistically tested and validated, confirming the model's strong predictive ability. It is important to note that a forecasting technique that performs well in one situation may not be suitable for another. Therefore, the validation of a specific model should be regularly assessed as time progresses. For the purpose of forecasting the annual exports of Albania, researchers can employ these models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0d8ae00de95edb0226fad574129969bcb6e56842" target='_blank'>
              FORECASTING EXPORTS IN ALBANIA
              </a>
            </td>
          <td>
            Brikena Sulejmani
          </td>
          <td>2024-07-30</td>
          <td>PEOPLE: International Journal of Social Sciences</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Probabilistic forecasting models for joint distributions of targets in irregular time series are a heavily under-researched area in machine learning with, to the best of our knowledge, only three models researched so far: GPR, the Gaussian Process Regression model~\citep{Durichen2015.Multitask}, TACTiS, the Transformer-Attentional Copulas for Time Series~\cite{Drouin2022.Tactis, ashok2024tactis} and ProFITi \citep{Yalavarthi2024.Probabilistica}, a multivariate normalizing flow model based on invertible attention layers. While ProFITi, thanks to using multivariate normalizing flows, is the more expressive model with better predictive performance, we will show that it suffers from marginalization inconsistency: it does not guarantee that the marginal distributions of a subset of variables in its predictive distributions coincide with the directly predicted distributions of these variables. Also, TACTiS does not provide any guarantees for marginalization consistency. We develop a novel probabilistic irregular time series forecasting model, Marginalization Consistent Mixtures of Separable Flows (moses), that mixes several normalizing flows with (i) Gaussian Processes with full covariance matrix as source distributions and (ii) a separable invertible transformation, aiming to combine the expressivity of normalizing flows with the marginalization consistency of Gaussians. In experiments on four different datasets we show that moses outperforms other state-of-the-art marginalization consistent models, performs on par with ProFITi, but different from ProFITi, guarantee marginalization consistency.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cae76678907266cc922fd43d348b16dedc1d8a1d" target='_blank'>
              Marginalization Consistent Mixture of Separable Flows for Probabilistic Irregular Time Series Forecasting
              </a>
            </td>
          <td>
            Vijaya Krishna Yalavarthi, Randolf Scholz, Kiran Madhusudhanan, Stefan Born, Lars Schmidt-Thieme
          </td>
          <td>2024-06-11</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>4</td>
        </tr>

        <tr id="Convolutional Neural Network (CNN) struggle to capture the multi-dimensional structural information of complex high-dimensional data, which limits their feature learning capability. This paper proposes a feature fusion method based on Topological Data Analysis (TDA) and CNN, named TDA-CNN. This method combines numerical distribution features captured by CNN with topological structure features captured by TDA to improve the feature learning and representation ability of CNN. TDA-CNN divides feature extraction into a CNN channel and a TDA channel. CNN channel extracts numerical distribution features, and the TDA channel extracts topological structure features. The two types of features are fused to form a combined feature representation, with the importance weights of each feature adaptively learned through an attention mechanism. Experimental validation on datasets such as Intel Image, Gender Images, and Chinese Calligraphy Styles by Calligraphers demonstrates that TDA-CNN improves the performance of VGG16, DenseNet121, and GoogleNet networks by 17.5%, 7.11%, and 4.45%, respectively. TDA-CNN demonstrates improved feature clustering and the ability to recognize important features. This effectively enhances the model's decision-making ability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5105f773dedf7106e4e996d6d2c7f094ec1f022f" target='_blank'>
              Research on fusing topological data analysis with convolutional neural network
              </a>
            </td>
          <td>
            Yang Han, Guangjun Qin, Ziyuan Liu, Yongqing Hu, Guangnan Liu, Qinglong Dai
          </td>
          <td>2024-06-19</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Conformal prediction offers a practical framework for distribution-free uncertainty quantification, providing finite-sample coverage guarantees under relatively mild assumptions on data exchangeability. However, these assumptions cease to hold for time series due to their temporally correlated nature. In this work, we present a novel use of conformal prediction for time series forecasting that incorporates time series decomposition. This approach allows us to model different temporal components individually. By applying specific conformal algorithms to each component and then merging the obtained prediction intervals, we customize our methods to account for the different exchangeability regimes underlying each component. Our decomposition-based approach is thoroughly discussed and empirically evaluated on synthetic and real-world data. We find that the method provides promising results on well-structured time series, but can be limited by factors such as the decomposition step for more complex data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/378cd3711a8103879551b49650e1ffcd928c112a" target='_blank'>
              Conformal time series decomposition with component-wise exchangeability
              </a>
            </td>
          <td>
            Derck W. E. Prinzhorn, Thijmen Nijdam, Putri A. van der Linden, Alexander Timans
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Nonlinear deformation is a dynamically changing pattern of multiple surface deformations caused by groundwater overexploitation, underground coal mining, landslides, urban construction, etc., which are often accompanied by severe damage to surface structures or lead to major geological disasters; therefore, the high-precision monitoring and prediction of nonlinear surface deformation is significant. Traditional deep learning methods encounter challenges such as long-term dependencies or difficulty capturing complex spatiotemporal patterns when predicting nonlinear deformations. In this study, we developed a dual-attention-mechanism CNN-LSTM network model (DACLnet) to monitor and accurately predict nonlinear surface deformations precisely. Using advanced time series InSAR results as input, the DACLnet integrates the spatial feature extraction capability of a convolutional neural network (CNN), the advantages of the time series learning of a long short-term memory (LSTM) network, and the enhanced focusing effect of the dual-attention mechanism on crucial information, significantly improving the prediction accuracy of nonlinear surface deformations. The groundwater overexploitation area of the Turpan Basin, China, is selected to test the nonlinear deformation prediction effect of the proposed DACLnet. The results demonstrate that the DACLnet accurately captures developmental trends in historical surface deformations and effectively predicts surface deformations for the next two months in the study area. Compared to traditional LSTM and CNN-LSTM methods, the root mean square error (RMSE) of the DACLnet improved by 85.09% and 68.57%, respectively. These research results can provide crucial technical support for the early warning and prevention of geological disasters and can serve as an effective alternative tool for short-term ground subsidence prediction in areas lacking hydrogeological and other related data.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a3e4cd39b87b946aaea06a1cb87af826f7c91e45" target='_blank'>
              DACLnet: A Dual-Attention-Mechanism CNN-LSTM Network for the Accurate Prediction of Nonlinear InSAR Deformation
              </a>
            </td>
          <td>
            Junyu Lu, Yuedong Wang, Yafei Zhu, Jingtao Liu, Yang Xu, Honglei Yang, Yuebin Wang
          </td>
          <td>2024-07-05</td>
          <td>Remote Sensing</td>
          <td>1</td>
          <td>1</td>
        </tr>

        <tr id="If the future can be predicted from network traffic data, which is a time series, it can achieve effects such as efficient resource allocation, prevention of malicious attacks, and energy saving. Many models based on statistical and deep learning techniques have been proposed, and most of these studies have focused on improving model structures and learning algorithms. Another approach to improving the prediction performance of the model is to obtain a good-quality data. With the aim of obtaining a good-quality data, this paper applies a dense sampling technique that augments time series data to the application of network traffic prediction and analyzes the performance improvement. As a dataset, UNSW-NB15, which is widely used for network traffic analysis, is used. Performance is analyzed using RMSE, MAE, and MAPE. To increase the objectivity of performance measurement, experiment is performed independently 10 times and the performance of existing sparse sampling and dense sampling is compared as a box plot. As a result of comparing the performance by changing the window size and the horizon factor, dense sampling consistently showed a better performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0384fa51dd8ad01df892352992eeae5a963b7696" target='_blank'>
              Improving prediction performance of network traffic using dense sampling technique
              </a>
            </td>
          <td>
            Jin-Seon Lee, Il-Seok Oh
          </td>
          <td>2024-06-28</td>
          <td>Korean Institute of Smart Media</td>
          <td>0</td>
          <td>6</td>
        </tr>

        <tr id="The recurring neural network is a deep learning algorithm that is commonly used to develop prediction systems. There are many variants of RNN such as RNN itself, long-short-term memory (LSTM), and gated recurring unit, so it is frequently debatable which algorithm from the RNN family has the most optimal efficiency and computation time. When developing a prediction system, sequential or time series data is required so that an accurate prediction can be made. Sequential or time series data involve data arranged in a time sequence, such as weather data, financial data, carbon emission data, and traffic data recorded over time. This research will be carried out by predicting the three RNN models against historical Bitcoin value data. The research method used is Experimental Design by comparing the performance between the three models on bitcoin value time series data, testing is done by involving hyperparameters such as Tanh, Sigmoid, and ReLU activation functions, batch size, and epochs. The aim of this research is to find out which RNN model can produce the most optimal performance and find out what performance measures can be used to evaluate and compare the performance between the three models. The results of the study show that LSTM is the most effective model with RMSE 0.012441 and MSE 0.000155 but inefficient because it takes 3 minutes 24 seconds to run the computation; in the meantime, the Tanh activation function gives the most optimal prediction than Sigmoid and RelU and therefore should be the main candidate to be used with RNN models when predicting Bitcoin prices.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/2db311833b4fea063c64e56f36d23366568c488b" target='_blank'>
              Comparative Analysis of Recurrent Neural Network Models Performance in Predicting Bitcoin Prices
              </a>
            </td>
          <td>
            Zidane Ikkoy Ramadhan, H. Widiputra
          </td>
          <td>2024-06-21</td>
          <td>Jurnal RESTI (Rekayasa Sistem dan Teknologi Informasi)</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Dealing with atypical traffic scenarios remains a challenging task in autonomous driving. However, most anomaly detection approaches cannot be trained on raw sensor data but require exposure to outlier data and powerful semantic segmentation models trained in a supervised fashion. This limits the representation of normality to labeled data, which does not scale well. In this work, we revisit unsupervised anomaly detection and present UMAD, leveraging generative world models and unsupervised image segmentation. Our method outperforms state-of-the-art unsupervised anomaly detection.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c7ab37705550c9ad32f1724d211d09464ab3972f" target='_blank'>
              UMAD: Unsupervised Mask-Level Anomaly Detection for Autonomous Driving
              </a>
            </td>
          <td>
            Daniel Bogdoll, Noël Ollick, Tim Joseph, J. M. Zöllner
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="The integration of Graph Neural Networks (GNNs) and Neural Ordinary and Partial Differential Equations has been extensively studied in recent years. GNN architectures powered by neural differential equations allow us to reason about their behavior, and develop GNNs with desired properties such as controlled smoothing or energy conservation. In this paper we take inspiration from Turing instabilities in a Reaction Diffusion (RD) system of partial differential equations, and propose a novel family of GNNs based on neural RD systems. We \textcolor{black}{demonstrate} that our RDGNN is powerful for the modeling of various data types, from homophilic, to heterophilic, and spatio-temporal datasets. We discuss the theoretical properties of our RDGNN, its implementation, and show that it improves or offers competitive performance to state-of-the-art methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/57254c8083a6fe4f39ea4b61b190c97bb047ab60" target='_blank'>
              Graph Neural Reaction Diffusion Models
              </a>
            </td>
          <td>
            Moshe Eliasof, Eldad Haber, Eran Treister
          </td>
          <td>2024-06-16</td>
          <td>ArXiv</td>
          <td>1</td>
          <td>15</td>
        </tr>

        <tr id="In the modern transport industry, vast and diverse information arrays, particularly those including time series data, are rapidly expanding. This growth presents an opportunity to improve the quality of forecasting. Researchers and practitioners are continuously developing innovative tools to predict their future values. The goal of the research is to improve the performance of automated forecasting environments in a systematic and structured way. This paper investigates the effect of substituting the initial time series with another of a similar nature, during the training phase of the model’s development. A financial data set and the Prophet model are employed for this objective. It is observed that the impact on the accuracy of the predicted future values is promising, albeit not significant. Based on the obtained results, valuable conclusions are drawn, and recommendations for further improvements are provided. By highlighting the importance of diverse data incorporation, this research assists in making informed choices and leveraging the full potential of available information for more precise forecasting outcomes.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/c8cdf9821562b0563c9797fb5821a5a46f40c681" target='_blank'>
              Time Series Cross-Sequence Prediction
              </a>
            </td>
          <td>
            K. Koparanov, E. Antonova, D. Minkovska, K. Georgiev
          </td>
          <td>2024-07-19</td>
          <td>WSEAS TRANSACTIONS ON BUSINESS AND ECONOMICS</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="As the user’s behavior changes at any time with cloud computing and network services, abnormal server resource utilization traffic will lead to severe service crashes and system downtime. The traditional single anomaly detection model cannot handle the rapid failure prediction ahead. Therefore, this study proposed ensemble learning combined with model-agnostic meta-reinforcement learning called ensemble meta-reinforcement learning (EMRL) to implement self-adaptive server anomaly detection rapidly and precisely, according to the time series of server resource utilization. The proposed ensemble approach combines hidden Markov model (HMM), variational autoencoder (VAE), temporal convolutional autoencoder (TCN-AE), and bidirectional long short-term memory (BLSTM). The EMRL algorithm trains this combination with several tasks to learn the implicit representation of various anomalous traffic, where each task executes trust region policy optimization (TRPO) to quickly adapt the time-varying data distribution and make rapid decisions precisely for an agent response. As a result, our proposed approach can improve the precision of anomaly prediction by 2.4 times and reduce the model deployment speed by 5.8 times on average because a meta-learner can immediately be applied to new tasks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d5c77be21defbeef3ef6ab96ede03dbafac4465e" target='_blank'>
              Self-Adaptive Server Anomaly Detection Using Ensemble Meta-Reinforcement Learning
              </a>
            </td>
          <td>
            B. Chang, H. Tsai, Guan-Ru Chen
          </td>
          <td>2024-06-15</td>
          <td>Electronics</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="This paper provides a comprehensive and detailed derivation of the backpropagation algorithm for graph convolutional neural networks using matrix calculus. The derivation is extended to include arbitrary element-wise activation functions and an arbitrary number of layers. The study addresses two fundamental problems, namely node classification and link prediction. To validate our method, we compare it with reverse-mode automatic differentiation. The experimental results demonstrate that the median sum of squared errors of the updated weight matrices, when comparing our method to the approach using reverse-mode automatic differentiation, falls within the range of $10^{-18}$ to $10^{-14}$. These outcomes are obtained from conducting experiments on a five-layer graph convolutional network, applied to a node classification problem on Zachary's karate club social network and a link prediction problem on a drug-drug interaction network. Finally, we show how the derived closed-form solution can facilitate the development of explainable AI and sensitivity analysis.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb3d9973fdba5eab7ecd6df75a82a06eea0ac6e4" target='_blank'>
              Derivation of Back-propagation for Graph Convolutional Networks using Matrix Calculus and its Application to Explainable Artificial Intelligence
              </a>
            </td>
          <td>
            Yen-Che Hsiao, Rongting Yue, Abhishek Dutta
          </td>
          <td>2024-08-02</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The identification of undesirable behavior in event logs is an important aspect of process mining that is often addressed by anomaly detection methods. Traditional anomaly detection methods tend to focus on statistically rare behavior and neglect the subtle difference between rarity and undesirability. The introduction of semantic anomaly detection has opened a promising avenue by identifying semantically deviant behavior. This work addresses a gap in semantic anomaly detection, which typically indicates the occurrence of an anomaly without explaining the nature of the anomaly. We propose xSemAD, an approach that uses a sequence-to-sequence model to go beyond pure identification and provides extended explanations. In essence, our approach learns constraints from a given process model repository and then checks whether these constraints hold in the considered event log. This approach not only helps understand the specifics of the undesired behavior, but also facilitates targeted corrective actions. Our experiments demonstrate that our approach outperforms existing state-of-the-art semantic anomaly detection methods.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/6a523dca5788941634b2c52f6841a4b22c312b94" target='_blank'>
              xSemAD: Explainable Semantic Anomaly Detection in Event Logs Using Sequence-to-Sequence Models
              </a>
            </td>
          <td>
            Kiran Busch, T. Kampik, Henrik Leopold
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Graph neural networks (GNNs) have demonstrated significant potential in analyzing complex graph-structured data. However, conventional GNNs encounter challenges in effectively incorporating global and local features. Therefore, this paper introduces a novel approach for GNN called multichannel adaptive data mixture augmentation (MAME-GNN). It enhances a GNN by adopting a multi-channel architecture and interactive learning to effectively capture and coordinate the interrelationships between local and global graph structures. Additionally, this paper introduces the polynomial–Gaussian mixture graph interpolation method to address the problem of single and sparse graph data, which generates diverse and nonlinear transformed samples, improving the model's generalization ability. The proposed MAME-GNN is validated through extensive experiments on publicly available datasets, showcasing its effectiveness. Compared to existing GNN models, the MAME-GNN exhibits superior performance, significantly enhancing the model's robustness and generalization ability.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b564aad743a63ef52877bc303764a84c8b6a66cb" target='_blank'>
              Multichannel Adaptive Data Mixture Augmentation for Graph Neural Networks
              </a>
            </td>
          <td>
            Zhonglin Ye, Lin Zhou, Mingyuan Li, Wei Zhang, Zhen Liu, Haixing Zhao
          </td>
          <td>2024-08-05</td>
          <td>International Journal of Data Warehousing and Mining</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Through integrating the evolutionary correlations across global states in the bidirectional recursion, an explainable Bayesian recurrent neural smoother (EBRNS) is proposed for offline data-assisted fixed-interval state smoothing. At first, the proposed model, containing global states in the evolutionary interval, is transformed into an equivalent model with bidirectional memory. This transformation incorporates crucial global state information with support for bi-directional recursive computation. For the transformed model, the joint state-memory-trend Bayesian filtering and smoothing frameworks are derived by introducing the bidirectional memory iteration mechanism and offline data into Bayesian estimation theory. The derived frameworks are implemented using the Gaussian approximation to ensure analytical properties and computational efficiency. Finally, the neural network modules within EBRNS and its two-stage training scheme are designed. Unlike most existing approaches that artificially combine deep learning and model-based estimation, the bidirectional recursion and internal gated structures of EBRNS are naturally derived from Bayesian estimation theory, explainably integrating prior model knowledge, online measurement, and offline data. Experiments on representative real-world datasets demonstrate that the high smoothing accuracy of EBRNS is accompanied by data efficiency and a lightweight parameter scale.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/57349533774d9106325c9d199314485305e9ade6" target='_blank'>
              Explainable Bayesian Recurrent Neural Smoother to Capture Global State Evolutionary Correlations
              </a>
            </td>
          <td>
            Shi Yan, Yan Liang, Huayu Zhang, Le Zheng, Difan Zou, Binglu Wang
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Reliable gas price forecasts are an essential information for gas and energy traders, for risk managers and also economists. However, ahead of the war in Ukraine Europe began to suffer from substantially increased and volatile gas prices which culminated in the aftermath of the North Stream 1 explosion. This shock changed both trend and volatility structure of the prices and has considerable effects on forecasting models. In this study we investigate whether modern machine learning methods such as neural networks are more resilient against such changes than statistical models such as autoregressive moving average (ARMA) models with conditional heteroskedasticity, or copula-based time series models. Thereby the focus lies on interval forecasting and applying respective evaluation measures. As data, the Front Month prices from the Dutch Title Transfer Facility, currently the predominant European exchange, are used. We see that, during the shock period, most models underestimate the variance while overestimating the variance in the after-shock period. Furthermore, we recognize that, during the shock, the simpler models, i.e. an ARMA model with conditional heteroskedasticity and the multilayer perceptron (a neural network), perform best with regards to prediction interval coverage. Interestingly, the widely-used long-short term neural network is outperformed by its competitors.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/85754bf41e34f87a3356a090d97ccc44867749ef" target='_blank'>
              Interval Forecasts for Gas Prices in the Face of Structural Breaks -- Statistical Models vs. Neural Networks
              </a>
            </td>
          <td>
            Stephan Schluter, Sven Pappert, Martin Neumann
          </td>
          <td>2024-07-23</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Explainable Artificial Intelligence (XAI) has become a widely discussed topic, the related technologies facilitate better understanding of conventional black-box models like Random Forest, Neural Networks and etc. However, domain-specific applications of XAI are still insufficient. To fill this gap, this research analyzes various machine learning models to the tasks of binary and multi-class classification for intrusion detection from network traffic on the same dataset using occlusion sensitivity. The models evaluated include Linear Regression, Logistic Regression, Linear Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Random Forest, Decision Trees, and Multi-Layer Perceptrons (MLP). We trained all models to the accuracy of 90\% on the UNSW-NB15 Dataset. We found that most classifiers leverage only less than three critical features to achieve such accuracies, indicating that effective feature engineering could actually be far more important for intrusion detection than applying complicated models. We also discover that Random Forest provides the best performance in terms of accuracy, time efficiency and robustness. Data and code available at https://github.com/pcwhy/XML-IntrusionDetection.git">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/811524166a4696543d364114d34371206937cb49" target='_blank'>
              Explainable AI for Comparative Analysis of Intrusion Detection Models
              </a>
            </td>
          <td>
            Pap M. Corea, Yongxin Liu, Jian Wang, Shuteng Niu, Houbing Song
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Anomaly Detection is a task in engineering aiming at identifying deviations from expected patterns in data. Data-driven approaches have emerged in past recent years due to the fact that a model of complex system may be hard or impossible to be derived in many scenarios. Moreover, unsupervised approaches have been particularly appealing for practitioners and scientists given the typical unavailability of tagged data. Such approaches are often integrated in frameworks, like Decision Support Systems, that assist domain experts and operators in the monitoring task. Human presence, by providing a limited amount of feedback, can be leveraged as a valuable source of information to iteratively enhance detection performance. In this work we introduce Extended B-ALIF, a framework designed to incrementally select and integrate expert feedback into the Extended Isolation Forest anomaly detection model. This study extends Bayesian Active Learning Isolation Forest (B-ALIF), which originally proposed the same theoretical principles for another anomaly detection model, the Isolation Forest.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d95dbcf1a2f9069a2cd1d4cbe6cc308a5a41793a" target='_blank'>
              Extended B-ALIF: Improving Anomaly Detection with Human Feedback
              </a>
            </td>
          <td>
            Valentina Zaccaria, Davide Sartor, Gian-Antonio Susto
          </td>
          <td>2024-06-11</td>
          <td>2024 32nd Mediterranean Conference on Control and Automation (MED)</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Graph Neural Networks (GNNs) have emerged as powerful tools for supervised machine learning over graph-structured data, while sampling-based node representation learning is widely utilized in unsupervised learning. However, scalability remains a major challenge in both supervised and unsupervised learning for large graphs (e.g., those with over 1 billion nodes). The scalability bottleneck largely stems from the mini-batch sampling phase in GNNs and the random walk sampling phase in unsupervised methods. These processes often require storing features or embeddings in memory. In the context of distributed training, they require frequent, inefficient random access to data stored across different workers. Such repeated inter-worker communication for each mini-batch leads to high communication overhead and computational inefficiency. We propose GraphScale, a unified framework for both supervised and unsupervised learning to store and process large graph data distributedly. The key insight in our design is the separation of workers who store data and those who perform the training. This separation allows us to decouple computing and storage in graph training, thus effectively building a pipeline where data fetching and data computation can overlap asynchronously. Our experiments show that GraphScale outperforms state-of-the-art methods for distributed training of both GNNs and node embeddings. We evaluate GraphScale both on public and proprietary graph datasets and observe a reduction of at least 40% in end-to-end training times compared to popular distributed frameworks, without any loss in performance. While most existing methods don't support billion-node graphs for training node embeddings, GraphScale is currently deployed in production at TikTok enabling efficient learning over such large graphs.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e5115bc6bf426026833b4f7fb72ae6282f678143" target='_blank'>
              GraphScale: A Framework to Enable Machine Learning over Billion-node Graphs
              </a>
            </td>
          <td>
            Vipul Gupta, Xin Chen, Ruoyun Huang, Fanlong Meng, Jianjun Chen, Yujun Yan
          </td>
          <td>2024-07-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="The effectiveness of univariate forecasting models is often hampered by conditions that cause them stress. A model is considered to be under stress if it shows a negative behaviour, such as higher-than-usual errors or increased uncertainty. Understanding the factors that cause stress to forecasting models is important to improve their reliability, transparency, and utility. This paper addresses this problem by contributing with a novel framework called MAST (Meta-learning and data Augmentation for Stress Testing). The proposed approach aims to model and characterize stress in univariate time series forecasting models, focusing on conditions where they exhibit large errors. In particular, MAST is a meta-learning approach that predicts the probability that a given model will perform poorly on a given time series based on a set of statistical time series features. MAST also encompasses a novel data augmentation technique based on oversampling to improve the metadata concerning stress. We conducted experiments using three benchmark datasets that contain a total of 49.794 time series to validate the performance of MAST. The results suggest that the proposed approach is able to identify conditions that lead to large errors. The method and experiments are publicly available in a repository.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/21df316c9190406997a7ddcf7668a9d6a38d85cf" target='_blank'>
              Meta-learning and Data Augmentation for Stress Testing Forecasting Models
              </a>
            </td>
          <td>
            Ricardo In'acio, Vítor Cerqueira, Mar'ilia Barandas, Carlos Soares
          </td>
          <td>2024-06-24</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Graph neural networks (GNNs) have demonstrated efficient processing of graph-structured data, making them a promising method for electroencephalogram (EEG) emotion recognition. However, due to dynamic functional connectivity and nonlinear relationships between brain regions, representing EEG as graph data remains a great challenge. To solve this problem, we proposed a multi-domain based graph representation learning (MD 2 GRL) framework to model EEG signals as graph data. Specifically, MD 2 GRL leverages gated recurrent units (GRU) and power spectral density (PSD) to construct node features of two subgraphs. Subsequently, the self-attention mechanism is adopted to learn the similarity matrix between nodes and fuse it with the intrinsic spatial matrix of EEG to compute the corresponding adjacency matrix. In addition, we introduced a learnable soft thresholding operator to sparsify the adjacency matrix to reduce noise in the graph structure. In the downstream task, we designed a dual-branch GNN and incorporated spatial asymmetry for graph coarsening. We conducted experiments using the publicly available datasets SEED and DEAP, separately for subject-dependent and subject-independent, to evaluate the performance of our model in emotion classification. Experimental results demonstrated that our method achieved state-of-the-art (SOTA) classification performance in both subject-dependent and subject-independent experiments. Furthermore, the visualization analysis of the learned graph structure reveals EEG channel connections that are significantly related to emotion and suppress irrelevant noise. These findings are consistent with established neuroscience research and demonstrate the potential of our approach in comprehending the neural underpinnings of emotion.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/37f3fa838df7d196b5883aabd08b11a8b7cc4559" target='_blank'>
              Multi-Domain Based Dynamic Graph Representation Learning for EEG Emotion Recognition.
              </a>
            </td>
          <td>
            Hao Tang, Songyun Xie, Xinzhou Xie, Yujie Cui, Bohan Li, Dalu Zheng, Yu Hao, Xiangming Wang, Yiye Jiang, Zhongyu Tian
          </td>
          <td>2024-06-17</td>
          <td>IEEE journal of biomedical and health informatics</td>
          <td>0</td>
          <td>9</td>
        </tr>

        <tr id="Adaptive machine learning models are revolutionizing real-time financial fraud prevention in dynamic environments, offering unparalleled accuracy and responsiveness to evolving fraud patterns. Financial institutions face constant threats from increasingly sophisticated fraud schemes that adapt and change over time. Traditional static models often fall short in addressing these rapidly shifting threats, necessitating the adoption of adaptive machine learning techniques. Adaptive machine learning models are designed to evolve continuously by learning from new data and adjusting to emerging fraud patterns. These models employ advanced algorithms, such as reinforcement learning, online learning, and deep learning, to maintain their effectiveness in detecting and preventing fraud. Reinforcement learning algorithms optimize detection strategies by receiving feedback from their actions, continually improving their decision-making processes. Online learning algorithms update models incrementally as new transaction data becomes available, ensuring that the models remain current and responsive. One of the key strengths of adaptive machine learning models is their ability to process vast amounts of data in real time. By leveraging technologies such as neural networks and ensemble learning, these models can analyze complex datasets, identify subtle anomalies, and detect fraudulent activities with high precision. Real-time data processing capabilities enable immediate detection and response to suspicious transactions, significantly reducing the risk of financial losses. Adaptive models also incorporate anomaly detection techniques to identify deviations from normal transaction behavior. By constantly learning from the latest data, these models can recognize previously unseen fraud patterns, providing a robust defense against novel threats. Additionally, the integration of explainable AI (XAI) techniques ensures that the decision-making processes of these models are transparent and interpretable, fostering trust and compliance with regulatory requirements. Implementing adaptive machine learning models for real-time fraud prevention involves addressing challenges such as data quality, computational efficiency, and model interpretability. Financial institutions must ensure the availability of high-quality data and invest in robust computational infrastructure to support real-time processing. Furthermore, adopting explainable AI techniques enhances model transparency and regulatory compliance. In conclusion, adaptive machine learning models offer a dynamic and effective solution for real-time financial fraud prevention. By continuously learning and adapting to new data, these models provide a resilient defense against evolving fraud schemes, enhancing the security and integrity of financial transactions. This adaptive approach not only mitigates financial risks but also strengthens the overall trustworthiness of financial systems.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1903ef02b963217f1096835c67029c6e655df6ce" target='_blank'>
              Adaptive machine learning models: Concepts for real-time financial fraud prevention in dynamic environments
              </a>
            </td>
          <td>
            Halima Oluwabunmi, Halima Oluwabunmi Bello, Adebimpe Bolatito, Maxwell Nana Ameyaw
          </td>
          <td>2024-07-30</td>
          <td>World Journal of Advanced Engineering Technology and Sciences</td>
          <td>2</td>
          <td>0</td>
        </tr>

        <tr id="
The task of evaluating deep learning algorithms in the context of Structural Health Monitoring (SHM) for damage detection is made particularly challenging by the limited availability of empirical data from damaged structures. Making it impossible to assert whether the trained algorithm would be able to pick up changes due to (unseen) damage. This study puts forth a methodology that employs synthesized anomalies to advance our understanding of the specific conditions under which deep learning algorithms for anomaly detection are succesfull and when they prove to be insensitive to damage. 



This research aims to develop a comprehensive deep learning model that utilizes raw data as its input, negating the need for specialized preprocessing or the development of anomaly indices that are constrained to specific types of anomalies. Central to our methodology is the introduction of simulated damage into the data set through various manipulations. This evaluation method could be generalized across diverse sensor types, such as accelerometer data (ACCs) and Fiber Bragg Sensors (FBGs). 



As a case study, we focus on accelerometer data, utilizing Power Spectral Density (PSD) as the input for a deep learning-based anomaly detection algorithm. The study employs both attenuation and the addition of harmonics at varying levels and frequencies to mimic anomalies, thereby investigation the model's areas of sensitivity. To empirically validate the approach, an 8-degree-of-freedom simulated system is used, and environmental effect is modelled by a linear stiffness reduction across all degrees of freedom (DOFs). Structural damage is then simulated by altering the stiffness of a specific DOF. 



Our results demonstrate a robust correlation between the model’s success in identifying these synthesized anomalies and its capability to detect actual structural damage. This correlation serves as a valuable guide for hyperparameter optimization. 
">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a2e24680719808c4db3bd7cbb829639fe637fab9" target='_blank'>
              An Evaluation Framework for Deep Learning-Based Anomaly Detection in Structural Health Monitoring.
              </a>
            </td>
          <td>
            Yacine Bel-Hadj, W. Weijtjens, C. Devriendt
          </td>
          <td>2024-07-01</td>
          <td>e-Journal of Nondestructive Testing</td>
          <td>0</td>
          <td>22</td>
        </tr>

        <tr id="Time series classification stands as a pivotal and intricate challenge across various domains, including finance, healthcare, and industrial systems. In contemporary research, there has been a notable upsurge in exploring feature extraction through random sampling. Unlike deep convolutional networks, these methods sidestep elaborate training procedures, yet they often necessitate generating a surplus of features to comprehensively encapsulate time series nuances. Consequently, some features may lack relevance to labels or exhibit multi-collinearity with others. In this paper, we propose a novel hierarchical feature selection method aided by ANOVA variance analysis to address this challenge. Through meticulous experimentation, we demonstrate that our method substantially reduces features by over 94% while preserving accuracy -- a significant advancement in the field of time series analysis and feature selection.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5c07ba99c8d7ae5a1590ce850bbbbc596fbc4609" target='_blank'>
              HIERVAR: A Hierarchical Feature Selection Method for Time Series Analysis
              </a>
            </td>
          <td>
            Alireza Keshavarzian, S. Valaee
          </td>
          <td>2024-07-22</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>44</td>
        </tr>

        <tr id="Given the ever-increasing volume of traffic and the developing nature of internet communications, security has emerged as one of the primary considerations in the design and implementation of any system or network. The frequency of network attacks is increasing, so the need for protecting both the users’ and systems’ data is crucial. In recent years, there has been an increase in use of machine learning and deep learning techniques as an aid for detecting network attacks. Various deep learning models have been in focus of researchers for the past couple of years, having in mind their capabilities for processing large amounts of data. However, when it comes to handling multi-class imbalanced data, proper detection of multiple attacks is still a challenge. In this paper, a multiclass network intrusion detection system is implemented using a convolutional neural network design. The CICIDS-2017 dataset, a highly imbalanced dataset with 15 distinct traffic classes, is used to train and test the model. Two feature selection techniques are used, and depending on the input dataset dimensions and the hyperparameter variations, 24 different CNN model variations are trained. In order to evaluate the models objectively, macro average metric is used. A comparison is made with a DNN model, in order to show the CNN model capabilities. It is shown that the proposed model outperforms DNN and detects multiple classes that are represented by a limited number of inputs in the dataset with high precision.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/404fbf4ea8e5ecb6354ac95c16e37d4d93b12acf" target='_blank'>
              Intrusion Detection System for Multiclass Detection based on a Convolutional Neural Network
              </a>
            </td>
          <td>
            Marija Milosevic, Vladimir Ciric, I. Milentijevic
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE 22nd Mediterranean Electrotechnical Conference (MELECON)</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Anomaly detection and localization in medical imaging remain critical challenges in healthcare. This paper introduces Spatial-MSMA (Multiscale Score Matching Analysis), a novel unsupervised method for anomaly localization in volumetric brain MRIs. Building upon the MSMA framework, our approach incorporates spatial information and conditional likelihoods to enhance anomaly detection capabilities. We employ a flexible normalizing flow model conditioned on patch positions and global image features to estimate patch-wise anomaly scores. The method is evaluated on a dataset of 1,650 T1- and T2-weighted brain MRIs from typically developing children, with simulated lesions added to the test set. Spatial-MSMA significantly outperforms existing methods, including reconstruction-based, generative-based, and interpretation-based approaches, in lesion detection and segmentation tasks. Our model achieves superior performance in both distance-based metrics (99th percentile Hausdorff Distance: $7.05 \pm 0.61$, Mean Surface Distance: $2.10 \pm 0.43$) and component-wise metrics (True Positive Rate: $0.83 \pm 0.01$, Positive Predictive Value: $0.96 \pm 0.01$). These results demonstrate Spatial-MSMA's potential for accurate and interpretable anomaly localization in medical imaging, with implications for improved diagnosis and treatment planning in clinical settings. Our code is available at~\url{https://github.com/ahsanMah/sade/}.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1d450e15d6b70b0920b525583236ea7b9d7d9291" target='_blank'>
              Localizing Anomalies via Multiscale Score Matching Analysis
              </a>
            </td>
          <td>
            Ahsan Mahmood, Junier Oliva, M. Styner
          </td>
          <td>2024-06-28</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="Anomaly detection deals with detecting deviations from established patterns within data. It has various applications like autonomous driving, predictive maintenance, and medical diagnosis. To improve anomaly detection accuracy, transfer learning can be applied to large, pre-trained models and adapt them to the specific application context. In this paper, we propose a novel framework for online-adaptive anomaly detection using transfer learning. The approach adapts to different environments by selecting visually similar training images and online fitting a normality model to EfficientNet features extracted from the training subset. Anomaly detection is then performed by computing the Mahalanobis distance between the normality model and the test image features. Different similarity measures (SIFT/FLANN, Cosine) and normality models (MVG, OCSVM) are employed and compared with each other. We evaluate the approach on different anomaly detection benchmarks and data collected in controlled laboratory settings. Experimental results showcase a detection accuracy exceeding 0.975, outperforming the state-of-the-art ET-NET approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/f44a09b820d86a16d1d2e4a6bce168e07b770de3" target='_blank'>
              Online-Adaptive Anomaly Detection for Defect Identification in Aircraft Assembly
              </a>
            </td>
          <td>
            Siddhant Shete, Dennis Mronga, Ankita Jadhav, Frank Kirchner
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="PurposeTo optimize train operations, dispatchers currently rely on experience for quick adjustments when delays occur. However, delay predictions often involve imprecise shifts based on known delay times. Real-time and accurate train delay predictions, facilitated by data-driven neural network models, can significantly reduce dispatcher stress and improve adjustment plans. Leveraging current train operation data, these models enable swift and precise predictions, addressing challenges posed by train delays in high-speed rail networks during unforeseen events.Design/methodology/approachThis paper proposes CBLA-net, a neural network architecture for predicting late arrival times. It combines CNN, Bi-LSTM, and attention mechanisms to extract features, handle time series data, and enhance information utilization. Trained on operational data from the Beijing-Tianjin line, it predicts the late arrival time of a target train at the next station using multidimensional input data from the target and preceding trains.FindingsThis study evaluates our model's predictive performance using two data approaches: one considering full data and another focusing only on late arrivals. Results show precise and rapid predictions. Training with full data achieves a MAE of approximately 0.54 minutes and a RMSE of 0.65 minutes, surpassing the model trained solely on delay data (MAE: is about 1.02 min, RMSE: is about 1.52 min). Despite superior overall performance with full data, the model excels at predicting delays exceeding 15 minutes when trained exclusively on late arrivals. For enhanced adaptability to real-world train operations, training with full data is recommended.Originality/valueThis paper introduces a novel neural network model, CBLA-net, for predicting train delay times. It innovatively compares and analyzes the model's performance using both full data and delay data formats. Additionally, the evaluation of the network's predictive capabilities considers different scenarios, providing a comprehensive demonstration of the model's predictive performance.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e57a492ae484333d79b26e0ab06d19a0ae256575" target='_blank'>
              Short-term train arrival delay prediction: a data-driven approach
              </a>
            </td>
          <td>
            Qingyun Fu, Shuxin Ding, Tao Zhang, Rongsheng Wang, Ping Hu, Cunlai Pu
          </td>
          <td>2024-07-02</td>
          <td>Railway Sciences</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="Cyberbullying incidents have surged due to the expansion of social media network  and advancements in internet technology, presenting a substantial challenge in online communities. Previous studies employing Support Vector Machine (SVM) techniques have exhibited promising outcomes, achieving a superior accuracy of 71.25%. However, recognizing the dynamic nature of cyberbullying behaviors and the necessity for more robust detection methodologies, this research explores cyberbullying detection on Twitter utilizing Convolutional Neural Network  (CNN) and Graph Neural Network  (GNN). The selection of CNN and GNN is motivated by the deficiencies observed in prior SVM-based approaches and the capacity of neural network  to capture intricate patterns in textual and network data. The GNN consistently outperforms CNN in terms of F1 score, accuracy, precision, and recall. With only 20 epochs, GNN achieves an accuracy of 80.25%, surpassing CNN's 68.43%. Through GNN optimization, its accuracy reaches 89.04% after 100 epochs, underscoring its efficacy in Twitter cyberbullying detection.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/40b12ed5c90a328fb8c5b7145d0e6c098ac20b3b" target='_blank'>
              The Performance Analysis of Graph Neural Network (GNN) and Convolutional Neural Network (CNN) Algorithms for Cyberbullying Detection in Twitter Comments
              </a>
            </td>
          <td>
            Muhammad Rizki Nurfiqri, Fitriyani
          </td>
          <td>2024-06-15</td>
          <td>Indonesian Journal of Computer Science</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="None">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/0c71a8797e6e40c6f408339df0649384ea07564f" target='_blank'>
              Anomaly analytics in data-driven machine learning applications
              </a>
            </td>
          <td>
            Shelernaz Azimi, Claus Pahl
          </td>
          <td>2024-07-12</td>
          <td>International Journal of Data Science and Analytics</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="Score-based diffusion models have recently emerged as state-of-the-art generative models for a variety of data modalities. Nonetheless, it remains unclear how to adapt these models to generate long multivariate time series. Viewing a time series as the discretization of an underlying continuous process, we introduce SigDiffusion, a novel diffusion model operating on log-signature embeddings of the data. The forward and backward processes gradually perturb and denoise log-signatures preserving their algebraic structure. To recover a signal from its log-signature, we provide new closed-form inversion formulae expressing the coefficients obtained by expanding the signal in a given basis (e.g. Fourier or orthogonal polynomials) as explicit polynomial functions of the log-signature. Finally, we show that combining SigDiffusion with these inversion formulae results in highly realistic time series generation, competitive with the current state-of-the-art on various datasets of synthetic and real-world examples.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/ece5bfacaf676cb0effa18100fdf73cff5cb6a11" target='_blank'>
              SigDiffusions: Score-Based Diffusion Models for Long Time Series via Log-Signature Embeddings
              </a>
            </td>
          <td>
            Barbora Barancikova, Zhuoyue Huang, Cristopher Salvi
          </td>
          <td>2024-06-14</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Predicting individual behavior is a crucial area of research in neuroscience. Graph Neural Networks (GNNs), as powerful tools for extracting graph-structured features, are increasingly being utilized in various functional connectivity (FC) based behavioral prediction tasks. However, current predictive models primarily focus on enhancing GNNs' ability to extract features from FC networks while neglecting the importance of upstream individual network construction quality. This oversight results in constructed functional networks that fail to adequately represent individual behavioral capacity, thereby affecting the subsequent prediction accuracy. To address this issue, we proposed a new GNN-based behavioral prediction framework, named Dual Multi-Hop Graph Convolutional Network (D-MHGCN). Through the joint training of two GCNs, this framework integrates individual functional network construction and behavioral prediction into a unified optimization model. It allows the model to dynamically adjust the individual functional cortical parcellation according to the downstream tasks, thus creating task-aware, individual-specific FCNs that largely enhance its ability to predict behavior scores. Additionally, we employed multi-hop graph convolution layers instead of traditional single-hop methods in GCN to capture complex hierarchical connectivity patterns in brain networks. Our experimental evaluations, conducted on the large, public Human Connectome Project dataset, demonstrate that our proposed method outperforms existing methods in various behavioral prediction tasks. Moreover, it produces more functionally homogeneous cortical parcellation, showcasing its practical utility and effectiveness. Our work not only enhances the accuracy of individual behavioral prediction but also provides deeper insights into the neural mechanisms underlying individual differences in behavior.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3921c4bb26b88409af0428fce146451f777f047d" target='_blank'>
              D-MHGCN: An End-to-End Individual Behavioral Prediction Model Using Dual Multi-Hop Graph Convolutional Network.
              </a>
            </td>
          <td>
            Xuyun Wen, Qumei Cao, Yunxi Zhao, Xia Wu, Daoqiang Zhang
          </td>
          <td>2024-06-27</td>
          <td>IEEE journal of biomedical and health informatics</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="
 Deep learning is an appropriate methodology for modeling complex industrial data in the field of soft sensors, owing to its powerful feature representation capability. Given the nonlinear and dynamic nature of the process industry, the key challenge for soft sensor technology is to effectively mine dynamic information from long sequences and accurately extract features of relevance to quality. A dual temporal attention mechanism-based convolutional long short-term memory network (DTA-ConvLSTM) under an encoder-decoder framework is proposed as a soft sensor model to acquire quality-relevant dynamic features from serial data. Considering different influences of process variables for prediction at multiple time steps and various locations, ConvLSTM and temporal self-attention mechanism are utilized as the encoder to adaptively fuse spatiotemporal features and capture long-term dynamic properties of process in order to capture the trends of industrial variables. Furthermore, a quality-driven temporal attention mechanism is employed throughout the decoding process to dynamically select relevant features to more accurately track quality changes. The encoder-decoder model meticulously analyses the interactions between process and quality variables by incorporating dual-sequence dynamic information to improve the prediction performance. The validity and superiority of the DTA-ConvLSTM model was validated on two industrial case studies of the debutanizer column and sulfur recovery unit. Compared to the traditional LSTM model, the proposed model demonstrated a substantial improvement with the accuracy R2 up to 97.3% and 94.9% and the root mean square error reducing to 0.122 and 0.022.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/19a32c6b236dd1617733cd71274eee3524cafcde" target='_blank'>
              Dual Temporal Attention Mechanism-based Convolutional LSTM Model for Industrial Dynamic Soft Sensor
              </a>
            </td>
          <td>
            Jiarui Cui, Yuyu Shi, Jian Huang, Xu Yang, Jingjing Gao, Qing Li
          </td>
          <td>2024-07-24</td>
          <td>Measurement Science and Technology</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="This article discusses the innovative application of artificial intelligence (AI) to develop a predictive model that aims to evaluate the condition of the machine by focusing on the probability of failure. The research uses a synthetic dataset prepared to simulate real-world situations where machines are equipped with sensors that monitor various health indicators and record the occurrence of faults. This data set consists of 10,000 inputs, each containing five numerical measurements: air temperature, process temperature, rotation speed, torque, and machine wear, in addition to the type of product produced, for a total of six input variables. The output of the model is the fault state of the machine, displayed as true or false. 
A hybrid artificial neural network integrating a GRU (Gated Recurrent Unit)-based model with the Transformer Encoder block was used for prediction. This combination highlights the superior predictive capabilities of the model. This approach represents a shift from traditional maintenance programs, which are often time-based and often result in unnecessary resource use, to a more efficient, condition-based maintenance strategy. This new strategy aims to ensure that maintenance activities are carried out only when necessary, thus optimizing resource use and minimizing downtime.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/1c7a00ac1229d66462f4224aafa0a988c41bd217" target='_blank'>
              Disrupting Downtime: Different Deep Learning Journeys into Predictive Maintenance Anomaly Detection
              </a>
            </td>
          <td>
            Hayriye Tanyıldız, Canan Batur Şahin, Özlem Batur Dinler
          </td>
          <td>2024-06-24</td>
          <td>NATURENGS MTU Journal of Engineering and Natural Sciences Malatya Turgut Ozal University</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Effectively learning from sequential data is a longstanding goal of Artificial Intelligence, especially in the case of long sequences. From the dawn of Machine Learning, several researchers engaged in the search of algorithms and architectures capable of processing sequences of patterns, retaining information about the past inputs while still leveraging the upcoming data, without losing precious long-term dependencies and correlations. While such an ultimate goal is inspired by the human hallmark of continuous real-time processing of sensory information, several solutions simplified the learning paradigm by artificially limiting the processed context or dealing with sequences of limited length, given in advance. These solutions were further emphasized by the large ubiquity of Transformers, that have initially shaded the role of Recurrent Neural Nets. However, recurrent networks are facing a strong recent revival due to the growing popularity of (deep) State-Space models and novel instances of large-context Transformers, which are both based on recurrent computations to go beyond several limits of currently ubiquitous technologies. In fact, the fast development of Large Language Models enhanced the interest in efficient solutions to process data over time. This survey provides an in-depth summary of the latest approaches that are based on recurrent models for sequential data processing. A complete taxonomy over the latest trends in architectural and algorithmic solutions is reported and discussed, guiding researchers in this appealing research field. The emerging picture suggests that there is room for thinking of novel routes, constituted by learning algorithms which depart from the standard Backpropagation Through Time, towards a more realistic scenario where patterns are effectively processed online, leveraging local-forward computations, opening to further research on this topic.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/edc2ee3d0a55da7a9378fc256fa867f0275d8a9a" target='_blank'>
              State-Space Modeling in Long Sequence Processing: A Survey on Recurrence in the Transformer Era
              </a>
            </td>
          <td>
            Matteo Tiezzi, Michele Casoni, Alessandro Betti, Marco Gori, S. Melacci
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>14</td>
        </tr>

        <tr id="We devise a novel method for implied volatility smoothing based on neural operators. The goal of implied volatility smoothing is to construct a smooth surface that links the collection of prices observed at a specific instant on a given option market. Such price data arises highly dynamically in ever-changing spatial configurations, which poses a major limitation to foundational machine learning approaches using classical neural networks. While large models in language and image processing deliver breakthrough results on vast corpora of raw data, in financial engineering the generalization from big historical datasets has been hindered by the need for considerable data pre-processing. In particular, implied volatility smoothing has remained an instance-by-instance, hands-on process both for neural network-based and traditional parametric strategies. Our general operator deep smoothing approach, instead, directly maps observed data to smoothed surfaces. We adapt the graph neural operator architecture to do so with high accuracy on ten years of raw intraday S&P 500 options data, using a single set of weights. The trained operator adheres to critical no-arbitrage constraints and is robust with respect to subsampling of inputs (occurring in practice in the context of outlier removal). We provide extensive historical benchmarks and showcase the generalization capability of our approach in a comparison with SVI, an industry standard parametrization for implied volatility. The operator deep smoothing approach thus opens up the use of neural networks on large historical datasets in financial engineering.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9ba8ee0b732411bf71bcc071a076ad227505ac9c" target='_blank'>
              Operator Deep Smoothing for Implied Volatility
              </a>
            </td>
          <td>
            Lukas Gonon, Antoine Jacquier, Ruben Wiedemann
          </td>
          <td>2024-06-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Forecasts of various processes have always been a sophisticated problem for statistics and data science. Over the past decades the solution procedures were updated by deep learning and kernel methods. According to many specialists, these approaches are much more precise, stable, and suitable compared to the classical statistical linear time series methods. Here we investigate how true this point of view is.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/5f27149df9a67b32c8200a8dab9b6a75eebb8015" target='_blank'>
              Time Series Analysis: yesterday, today, tomorrow
              </a>
            </td>
          <td>
            Igor Mackarov
          </td>
          <td>2024-06-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Modern recurrent layers are emerging as a promising path toward edge deployment of foundation models, especially in the context of large language models (LLMs). Compressing the whole input sequence in a finite-dimensional representation enables recurrent layers to model long-range dependencies while maintaining a constant inference cost for each token and a fixed memory requirement. However, the practical deployment of LLMs in resource-limited environments often requires further model compression, such as quantization and pruning. While these techniques are well-established for attention-based models, their effects on recurrent layers remain underexplored. In this preliminary work, we focus on post-training quantization for recurrent LLMs and show that Mamba models exhibit the same pattern of outlier channels observed in attention-based LLMs. We show that the reason for the difficulty of quantizing SSMs is caused by activation outliers, similar to those observed in transformer-based LLMs. We report baseline results for post-training quantization of Mamba that do not take into account the activation outliers and suggest first steps for outlier-aware quantization.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3d553478d06f751f872a82c774c8262ce66d47fd" target='_blank'>
              Mamba-PTQ: Outlier Channels in Recurrent Large Language Models
              </a>
            </td>
          <td>
            Alessandro Pierro, Steven Abreu
          </td>
          <td>2024-07-17</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Explanation for Multivariate Time Series Classification (MTSC) is an important topic that is under explored. There are very few quantitative evaluation methodologies and even fewer examples of actionable explanation, where the explanation methods are shown to objectively improve specific computational tasks on time series data. In this paper we focus on analyzing InterpretTime, a recent evaluation methodology for attribution methods applied to MTSC. We reproduce the original paper results, showcase some significant weaknesses of the methodology and propose ideas to improve both its accuracy and efficiency. Unlike related work, we go beyond evaluation and also showcase the actionability of the produced explainer ranking, by using the best attribution methods for the task of channel selection in MTSC. We find that perturbation-based methods such as SHAP and Feature Ablation work well across a set of datasets, classifiers and tasks and outperform gradient-based methods. We apply the best ranked explainers to channel selection for MTSC and show significant data size reduction and improved classifier accuracy.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e742c19db2c0c145c4f58605409e9905e5e13ed7" target='_blank'>
              Improving the Evaluation and Actionability of Explanation Methods for Multivariate Time Series Classification
              </a>
            </td>
          <td>
            D. Serramazza, Thach Le Nguyen, Georgiana Ifrim
          </td>
          <td>2024-06-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="In an era where machine learning permeates every facet of human existence, and data evolves incessantly, the application of machine learning models transcends mere data processing. It involves navigating constant changes exemplified by the phenomenon of concept drift, which often affects model performance.

These drifts can be recurrent due to the cyclic nature of the underlying data generation processes, which could be influenced by recurrent phenomena such as weather and time of the day. 

Stream Learning on data streams with recurrent concept drifts attempts to learn from such streams of data.

The survey underscores the significance of the field and its practical applications, delving into nuanced definitions of machine learning for data streams afflicted by recurrent concept drifts. It explores diverse methodological approaches, elucidating their key design components. Additionally, it examines various evaluation techniques, benchmark datasets, and available software tailored for simulating and analysing data streams with recurrent concept drifts. Concluding, the survey offers insights into potential avenues for future research in the field.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cf06b11f144b00a4ce791a494e6563e109b2d5e9" target='_blank'>
              Recurrent Concept Drifts on Data Streams
              </a>
            </td>
          <td>
            N. Gunasekara, Bernhard Pfahringer, Heitor Murilo Gomes, A. Bifet, Yun Sing Koh
          </td>
          <td>2024-08-01</td>
          <td>Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence</td>
          <td>0</td>
          <td>50</td>
        </tr>

        <tr id="To gain access to networks, various intrusion attack types have been developed and enhanced. The increasing importance of computer networks in daily life is a result of our growing dependence on them. Given this, it is glaringly obvious that algorithmic tools with strong detection performance and dependability are required for a variety of attack types. The objective is to develop a system for intrusion detection based on deep reinforcement learning. On the basis of the Markov decision procedure, the developed system can construct patterns appropriate for classification purposes based on extensive amounts of informative records. Deep Q‐Learning (DQL), Soft DQL, Double DQL, and Soft double DQL are examined from two perspectives. An evaluation of the authors’ methods using UNSW‐NB15 data demonstrates their superiority regarding accuracy, precision, recall, and F1 score. The validity of the model trained on the UNSW‐NB15 dataset was also checked using the BoT‐IoT and ToN‐IoT datasets, yielding competitive results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/30ce8622c0fab63e6c3065f26e32f1636dabeffe" target='_blank'>
              Analysis of anomalous behaviour in network systems using deep reinforcement learning with convolutional neural network architecture
              </a>
            </td>
          <td>
            Mohammad Hossein Modirrousta, Parisa Forghani Arani, Reza Kazemi, Mahdi Aliyari‐Shoorehdeli
          </td>
          <td>2024-06-27</td>
          <td>CAAI Transactions on Intelligence Technology</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Graphs serve as generic tools to encode the underlying relational structure of data. Often this graph is not given, and so the task of inferring it from nodal observations becomes important. Traditional approaches formulate a convex inverse problem with a smoothness promoting objective and rely on iterative methods to obtain a solution. In supervised settings where graph labels are available, one can unroll and truncate these iterations into a deep network that is trained end-to-end. Such a network is parameter efficient and inherits inductive bias from the optimization formulation, an appealing aspect for data constrained settings in, e.g., medicine, finance, and the natural sciences. But typically such settings care equally about uncertainty over edge predictions, not just point estimates. Here we introduce novel iterations with independently interpretable parameters, i.e., parameters whose values - independent of other parameters' settings - proportionally influence characteristics of the estimated graph, such as edge sparsity. After unrolling these iterations, prior knowledge over such graph characteristics shape prior distributions over these independently interpretable network parameters to yield a Bayesian neural network (BNN) capable of graph structure learning (GSL) from smooth signal observations. Fast execution and parameter efficiency allow for high-fidelity posterior approximation via Markov Chain Monte Carlo (MCMC) and thus uncertainty quantification on edge predictions. Synthetic and real data experiments corroborate this model's ability to provide well-calibrated estimates of uncertainty, in test cases that include unveiling economic sector modular structure from S$\&$P$500$ data and recovering pairwise digit similarities from MNIST images. Overall, this framework enables GSL in modest-scale applications where uncertainty on the data structure is paramount.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/d4a2a2d1e598080f8f15315512f0885aeba3c17a" target='_blank'>
              Graph Structure Learning with Interpretable Bayesian Neural Networks
              </a>
            </td>
          <td>
            Max Wasserman, Gonzalo Mateos
          </td>
          <td>2024-06-20</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>3</td>
        </tr>

        <tr id="Smart grids are ushering in a transformative era for energy distribution and consumption, yet their emergence also brings forth novel security and fraud detection challenges. The intricacy of detecting fraud within smart grids demands sophisticated techniques for scrutinizing vast volumes of time series data. This paper introduces a novel approach that amalgamates time series aggregation functions, time series clustering using the Spearman’s distance, and reservoir computing forecasting to effectively uncover fraud within smart grid systems. This is then compared with the real values to classify each prosumer behaviour as regular or fraudulent. The approach is validated by means of data collected from the Parc Bit distribution grid, located in the outskirts of Palma (Balearic Islands), Spain. The results obtained, including the comparison with previous works, demonstrate the effectiveness of the approach proposed, shedding light on its promising potential. In more detail, we show its ability to reduce the false positive rate while maintaining a high true positive ratio, resulting in an increased AUC score. As a net effect this helps to mitigate financial losses and address the various impacts associated with fraudulent activity on smart grids.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/cb7050713f3353a01f4b971849048f0d399d1a97" target='_blank'>
              NTL Detection in Smart Grids by means of a Reservoir Computing-based Solution
              </a>
            </td>
          <td>
            Adrià Serra Oliver, Vincent Canals Guinand, Pau Joan Cortés Forteza, Alberto Ortiz Rodríguez
          </td>
          <td>2024-06-25</td>
          <td>2024 IEEE 22nd Mediterranean Electrotechnical Conference (MELECON)</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="With the rapid expansion and ubiquitous presence of the Internet of Things (IoT), the proliferation of IoT devices has reached unprecedented levels, heightening concerns about IoT security. Intrusion detection based on deep learning has become a crucial approach for safeguarding IoT ecosystems. However, challenges remain in IoT intrusion detection research, including inadequate feature representation at the classifier level and poor correlation among extracted traffic features, leading to diminished classification accuracy. To address these issues, we propose a novel transformer-based IoT intrusion detection model, MBConv-ViT (MobileNet Convolution and Vision Transformer), which enhances the correlation of extracted features by fusing local and global features. By leveraging the high correlation of traffic flow, our model can identify subtle differences in IoT traffic flow, thereby achieving precise classification of attack traffic. Experiments based on the open datasets TON-IoT and Bot-IoT demonstrate that the accuracy of the MBConv-ViT model, respectively, 97.14% and 99.99%, is more effective than several existing typical models.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b6488fded5fb0728d7c3bd04f3afa3fba68d0450" target='_blank'>
              A Deep Learning-Based Intrusion Detection Model Integrating Convolutional Neural Network and Vision Transformer for Network Traffic Attack in the Internet of Things
              </a>
            </td>
          <td>
            Chunlai Du, Yanhui Guo, Yuhang Zhang
          </td>
          <td>2024-07-09</td>
          <td>Electronics</td>
          <td>0</td>
          <td>7</td>
        </tr>

        <tr id="Most state-of-the-art AI applications in atmospheric science are based on classic deep learning approaches. However, such approaches cannot automatically integrate multiple complicated procedures to construct an intelligent agent, since each functionality is enabled by a separate model learned from independent climate datasets. The emergence of foundation models, especially multimodal foundation models, with their ability to process heterogeneous input data and execute complex tasks, offers a substantial opportunity to overcome this challenge. In this report, we want to explore a central question - how the state-of-the-art foundation model, i.e., GPT-4o, performs various atmospheric scientific tasks. Toward this end, we conduct a case study by categorizing the tasks into four main classes, including climate data processing, physical diagnosis, forecast and prediction, and adaptation and mitigation. For each task, we comprehensively evaluate the GPT-4o's performance along with a concrete discussion. We hope that this report may shed new light on future AI applications and research in atmospheric science.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/18a501f9bccb6fcfbb0d37e1ac9f7d535942b340" target='_blank'>
              On the Opportunities of (Re)-Exploring Atmospheric Science by Foundation Models: A Case Study
              </a>
            </td>
          <td>
            Lujia Zhang, Hanzhe Cui, Yurong Song, Chenyue Li, Binhang Yuan, Mengqian Lu
          </td>
          <td>2024-07-25</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Evaluating the performance of causal discovery algorithms that aim to find causal relationships between time-dependent processes remains a challenging topic. In this paper, we show that certain characteristics of datasets, such as varsortability (Reisach et al. 2021) and $R^2$-sortability (Reisach et al. 2023), also occur in datasets for autocorrelated stationary time series. We illustrate this empirically using four types of data: simulated data based on SVAR models and Erd\H{o}s-R\'enyi graphs, the data used in the 2019 causality-for-climate challenge (Runge et al. 2019), real-world river stream datasets, and real-world data generated by the Causal Chamber of (Gamella et al. 2024). To do this, we adapt var- and $R^2$-sortability to time series data. We also investigate the extent to which the performance of score-based causal discovery methods goes hand in hand with high sortability. Arguably, our most surprising finding is that the investigated real-world datasets exhibit high varsortability and low $R^2$-sortability indicating that scales may carry a significant amount of causal information.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e3d479da542a4353c2379183fe7b1a2c9e34e919" target='_blank'>
              Sortability of Time Series Data
              </a>
            </td>
          <td>
            Christopher Lohse, Jonas Wahl
          </td>
          <td>2024-07-18</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="Capturing long-term dependency from historical behaviors is the key to the success of sequential recommendation; however, existing methods focus on extracting global sequential information while neglecting to obtain deep representations from subsequences. Previous research has revealed that the restricted inter-item transfer is fundamental to sequential modeling, and some potential substructures of sequences can help models learn more effective long-term dependency compared to the whole sequence. To automatically find better subsequences and perform efficient learning, we propose a sequential recommendation model with a gated recurrent unit and Transformers, abbreviated as GAT4Rec, which employs Transformers with shared parameters across layers to model users’ historical interaction sequences. The representation learned by the gated recurrent unit is used as the gating signal to identify the optimal substructure in user sequences. The fused representation of the subsequence and edge information is extracted by the encoding layer to make the corresponding recommendations. Experimental results on four well-known publicly available datasets demonstrate that our GAT4Rec model outperforms other recommendation models, achieving performance improvements of 5.77%, 1.35%, 11.58%, and 1.79% in the normalized discounted cumulative gain metric (NDCG@10), respectively.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/79a52d00fb976862ea5711ed640577ec503068bd" target='_blank'>
              GAT4Rec: Sequential Recommendation with a Gated Recurrent Unit and Transformers
              </a>
            </td>
          <td>
            Huaiwen He, Xiangdong Yang, Feng Huang, Feng Yi, Shangsong Liang
          </td>
          <td>2024-07-12</td>
          <td>Mathematics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Anomaly detection for network traffic aims to analyze the characteristics of network traffic in order to discover unknown attacks. Currently, existing detection methods have achieved promising results against high-intensity attacks that aim to interrupt the operation of the target system. In reality, attack behaviors that are commonly exhibited are highly concealed and disruptive. In addition, the attack scales are flexible and variable. In this paper, we construct a multiscale network intrusion behavior dataset, which includes three attack scales and two multiscale attack patterns based on probability distribution. Specifically, we propose a stacked ensemble learning-based detection model for anomalous traffic (or SEDAT for short) to defend against highly concealed multiscale attacks. The model employs a random forest (RF)-based method to select features and introduces multiple base learning autoencoders (AEs) to enhance the representation of multiscale attack behaviors. In addressing the challenge of a single model’s inability to capture the regularities of multiscale attack behaviors, SEDAT is capable of adapting to the complex multiscale characteristics in network traffic, enabling the prediction of network access behavior. Comparative experiments demonstrate that SEDAT exhibits superior detection capabilities in multiscale network attacks. In particular, SEDAT achieves an improvement of at least 5% accuracy over baseline methods for detecting multiscale attacks.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/3e52231e4a1584166846a97c80ca5aa9cdc211a7" target='_blank'>
              SEDAT: A Stacked Ensemble Learning-Based Detection Model for Multiscale Network Attacks
              </a>
            </td>
          <td>
            Yan Feng, Zhihai Yang, Qindong Sun, Yanxiao Liu
          </td>
          <td>2024-07-26</td>
          <td>Electronics</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="Change point detection in time series seeks to identify times when the probability distribution of time series changes. It is widely applied in many areas, such as human-activity sensing and medical science. In the context of multivariate time series, this typically involves examining the joint distribution of high-dimensional data: If any one variable changes, the whole time series is assumed to have changed. However, in practical applications, we may be interested only in certain components of the time series, exploring abrupt changes in their distributions in the presence of other time series. Here, assuming an underlying structural causal model that governs the time-series data generation, we address this problem by proposing a two-stage non-parametric algorithm that first learns parts of the causal structure through constraint-based discovery methods. The algorithm then uses conditional relative Pearson divergence estimation to identify the change points. The conditional relative Pearson divergence quantifies the distribution disparity between consecutive segments in the time series, while the causal discovery method enables a focus on the causal mechanism, facilitating access to independent and identically distributed (IID) samples. Theoretically, the typical assumption of samples being IID in conventional change point detection methods can be relaxed based on the Causal Markov Condition. Through experiments on both synthetic and real-world datasets, we validate the correctness and utility of our approach.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/010ab97fc0f4c786a8c27a35f6bd53cf9efb60a2" target='_blank'>
              Causal Discovery-Driven Change Point Detection in Time Series
              </a>
            </td>
          <td>
            Shanyun Gao, Raghavendra Addanki, Tong Yu, Ryan Rossi, Murat Kocaoglu
          </td>
          <td>2024-07-10</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>8</td>
        </tr>

        <tr id="Network behavior during intrusion deviates from the standard, which can be identified by establishing a baseline of typical activity. Accurate detection of diverse attack classes with machine learning relies on having adequate representative samples for each class. Typically, highly imbalanced datasets like the NSL-KDD led to a biased model favoring the dominant classes. To address the challenge, we employ a Conditional Tabular Generative Adversarial Network (CTGAN) for generating synthetic samples to balance the dataset effectively. Later on, a deep neural network with a final layer comprising 4 neurons is applied for multi-class classification. The proposed method is compared with state-of-the-art approaches that utilize Conditional Generative Adversarial Network (CGAN) and Wasserstein Conditional Generative Adversarial Network (WCGAN) sampling techniques is found to yield an average improvement of 105.93%, 56.37%, and 80.05% based on Precision, Recall, and F1 Score.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/505b92efdc3f85ac7b5c3ae5160bd07ccef4cc98" target='_blank'>
              Enhancing Intrusion Detection Through Deep Learning and Generative Adversarial Network
              </a>
            </td>
          <td>
            Md Habibur Rahman, Leo Martinez, Avdesh Mishra, Mais Nijim, Ayush Goyal, David Hicks
          </td>
          <td>2024-06-11</td>
          <td>2024 4th Interdisciplinary Conference on Electrics and Computer (INTCEC)</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Building prediction intervals for time series forecasting problems presents a complex challenge, particularly when relying solely on point predictors, a common scenario for practitioners in the industry. While research has primarily focused on achieving increasingly efficient valid intervals, we argue that, when evaluating a set of intervals, traditional measures alone are insufficient. There are additional crucial characteristics: the intervals must vary in length, with this variation directly linked to the difficulty of the prediction, and the coverage of the interval must remain independent of the difficulty of the prediction for practical utility. We propose the Heteroscedastic Quantile Regression (HQR) model and the Width-Adaptive Conformal Inference (WACI) method, providing theoretical coverage guarantees, to overcome those issues, respectively. The methodologies are evaluated in the context of Electricity Price Forecasting and Wind Power Forecasting, representing complex scenarios in time series forecasting. The results demonstrate that HQR and WACI not only improve or achieve typical measures of validity and efficiency but also successfully fulfil the commonly ignored mentioned characteristics.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/9c18bfd83fa9808cc2d8310863d5b8bcc05f3b70" target='_blank'>
              Enhancing reliability in prediction intervals using point forecasters: Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference
              </a>
            </td>
          <td>
            Carlos Sebasti'an, Carlos E. Gonz'alez-Guill'en, Jes'us Juan
          </td>
          <td>2024-06-21</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="With technological advancements, much data is being captured by sensors, smartphones, wearable devices, and so forth. These vast datasets are stored in data centres and utilized to forge data‐driven models for the condition monitoring of infrastructures and systems through future data mining tasks. However, these datasets often surpass the processing capabilities of traditional information systems and methodologies due to their significant size. Additionally, not all samples within these datasets contribute valuable information during the model training phase, leading to inefficiencies. The processing and training of Machine Learning algorithms become time‐consuming, and storing all the data demands excessive space, contributing to the Big Data challenge. In this paper, we propose two novel techniques to reduce large time‐series datasets into more compact versions without undermining the predictive performance of the resulting models. These methods also aim to decrease the time required for training the models and the storage space needed for the condensed datasets. We evaluated our techniques on five public datasets, employing three Machine Learning algorithms: Holt‐Winters, SARIMA, and LSTM. The outcomes indicate that for most of the datasets examined, our techniques maintain, and in several instances enhance, the forecasting accuracy of the models. Moreover, we significantly reduced the time required to train the Machine Learning algorithms employed.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/33d3e765763fbda62c139a833d2484281eec1ade" target='_blank'>
              Sampling approaches to reduce very frequent seasonal time series
              </a>
            </td>
          <td>
            Afonso Baldo, Paulo J. S. Ferreira, João Mendes‐Moreira
          </td>
          <td>2024-07-25</td>
          <td>Expert Systems</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertook empirical analyses to understand this bias and discovered that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/937461ab8d8ec3b550a31ce461e668fc910bf25e" target='_blank'>
              Fredformer: Frequency Debiased Transformer for Time Series Forecasting
              </a>
            </td>
          <td>
            Xihao Piao, Zheng Chen, Taichi Murayama, Yasuko Matsubara, Yasushi Sakurai
          </td>
          <td>2024-06-13</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>13</td>
        </tr>

        <tr id="In light of the rapid evolution of Artificial Intelligence (AI), a growing number of researchers are investigating the use of Artificial Neural Networks (ANNs) to enhance first-principle Vehicle Models (VMs) or potentially replace them altogether. This paper investigates how AI can be used optimally to identify a VM in the context of a specific case study based on a small-scale experimental vehicle. To this end, three different VMs, each based on a distinct approach, are implemented and compared: (1) a Kinematic Vehicle Model (KVM), (2) a Deep Neural Network (DNN) based VM, and (3) a coupled approach of DNN with KVM, namely Improved KVM (IKVM), where the DNN is used to learn any unmodeled errors produced by the KVM. In the context of the DNN-based approaches, four types of DNNs are implemented based on different configurations of layers (fully connected, convolution, and long short-term memory). For DNN training and evaluation, a custom dataset of driving data is created by driving an F1tenth model car for around nine and a half hours on an indoor track while recording all motions using a motion tracking system. The experiments examine the VMs based on multiple performance metrics: the sampling period, 12 different scenarios, and the number of prediction steps the VMs are able to regressively predict without receiving updates regarding extrinsic vehicle states before the error grows too large, i.e., above 1 cm. Our findings are that DNN can increase KVM fidelity substantially. The optimal use of VMs, however, depends on the problem parameters and the vehicle states to be predicted.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/49e9737dd539cedecb4f06c7b57dd3a94cf9ecec" target='_blank'>
              Experimental Evaluation of Deep Neural Networks for Vehicle Model Identification
              </a>
            </td>
          <td>
            Amira Moualhi, Maryam Nezami, Saleh Mulhem, Georg Schildbach
          </td>
          <td>2024-06-25</td>
          <td>2024 European Control Conference (ECC)</td>
          <td>0</td>
          <td>17</td>
        </tr>

        <tr id="Self-Organizing Maps (SOM) promise a lightweight approach for multivariate time series anomaly detection in lightweight autonomous embedded systems. However, the enormous volume of time series data from autonomous systems testing requires huge SOMs with impractical search overhead. We present KDTree-SOM that effectively optimizes the winner node search for huge SOMs by reconstructing the SOM as a k-dimensional tree (kd-tree). KDTree-SOM achieves on average a 4 × inference time reduction for huge SOMs while achieving up to 95% anomaly detection accuracy with only KB-level memory overhead, demonstrating its potential for anomaly detection in lightweight autonomous embedded platforms.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/e340f7ae16b34b38e4f0879b34dea98ad863a2d8" target='_blank'>
              KDTree-SOM: Self-organizing Map based Anomaly Detection for Lightweight Autonomous Embedded Systems
              </a>
            </td>
          <td>
            Ping-Xiang Chen, Dongjoo Seo, Biswadip Maity, Nikil Dutt
          </td>
          <td>2024-06-12</td>
          <td>Proceedings of the Great Lakes Symposium on VLSI 2024</td>
          <td>0</td>
          <td>5</td>
        </tr>

        <tr id="
 The rise of Internet of Things (IoT) devices has brought about an increase in security risks, emphasizing the need for effective anomaly detection systems. Previous research introduced a dynamic voting classifier to overcome overfitting or inaccurate accuracies caused by dataset imbalance. This article introduces a new method for IoT anomaly detection that employs a hybrid voting classifier, which combines several machine learning models. To solve the overfitting and class weight issues, an adaptive voting classifier is used that adjusts weights according to the highest preference for accuracy. The developing voting system increases the effectiveness of more accurate classifiers, enhancing the group's overall capability. A proposed combined classifier combines Logistic Regression, AdaBoost, Gradient Boosting, and Multi-Layer Perceptron models using a soft voting method. To develop and assess this method, the CIC-IoT-2023 dataset is utilized, which contains 33 types of IoT attacks across 7 categories. This process includes thorough data preprocessing and feature selection from a pool of 42 available attributes. The performance of this approach is measured against individual classifiers across binary, 8-class, and 34-class classification tasks. The results highlight the effectiveness of the hybrid model. It achieves 98.95% accuracy, 76.72% recall, and 72.01% F1-score in the 34-class problem, surpassing the performance of all individual models. For the 8-class task, the hybrid classifier attains 99.39% accuracy, 90.89% recall, and an 83.01% F1-score. This demonstrates the high potential of the hybrid approach for IoT anomaly detection.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/a40c07139a4b32a768d46ddcad299ec43c84709f" target='_blank'>
              Unveiling anomalies: harnessing machine learning for detection and insights
              </a>
            </td>
          <td>
            Shubh Gupta, Sanoj Kumar, Karan Singh, Deepika Saini
          </td>
          <td>2024-07-23</td>
          <td>Engineering Research Express</td>
          <td>0</td>
          <td>2</td>
        </tr>

        <tr id="Machine Learning (ML) is extensively used for predicting transfer times for general purpose Wide Area Networks (WANs) or public Internet applications, but for Research and Education Networks (RENs) two major gaps exist in literature. First, RENs i.e. networks carrying large data flows have received limited attention by the networking community. RENs behave differently compared to the general purpose Internet applications and other network types. Hence, ML models from other network types cannot be used interchangeably for large data transfers. Second, the ML models are used as blackboxes to train on measured network values and then used to predict transfer times or other runtime network parameters. In this paper, we present a dynamical systems model of the large data transfers typical of RENs in the form of a system of Ordinary Differential Equations (ODEs) inspired by the Lotka-Volterra competition model. We present a transfer time prediction component called Dynamic Transfer Time Predictor (DTTP) which solves the ODEs and predicts the future transfer times. Second we formulate a loss function based on Lyapunov function called Lyapunov Drift Correction (LDC) that self-corrects the transfer time prediction errors dynamically.To design and develop our model, we studied real-world datasets consisting of over 100 million transfer records collected from platforms such as Open Science Grid (OSG), Large Hadron Collider Optical Private Network (LHCOPN), Worldwide LHC Grid (WLCG), as well as the RENs of Internet2 and ESNet. We integrate our model into well-known neural network models and regressors and present evaluation results.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/4cf5d71336cf90025f7d641a600628cc1d15b0d7" target='_blank'>
              Improving Transfer Time Prediction of ML Models via Auto-correcting Dynamical Systems Modeling
              </a>
            </td>
          <td>
            Venkat Sai Suman Lamba Karanam, B. Ramamurthy
          </td>
          <td>2024-06-24</td>
          <td>2024 IEEE 10th International Conference on Network Softwarization (NetSoft)</td>
          <td>0</td>
          <td>38</td>
        </tr>

        <tr id="Accurate streamflow forecasting is crucial for effectively managing water resources, particularly in countries like Colombia, where hydroelectric power generation significantly contributes to the national energy grid. Although highly interpretable, traditional deterministic, physically-driven models often suffer from complexity and require extensive parameterization. Data-driven models like Linear Autoregressive (LAR) and Long Short-Term Memory (LSTM) networks offer simplicity and performance but cannot quantify uncertainty. This work introduces Sparse Variational Gaussian Processes (SVGPs) for forecasting streamflow contributions. The proposed SVGP model reduces computational complexity compared to traditional Gaussian Processes, making it highly scalable for large datasets. The methodology employs optimal hyperparameters and shared inducing points to capture short-term and long-term relationships among reservoirs. Training, validation, and analysis of the proposed approach consider the streamflow dataset from 23 geographically dispersed reservoirs recorded during twelve years in Colombia. Performance assessment reveals that the proposal outperforms baseline Linear Autoregressive (LAR) and Long Short-Term Memory (LSTM) models in three key aspects: adaptability to changing dynamics, provision of informative confidence intervals through Bayesian inference, and enhanced forecasting accuracy. Therefore, the SVGP-based forecasting methodology offers a scalable and interpretable solution for multi-output streamflow forecasting, thereby contributing to more effective water resource management and hydroelectric planning.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/82ecf0c70aa07f89d48d8aa7c5d4ec1674f0475d" target='_blank'>
              Scalable and Interpretable Forecasting of Hydrological Time Series Based on Variational Gaussian Processes
              </a>
            </td>
          <td>
            J. D. Pastrana-Cortés, J. Gil-González, A. Álvarez-Meza, D. Cárdenas-Peña, Á. Orozco-Gutiérrez
          </td>
          <td>2024-07-15</td>
          <td>Water</td>
          <td>0</td>
          <td>15</td>
        </tr>

        <tr id="Extensive research indicates that microRNAs (miRNAs) play a crucial role in the analysis of complex human diseases. Recently, numerous methods utilizing graph neural networks have been developed to investigate the complex relationships between miRNAs and diseases. However, these methods often face challenges in terms of overall effectiveness and are sensitive to node positioning. To address these issues, the researchers introduce DARSFormer, an advanced deep learning model that integrates dynamic attention mechanisms with a spectral graph Transformer effectively. In the DARSFormer model, a miRNA-disease heterogeneous network is constructed initially. This network undergoes spectral decomposition into eigenvalues and eigenvectors, with the eigenvalue scalars being mapped into a vector space subsequently. An orthogonal graph neural network is employed to refine the parameter matrix. The enhanced features are then input into a graph Transformer, which utilizes a dynamic attention mechanism to amalgamate features by aggregating the enhanced neighbor features of miRNA and disease nodes. A projection layer is subsequently utilized to derive the association scores between miRNAs and diseases. The performance of DARSFormer in predicting miRNA-disease associations is exemplary. It achieves an AUC of 94.18% in a five-fold cross-validation on the HMDD v2.0 database. Similarly, on HMDD v3.2, it records an AUC of 95.27%. Case studies involving colorectal, esophageal, and prostate tumors confirm 27, 28, and 26 of the top 30 associated miRNAs against the dbDEMC and miR2Disease databases, respectively. The code and data for DARSFormer are accessible at https://github.com/baibaibaialone/DARSFormer.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/bcbba43c6863b6cf1131d13591a94aa701556932" target='_blank'>
              Predicting miRNA-disease Associations Based on Spectral Graph Transformer with Dynamic Attention and Regularization.
              </a>
            </td>
          <td>
            Zhengwei Li, Xu Bai, Ru Nie, Yanyan Liu, Lei Zhang, Zhuhong You
          </td>
          <td>2024-08-05</td>
          <td>IEEE journal of biomedical and health informatics</td>
          <td>0</td>
          <td>10</td>
        </tr>

        <tr id="This study proposes a novel long short-term memory (LSTM)-based model for predicting future physical properties based on partial data of molecular dynamics (MD) simulation. It extracts latent vectors from atomic coordinates of MD simulations using graph convolutional network, utilizes LSTM to learn temporal trends in latent vectors and make one-step-ahead predictions of physical properties through fully connected layers. Validating with MD simulations of Ni solid-liquid systems, the model achieved accurate one-step-ahead prediction for time variation of the potential energy during solidification and melting processes using residual connections. Recursive use of predicted values enabled long-term prediction from just the first 20 snapshots of the MD simulation. The prediction has captured the feature of potential energy bending at low temperatures, which represents completion of solidification, despite that the MD data in short time do not have such a bending characteristic. Remarkably, for long-time prediction over 900 ps, the computation time was reduced to 1/700th of a full MD simulation of the same duration. This approach has shown the potential to significantly reduce computational cost for prediction of physical properties by efficiently utilizing the data of MD simulation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/662da49d1afae1c78b9917edaa36e73769970afc" target='_blank'>
              Predicting long-term trends in physical properties from short-term molecular dynamics simulations using long short-term memory
              </a>
            </td>
          <td>
            Kota Noda, Yasushi Shibuta
          </td>
          <td>2024-06-13</td>
          <td>Journal of Physics: Condensed Matter</td>
          <td>0</td>
          <td>1</td>
        </tr>

        <tr id="This study delves into the creation of anomaly detection technology applicable to a range of equipment groups within smart factories. This advanced technology uses high-performance MEMS vibration sensors, edge CMS devices, and PHM platforms to tackle issues such as data imbalance, learning model limitations, complex equipment operating patterns, and real-time processing. It also addresses central server concentration, data cycling problem, various equipment classification, and algorithm operation problems that can arise when implementing systems in the field. Using AI-based vibration detection algorithms, data can be collected at high sampling rates and analyzed in real-time through edge computing, minimizing latency and mitigating server capacity issues compared to cloud-based analytics. The system continually monitors and learns standard performance data from equipment to provide practical solutions that minimize equipment failures and downtimes. The results of this study are impressive, as it has successfully developed anomaly detection framework and PHM systems that are expected to enhance the efficiency and sustainability of smart factories. Furthermore, the study aims to showcase and improve the effectiveness of predictive maintenance in both domestic and international automotive factory production lines. This revolutionary technology will be a key component in smart and software-defined factories and help companies achieve intelligent automation.">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/512c6b0b8bf35d41ca3c88f89e4c977aaef23c9a" target='_blank'>
              Development of Anomaly Detection Technology Applicable to Various Equipment Groups in Smart Factory
              </a>
            </td>
          <td>
            Kiwon Park, Myoung Gyo Lee, Sung Yong Cho, Yoon Jang, Young Tae Choi
          </td>
          <td>2024-06-27</td>
          <td>PHM Society European Conference</td>
          <td>0</td>
          <td>0</td>
        </tr>

        <tr id="In many situations, the measurements of a studied phenomenon are provided sequentially, and the prediction of its class needs to be made as early as possible so as not to incur too high a time penalty, but not too early and risk paying the cost of misclassification. This problem has been particularly studied in the case of time series, and is known as Early Classification of Time Series (ECTS). Although it has been the subject of a growing body of literature, there is still a lack of a systematic, shared evaluation protocol to compare the relative merits of the various existing methods. This document begins by situating these methods within a principle-based taxonomy. It defines dimensions for organizing their evaluation, and then reports the results of a very extensive set of experiments along these dimensions involving nine state-of-the art ECTS algorithms. In addition, these and other experiments can be carried out using an open-source library in which most of the existing ECTS algorithms have been implemented (see \url{https://github.com/ML-EDM/ml_edm}).">
          <td id="tag"><i class="material-icons">visibility_off</i></td>
          <td><a href="https://www.semanticscholar.org/paper/b398a7a69b9500f231044e06baa20805f2eb5afd" target='_blank'>
              Early Classification of Time Series: Taxonomy and Benchmark
              </a>
            </td>
          <td>
            Aurélien Renault, A. Bondu, Antoine Cornu'ejols, Vincent Lemaire
          </td>
          <td>2024-06-26</td>
          <td>ArXiv</td>
          <td>0</td>
          <td>10</td>
        </tr>

    </tbody>
    <tfoot>
      <tr>
          <th>Abstract</th>
          <th>Title</th>
          <th>Authors</th>
          <th>Publication Date</th>
          <th>Journal/Conference</th>
          <th>Citation count</th>
          <th>Highest h-index</th>
      </tr>
    </tfoot>
    </table>
    </p>
  </div>

</body>

<script>

  function create_author_list(author_list) {
    let td_author_element = document.getElementById();
    for (let i = 0; i < author_list.length; i++) {
          // tdElements[i].innerHTML = greet(tdElements[i].innerHTML);
          alert (author_list[i]);
      }
  }

  var trace1 = {
    x: ['2023', '2024'],
    y: [0, 24],
    name: 'Num of citations',
    yaxis: 'y1',
    type: 'scatter'
  };

  var data = [trace1];

  var layout = {
    yaxis: {
      title: 'Num of citations',
      }
  };
  Plotly.newPlot('myDiv1', data, layout);
</script>
<script>
var dataTableOptions = {
        initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;

                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);

                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';

                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    scrollX: true,
    scrollCollapse: true,
    paging: true,
    fixedColumns: true,
    columnDefs: [
        {"className": "dt-center", "targets": "_all"},
        // set width for both columns 0 and 1 as 25%
        { width: '7%', targets: 0 },
        { width: '30%', targets: 1 },
        { width: '25%', targets: 2 },
        { width: '15%', targets: 4 }

      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  }
  new DataTable('#table1', dataTableOptions);
  new DataTable('#table2', dataTableOptions);

  var table1 = $('#table1').DataTable();
  $('#table1 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table1.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
    }
  });
  var table2 = $('#table2').DataTable();
  $('#table2 tbody').on('click', 'td:first-child', function () {
    var tr = $(this).closest('tr');
    var row = table2.row( tr );

    var rowId = tr.attr('id');
    // alert(rowId);

    if (row.child.isShown()) {
      // This row is already open - close it.
      row.child.hide();
      tr.removeClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility_off</i>');
    } else {
      // Open row.
      // row.child('foo').show();
      var content = '<div class="child-row-content"><strong>Abstract:</strong> ' + rowId + '</div>';
      row.child(content).show();
      tr.addClass('shown');
      tr.find('td:first-child').html('<i class="material-icons">visibility</i>');
    }
  });
</script>
<style>
  .child-row-content {
    text-align: justify;
    text-justify: inter-word;
    word-wrap: break-word; /* Ensure long words are broken */
    white-space: normal; /* Ensure text wraps to the next line */
    max-width: 100%; /* Ensure content does not exceed the table width */
    padding: 10px; /* Optional: add some padding for better readability */
    /* font size */
    font-size: small;
  }
</style>
</html>







  
  




  



                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../assets/javascripts/bundle.c8d2eff1.min.js"></script>
      
    
<script>
  // Execute intro.js when a button with id 'intro' is clicked
  function startIntro(){
      introJs().setOptions({
          tooltipClass: 'customTooltip'
      }).start();
  }
</script>
<script>
  

  // new DataTable('#table1', {
  //   order: [[5, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });

  // new DataTable('#table2', {
  //   order: [[3, 'desc']],
  //   "columnDefs": [
  //       {"className": "dt-center", "targets": "_all"},
  //       { width: '30%', targets: 0 }
  //     ],
  //   pageLength: 10,
  //   layout: {
  //       topStart: {
  //           buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
  //       }
  //   }
  // });
  new DataTable('#table3', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
  new DataTable('#table4', {
    initComplete: function () {
        this.api()
            .columns()
            .every(function () {
                let column = this;
 
                // Create select element
                let select = document.createElement('select');
                select.add(new Option(''));
                column.footer().replaceChildren(select);
 
                // Apply listener for user change in value
                select.addEventListener('change', function () {
                    column
                        .search(select.value, {exact: true})
                        .draw();
                });

                // keep the width of the select element same as the column
                select.style.width = '100%';
 
                // Add list of options
                column
                    .data()
                    .unique()
                    .sort()
                    .each(function (d, j) {
                        select.add(new Option(d));
                    });
            });
    },
    // order: [[3, 'desc']],
    "columnDefs": [
        {"className": "dt-center", "targets": "_all"},
        { width: '30%', targets: 0 }
      ],
    pageLength: 10,
    layout: {
        topStart: {
            buttons: ['copy', 'csv', 'excel', 'pdf', 'print']
        }
    }
  });
</script>


  </body>
</html>